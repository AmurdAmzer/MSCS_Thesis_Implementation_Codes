{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMfCW1WogC9t92PneKGL0rq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AmurdAmzer/SAKT-Paper_Implementation/blob/main/SAKTPaperImplementation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cell 1: Uploading data via Colab's File Browser"
      ],
      "metadata": {
        "id": "r3Mjf06XlwkI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 1: Uploading data via Colab's File Browser\n",
        "\n",
        "# os stands for \"Operating System\" - it's like a special toolkit that lets your Python code talk to your computer's file system.\n",
        "import os\n",
        "print(\"Files in current directory:\")\n",
        "for file in os.listdir():\n",
        "  if file.endswith(\".csv\"):\n",
        "    print(f\" - {file}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LacccNggl2gy",
        "outputId": "61f5c92e-7dd0-4d96-812b-786a9441165b"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files in current directory:\n",
            " - skill_builder_data_corrected_collapsed.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Cell 2: Imports and File upload"
      ],
      "metadata": {
        "id": "soB_jKNd5A3M"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "l7PycBC01iR8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95b87421-c828-408b-f5f4-79c74ba984f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (346860, 31)\n",
            "\n",
            "Column names: ['Unnamed: 0', 'order_id', 'assignment_id', 'user_id', 'assistment_id', 'problem_id', 'original', 'correct', 'attempt_count', 'ms_first_response', 'tutor_mode', 'answer_type', 'sequence_id', 'student_class_id', 'position', 'type', 'base_sequence_id', 'skill_id', 'skill_name', 'teacher_id', 'school_id', 'hint_count', 'hint_total', 'overlap_time', 'template_id', 'answer_id', 'answer_text', 'first_action', 'bottom_hint', 'opportunity', 'opportunity_original']\n",
            "\n",
            "First 5 rows:    Unnamed: 0  order_id  assignment_id  user_id  assistment_id  problem_id  \\\n",
            "0           1  33022537         277618    64525          33139       51424   \n",
            "1           2  33022709         277618    64525          33150       51435   \n",
            "2           3  35450204         220674    70363          33159       51444   \n",
            "3           4  35450295         220674    70363          33110       51395   \n",
            "4           5  35450311         220674    70363          33196       51481   \n",
            "\n",
            "   original  correct  attempt_count  ms_first_response  ... hint_count  \\\n",
            "0         1        1              1              32454  ...          0   \n",
            "1         1        1              1               4922  ...          0   \n",
            "2         1        0              2              25390  ...          0   \n",
            "3         1        1              1               4859  ...          0   \n",
            "4         1        0             14              19813  ...          3   \n",
            "\n",
            "  hint_total  overlap_time  template_id  answer_id answer_text  first_action  \\\n",
            "0          3         32454        30799        NaN          26             0   \n",
            "1          3          4922        30799        NaN          55             0   \n",
            "2          3         42000        30799        NaN          88             0   \n",
            "3          3          4859        30059        NaN          41             0   \n",
            "4          4        124564        30060        NaN          65             0   \n",
            "\n",
            "  bottom_hint opportunity  opportunity_original  \n",
            "0         NaN           1                   1.0  \n",
            "1         NaN           2                   2.0  \n",
            "2         NaN           1                   1.0  \n",
            "3         NaN           2                   2.0  \n",
            "4         0.0           3                   3.0  \n",
            "\n",
            "[5 rows x 31 columns]\n"
          ]
        }
      ],
      "source": [
        "# Cell 2: Imports and File upload (df = pd.read_csv(....)) opens the spreadsheet(data) and puts it into a \"DataFrame\"\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Load the CSV file (Remember to use the actual file name)\n",
        "# encoding='ISO-8859-1' handles special characters in the data\n",
        "# low_memory=False prevents dtype warnings for mixed types\n",
        "\n",
        "# I choose to call my loaded data df, df can mean DataFrame, df is just a variable name, I can choose to name my uploaded data anything.\n",
        "# low_memory=False tells the computer \"take your time reading this properly, don't rush\"\n",
        "# encoding='ISO-8859-1' = Like telling your computer \"hey, this file might have special characters like currency symbols ($ etc.)\".\n",
        "#  Real-Life Analogy\n",
        "\"\"\"\n",
        "It's like telling your computer:\n",
        "When you read this file, treat these bytes or symbols as Latin-style letters â€” not random gibberish.\"\n",
        "\n",
        "If you don't specify the correct encoding, Python might fail to read the file\n",
        "\"\"\"\n",
        "\n",
        "df = pd.read_csv('skill_builder_data_corrected_collapsed.csv', encoding='ISO-8859-1', low_memory=False)\n",
        "\n",
        "# Show basic information about the dataset\n",
        "print(f\"Dataset shape: {df.shape}\") # (rows, columns)\n",
        "print(f\"\\nColumn names: {list(df.columns)}\") # all column names\n",
        "print(f\"\\nFirst 5 rows: {df.head()}\") # preview first 5 rows. head() has a default parameter built in df.head(n=5). you could specify by df.head(5) etc.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cell3: Key Statistics - Understanding the dataset size and scope"
      ],
      "metadata": {
        "id": "Ir9cX3Pr5Ncc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: Key Statistics - Understanding the dataset size and scope\n",
        "\n",
        "print(\"===DATASET OVERVIEW===\")\n",
        "\n",
        "# Count total number of student interactions = number of rows\n",
        "print(f\"Toatal interactions: {len(df)}\")\n",
        "\n",
        "# Count unique students - each student has a unique user_id\n",
        "print(f\"Unique students: {df['user_id'].nunique()}\") # df['user_id] = access the user id column. nunique = count how many unique values are in that column\n",
        "\n",
        "# Count unique problems - individual questions students attempted\n",
        "print(f\"Unique problems: {df['problem_id'].nunique()}\")\n",
        "\n",
        "# Count unique skills - knowledge concepts being tested. this is crucial because I will create embeddings for each skill\n",
        "print(f\"Unique skills: {df['skill_id'].nunique()}\")\n",
        "\n",
        "# Calculate overall performance - percentage of correct answers\n",
        "print(f\"\\nCorrect rate: {df['correct'].mean():.2%}\")  # .2% means format the number as percentage with two decimal places, so for eg. 0.825641 becomes 82.56%\n",
        "\n",
        "# Check data completeness for skill_id (Critical for SAKT)\n",
        "# NB. SAKT needs skill_id to work - rows without the skill_id must be removed\n",
        "print(f\"Rows with skill_id: {df['skill_id'].notna().sum()}\")\n",
        "print(f\"Rows missing skill_id: {df['skill_id'].isna().sum()} ({df['skill_id'].isna().mean():.1%})\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cqUdL-mz24hc",
        "outputId": "896a5319-5018-42d3-d39c-a7fdd9575db8"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===DATASET OVERVIEW===\n",
            "Toatal interactions: 346860\n",
            "Unique students: 4217\n",
            "Unique problems: 26688\n",
            "Unique skills: 149\n",
            "\n",
            "Correct rate: 64.53%\n",
            "Rows with skill_id: 283105\n",
            "Rows missing skill_id: 63755 (18.4%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CELL 4: Examine One Student's Learning Journey (Creating a case study).\n",
        "# Before SAKT learns from all students, you want to see what one student's learning path looks like\n",
        "# This helps to understand the sequential nature of the data"
      ],
      "metadata": {
        "id": "QhKU22wsrOMe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4: Examine One Student's (random) Learning Journey\n",
        "# This helps us understand the sequential nature of the data\n",
        "\n",
        "# Find students sorted by number of attempts (most active students)\n",
        "student_activity = df['user_id'].value_counts()\n",
        "\n",
        "# Pick the 11th most active student (avoid outliers)\n",
        "student_id = student_activity.index[20]\n",
        "\n",
        "# Get all data for this student, sorted by time\n",
        "# Order_id represents the sequence of attempsts\n",
        "\n",
        "student_data = df[df['user_id'] == student_id].sort_values('order_id')\n",
        "\n",
        "# Display student summary\n",
        "print(f\"Student {student_id} attempted {len(student_data)} problems\")\n",
        "print(f\"Skills attempted: {student_data['skill_id'].nunique()}\")\n",
        "print(f\"Correct rate: {student_data['correct'].mean():.2%}\")\n",
        "\n",
        "# Show their first 30 attempts to see the sequential Pattern\n",
        "print(\"\\nFirst 30 attempts:\")\n",
        "print(student_data[['order_id', 'skill_id', 'correct', 'ms_first_response']].head(30))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8nUiiGtprW7R",
        "outputId": "dcf5122f-9805-429e-fe3b-db92e1d12e8a"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Student 96235 attempted 977 problems\n",
            "Skills attempted: 87\n",
            "Correct rate: 58.03%\n",
            "\n",
            "First 30 attempts:\n",
            "        order_id skill_id  correct  ms_first_response\n",
            "96959   38171250       49        0               6228\n",
            "96960   38171251       49        1               4544\n",
            "96961   38171252       49        1               6059\n",
            "96962   38171253       49        0               6154\n",
            "96963   38171254       49        0              11896\n",
            "96964   38171255       49        1              11180\n",
            "96965   38171256       49        0               5603\n",
            "96966   38171257       49        0               5152\n",
            "96967   38171258       49        0               8184\n",
            "96968   38171259       49        0               3551\n",
            "96969   38171260       49        1               7122\n",
            "96970   38171261       49        1               4576\n",
            "96971   38171262       49        0               4139\n",
            "96972   38171263       49        0               6022\n",
            "96973   38171264       49        0              14956\n",
            "96974   38171265       49        1               9418\n",
            "96975   38171266       49        1              11435\n",
            "96976   38171267       49        0               6624\n",
            "96977   38171268       49        0               6217\n",
            "96978   38171269       49        1               9203\n",
            "259830  38171273      311        1             105282\n",
            "229704  38171275      307        1              15819\n",
            "259831  38171300      311        1              80594\n",
            "282263  38171301      368        0              64103\n",
            "282264  38171302      368        0             113526\n",
            "282265  38171303      368        1              67698\n",
            "282266  38171304      368        1              48215\n",
            "282267  38171305      368        1              92317\n",
            "60818   38171315       32        1              12908\n",
            "11447   38171316        4        0              15694\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CELL 5: Visualize Key Patterns to Understand The Data Better"
      ],
      "metadata": {
        "id": "j4AOb7nX5wK_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 5: Visualing Key Patterns to Understand the Data Better\n",
        "\n",
        "# Calculate seuence length for each student (how many problems/quenstions/skills did each student attempt)\n",
        "\n",
        "seq_lengths = df.groupby('user_id').size()\n",
        "\n",
        "# seq_len for the Top 20 students\n",
        "print(\"top 20 Most Active Students:\")\n",
        "print(\"user_id: sequence length\")\n",
        "sorted_seq = seq_lengths.sort_values(ascending=False)\n",
        "for user_id, length in sorted_seq.head(20).items():\n",
        "  print(f\"{user_id}:     {length}\")\n",
        "\n",
        "print(\"\\n\")\n",
        "\n",
        "\n",
        "# seq_len for the Buttom 20 students\n",
        "print(\"Buttom 20 Least Active Students:\")\n",
        "print(\"user_id: sequence length\")\n",
        "sorted_seq = seq_lengths.sort_values(ascending=False)\n",
        "for user_id, length in sorted_seq.tail(20).items():\n",
        "  print(f\"{user_id}:     {length}\")\n",
        "\n",
        "print(\"\\n\")\n",
        "\n",
        "# Random 20 students sorted by length\n",
        "print(\"20 Random students sorted by length\")\n",
        "print(\"user_id: sequence length\")\n",
        "random_sample = seq_lengths.sample(20).sort_values() # If you remove the ascending argument, pandas uses the default, which is: ascending=True\n",
        "for user_id, length in random_sample.items():\n",
        "  print(f\"{user_id}:     {length}\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WPbKjUOS5vWK",
        "outputId": "216d8c31-d624-491d-8358-c88633b6b4b5"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "top 20 Most Active Students:\n",
            "user_id: sequence length\n",
            "78978:     1383\n",
            "78970:     1334\n",
            "79032:     1264\n",
            "79021:     1239\n",
            "96274:     1236\n",
            "78979:     1163\n",
            "96244:     1149\n",
            "75169:     1129\n",
            "79013:     1124\n",
            "78989:     1115\n",
            "79029:     1112\n",
            "78980:     1112\n",
            "79019:     1095\n",
            "71881:     1089\n",
            "78987:     1084\n",
            "96243:     1083\n",
            "79031:     1064\n",
            "79018:     1041\n",
            "96265:     1014\n",
            "79012:     1005\n",
            "\n",
            "\n",
            "Buttom 20 Least Active Students:\n",
            "user_id: sequence length\n",
            "87330:     1\n",
            "87336:     1\n",
            "87337:     1\n",
            "91464:     1\n",
            "91439:     1\n",
            "87376:     1\n",
            "92527:     1\n",
            "85549:     1\n",
            "85553:     1\n",
            "85554:     1\n",
            "85555:     1\n",
            "85556:     1\n",
            "85558:     1\n",
            "85367:     1\n",
            "77725:     1\n",
            "74701:     1\n",
            "71163:     1\n",
            "84309:     1\n",
            "51933:     1\n",
            "84305:     1\n",
            "\n",
            "\n",
            "20 Random students sorted by length\n",
            "user_id: sequence length\n",
            "88912:     3\n",
            "91162:     3\n",
            "90172:     4\n",
            "87318:     6\n",
            "86741:     19\n",
            "87521:     19\n",
            "84523:     21\n",
            "74674:     24\n",
            "80954:     26\n",
            "83960:     35\n",
            "91930:     39\n",
            "85054:     50\n",
            "78040:     52\n",
            "89249:     56\n",
            "84562:     62\n",
            "87996:     62\n",
            "80138:     64\n",
            "87995:     78\n",
            "83194:     197\n",
            "78073:     794\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CELL 6: Data Quality Check - Critical for Reliable Model Training"
      ],
      "metadata": {
        "id": "ATuGUM4053dg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 6: Data Quality Check - Critical for Reliable Model Training\n",
        "\n",
        "print(\"Data Quality Report:\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# Check 1: Duplicate rows (same data appearing multiple times)\n",
        "duplicate_count = df.duplicated().sum()\n",
        "print(f\"Duplicate rows: {duplicate_count}\")\n",
        "if duplicate_count > 0:\n",
        "    print(\"Need to remove duplicates!\")\n",
        "\n",
        "# Check 2: Students with very few attempts\n",
        "# SAKT needs sufficient history to learn patterns\n",
        "too_few = (seq_lengths < 5).sum()\n",
        "print(f\"Students with <5 attempts: {too_few} ({too_few/len(seq_lengths)*100:.1f}%)\")\n",
        "\n",
        "# Check 3: Students with too many attempts (potential outliers)\n",
        "too_many = (seq_lengths > 500).sum()\n",
        "print(f\"Students with >500 attempts: {too_many}\")\n",
        "\n",
        "# Check 4: Temporal ordering validation\n",
        "# Sort by user and order_id to check sequence integrity\n",
        "df_sorted = df.sort_values(['user_id', 'order_id'])\n",
        "# For each user, check if order_id always increases\n",
        "is_ordered = df_sorted.groupby('user_id')['order_id'].apply(\n",
        "    lambda x: (x.diff().dropna() > 0).all()  # diff() calculates difference between consecutive values\n",
        ").all()\n",
        "print(f\"All sequences properly ordered: {is_ordered}\")\n",
        "\n",
        "# Check 5: Original vs scaffolding problems\n",
        "# Original = main problem, scaffolding = hints/sub-problems\n",
        "original_count = (df['original'] == 1).sum()\n",
        "scaffold_count = (df['original'] == 0).sum()\n",
        "print(f\"\\nOriginal problems: {original_count} ({original_count/len(df)*100:.1f}%)\")\n",
        "print(f\"Scaffolding problems: {scaffold_count} ({scaffold_count/len(df)*100:.1f}%)\")\n",
        "print(\"SAKT paper uses only original problems\")\n",
        "\n",
        "# Check 6: Answer distribution\n",
        "print(f\"\\nAnswer distribution:\")\n",
        "print(f\"Correct: {(df['correct'] == 1).sum()} ({df['correct'].mean()*100:.1f}%)\")\n",
        "print(f\"Incorrect: {(df['correct'] == 0).sum()} ({(1-df['correct'].mean())*100:.1f}%)\")\n",
        "\n",
        "# Check 7: Critical missing data for SAKT\n",
        "print(f\"\\nMissing data analysis:\")\n",
        "print(f\"Missing skill_id: {df['skill_id'].isna().sum()} rows ({df['skill_id'].isna().mean()*100:.1f}%)\")\n",
        "print(f\"Missing user_id: {df['user_id'].isna().sum()} rows\")\n",
        "print(f\"Missing correct: {df['correct'].isna().sum()} rows\")\n",
        "print(\"SAKT requires skill_id, user_id, and correct to be present\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nh6WRXAH56qi",
        "outputId": "ef71210c-c37d-4ce7-afb9-647fc8547620"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data Quality Report:\n",
            "----------------------------------------\n",
            "Duplicate rows: 0\n",
            "Students with <5 attempts: 473 (11.2%)\n",
            "Students with >500 attempts: 181\n",
            "All sequences properly ordered: True\n",
            "\n",
            "Original problems: 275458 (79.4%)\n",
            "Scaffolding problems: 71402 (20.6%)\n",
            "SAKT paper uses only original problems\n",
            "\n",
            "Answer distribution:\n",
            "Correct: 223818 (64.5%)\n",
            "Incorrect: 123042 (35.5%)\n",
            "\n",
            "Missing data analysis:\n",
            "Missing skill_id: 63755 rows (18.4%)\n",
            "Missing user_id: 0 rows\n",
            "Missing correct: 0 rows\n",
            "SAKT requires skill_id, user_id, and correct to be present\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CELL 7: Data Preprocessing for SAKT"
      ],
      "metadata": {
        "id": "OKW2EPOJ7Y3Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 7: Data Preprocessing for SAKT\n",
        "\n",
        "print(\"Starting data preprocessing...\")\n",
        "print(f\"Original data shape: {df.shape}\")\n",
        "\n",
        "# Step 1: Keep only original problems (main problems, not hints)\n",
        "# SAKT paper specifies using only original problems\n",
        "df_clean = df[df['original'] == 1].copy()\n",
        "print(f\"\\nAfter keeping only original problems: {df_clean.shape}\")\n",
        "\n",
        "# Step 2: Remove rows with missing skill_id\n",
        "# SAKT requires skill_id to create embeddings\n",
        "df_clean = df_clean.dropna(subset=['skill_id'])\n",
        "print(f\"After removing missing skill_id: {df_clean.shape}\")\n",
        "\n",
        "# Step 3: Convert skill_id to integer (it might be float due to NaN values)\n",
        "df_clean['skill_id'] = df_clean['skill_id'].astype(int)\n",
        "\n",
        "# Step 4: Calculate sequence lengths per student\n",
        "student_seq_lengths = df_clean.groupby('user_id').size()\n",
        "\n",
        "# Step 5: Keep only students with >= 0 attempts\n",
        "# Too few attempts don't provide enough learning history\n",
        "valid_students = student_seq_lengths[student_seq_lengths >= 0].index\n",
        "df_clean = df_clean[df_clean['user_id'].isin(valid_students)]\n",
        "print(f\"After removing students with <5 attempts: {df_clean.shape}\")\n",
        "\n",
        "# Step 6: Sort by user_id and order_id (temporal order)\n",
        "df_clean = df_clean.sort_values(['user_id', 'order_id'])\n",
        "\n",
        "print(f\"\\nFinal clean dataset:\")\n",
        "print(f\"- Total interactions: {len(df_clean)}\")\n",
        "print(f\"- Unique students: {df_clean['user_id'].nunique()}\")\n",
        "print(f\"- Unique skills: {df_clean['skill_id'].nunique()}\")\n",
        "print(f\"- Average correct rate: {df_clean['correct'].mean():.2%}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gLCecolb7fwX",
        "outputId": "d96301c1-984d-46a6-a50d-550bde639858"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting data preprocessing...\n",
            "Original data shape: (346860, 31)\n",
            "\n",
            "After keeping only original problems: (275458, 31)\n",
            "After removing missing skill_id: (259399, 31)\n",
            "After removing students with <5 attempts: (259399, 31)\n",
            "\n",
            "Final clean dataset:\n",
            "- Total interactions: 259399\n",
            "- Unique students: 4163\n",
            "- Unique skills: 145\n",
            "- Average correct rate: 65.80%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-42-713116771.py:17: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df_clean['skill_id'] = df_clean['skill_id'].astype(int)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CELL 8: Transform Data into SAKT Input Format"
      ],
      "metadata": {
        "id": "D9i4en2K7iJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 8: Transform Data into SAKT Input Format\n",
        "\n",
        "# Get unique skills and create mapping\n",
        "unique_skills = sorted(df_clean['skill_id'].unique())\n",
        "num_skills = len(unique_skills)\n",
        "\n",
        "# Create skill_id to index mapping (0 to num_skills-1)\n",
        "skill_to_idx = {skill: idx for idx, skill in enumerate(unique_skills)}\n",
        "\n",
        "print(f\"Number of unique skills: {num_skills}\")\n",
        "print(f\"Skill IDs range: {min(unique_skills)} to {max(unique_skills)}\")\n",
        "\n",
        "# Function to create sequences for each student\n",
        "def create_student_sequences(df_clean, skill_to_idx, num_skills):\n",
        "    \"\"\"\n",
        "    Convert student interactions into SAKT format:\n",
        "    - interaction = skill_idx + (correct * num_skills)\n",
        "    \"\"\"\n",
        "    sequences = []\n",
        "\n",
        "    # Process each student\n",
        "    for user_id, user_data in df_clean.groupby('user_id'):\n",
        "        # Get student's attempt history\n",
        "        skills = user_data['skill_id'].values\n",
        "        corrects = user_data['correct'].values\n",
        "\n",
        "        # Convert skill_id to indices\n",
        "        skill_indices = [skill_to_idx[skill] for skill in skills]\n",
        "\n",
        "        # Create interaction sequence (SAKT encoding)\n",
        "        # interaction = skill_index + (correct * num_skills)\n",
        "        interactions = []\n",
        "        for skill_idx, correct in zip(skill_indices, corrects):\n",
        "            interaction = skill_idx + (correct * num_skills)\n",
        "            interactions.append(interaction)\n",
        "\n",
        "        sequences.append({\n",
        "            'user_id': user_id,\n",
        "            'skill_indices': skill_indices,\n",
        "            'corrects': corrects,\n",
        "            'interactions': interactions,\n",
        "            'length': len(interactions)\n",
        "        })\n",
        "\n",
        "    return sequences\n",
        "\n",
        "# Create sequences\n",
        "print(\"\\nCreating student sequences...\")\n",
        "sequences = create_student_sequences(df_clean, skill_to_idx, num_skills)\n",
        "\n",
        "# Show example sequence\n",
        "print(f\"\\nExample sequence (first student):\")\n",
        "example = sequences[0]\n",
        "print(f\"User ID: {example['user_id']}\")\n",
        "print(f\"Sequence length: {example['length']}\")\n",
        "print(f\"First 5 skills attempted: {example['skill_indices'][:5]}\")\n",
        "print(f\"First 5 responses (0=wrong, 1=correct): {example['corrects'][:5]}\")\n",
        "print(f\"First 5 interaction encodings: {example['interactions'][:5]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "osY3ties7k2s",
        "outputId": "535d5e1e-518f-4664-b431-32e1b1028c77"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of unique skills: 145\n",
            "Skill IDs range: 1 to 173190193221\n",
            "\n",
            "Creating student sequences...\n",
            "\n",
            "Example sequence (first student):\n",
            "User ID: 14\n",
            "Sequence length: 19\n",
            "First 5 skills attempted: [132, 132, 132, 132, 132]\n",
            "First 5 responses (0=wrong, 1=correct): [0 1 0 0 0]\n",
            "First 5 interaction encodings: [np.int64(132), np.int64(277), np.int64(132), np.int64(132), np.int64(132)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "HPht-lxO7mZO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CELL 9: Split Data for Training (Student-Level Split)"
      ],
      "metadata": {
        "id": "ZW62OMAt7oKR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 9: Split Data for Training (Student-Level Split)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split at student level (not interaction level) to prevent data leakage\n",
        "# Each student's full sequence goes into either train, val, or test\n",
        "\n",
        "# Use 80/20 split as in the paper (no separate validation)\n",
        "train_sequences, test_sequences = train_test_split(\n",
        "    sequences,\n",
        "    test_size=0.2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# For hyperparameter tuning, create a small validation set from training\n",
        "#train_sequences, val_sequences = train_test_split(\n",
        "#    train_sequences,\n",
        "#    test_size=0.1,  # 10% of training data for validation\n",
        "#    random_state=42\n",
        "#)\n",
        "\n",
        "print(f\"Dataset splits (matching paper):\")\n",
        "print(f\"- Train: {len(train_sequences)} students (80%)\")\n",
        "#print(f\"- Val: {len(val_sequences)} students\")\n",
        "print(f\"- Test: {len(test_sequences)} students (20%)\")\n",
        "\n",
        "# Calculate total interactions per split\n",
        "train_interactions = sum(seq['length'] for seq in train_sequences)\n",
        "#val_interactions = sum(seq['length'] for seq in val_sequences)\n",
        "test_interactions = sum(seq['length'] for seq in test_sequences)\n",
        "\n",
        "print(f\"\\nTotal interactions per split:\")\n",
        "print(f\"- Train: {train_interactions:,}\")\n",
        "#print(f\"- Val: {val_interactions:,}\")\n",
        "print(f\"- Test: {test_interactions:,}\")\n",
        "\n",
        "# Save the processed data\n",
        "import pickle\n",
        "\n",
        "save_data = {\n",
        "    'train': train_sequences,\n",
        "    'val': train_sequences, #change to val_sequences when you want to create a validation data. currently I am using trainin data as validation data for compatibility with the theSAKT paper\n",
        "    'test': test_sequences,\n",
        "    'num_skills': num_skills,\n",
        "    'skill_to_idx': skill_to_idx\n",
        "}\n",
        "\n",
        "with open('sakt_preprocessed_data.pkl', 'wb') as f:\n",
        "    pickle.dump(save_data, f)\n",
        "\n",
        "print(\"\\nData preprocessing complete! Saved to 'sakt_preprocessed_data.pkl'\")"
      ],
      "metadata": {
        "id": "eV3IUXSR7qwt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a768b5b7-fdd8-49f9-80a2-20e9c1b325d5"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset splits (matching paper):\n",
            "- Train: 3330 students (80%)\n",
            "- Test: 833 students (20%)\n",
            "\n",
            "Total interactions per split:\n",
            "- Train: 206,654\n",
            "- Test: 52,745\n",
            "\n",
            "Data preprocessing complete! Saved to 'sakt_preprocessed_data.pkl'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CELL 10: SAKT Model Implementation"
      ],
      "metadata": {
        "id": "JzMvcACxEkYx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 10: SAKT Model Implementation\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F #importing the functional module from PyTorch's torch.nn package, and giving it a shorter alias: F.\n",
        "import math\n",
        "\n",
        "class SAKT(nn.Module): # Class SAKT model that inherits nn.module\n",
        "    def __init__(self, num_skills, embed_dim=100, num_heads=5, dropout=0.2):\n",
        "        \"\"\"\n",
        "        SAKT Model matching the paper implementation\n",
        "\n",
        "        Args:\n",
        "            num_skills: Number of unique skills (145 in this case)\n",
        "            embed_dim: Embedding dimension (paper uses 128)\n",
        "            num_heads: Number of attention heads (paper uses 5)\n",
        "            dropout: Dropout rate (paper uses 0.2)\n",
        "        \"\"\"\n",
        "        super(SAKT, self).__init__()\n",
        "\n",
        "        self.num_skills = num_skills\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "        # Embedding layers\n",
        "        # Interaction embedding: Maps (skill + correct*num_skills) to vectors\n",
        "        self.interaction_embed = nn.Embedding(\n",
        "            num_skills * 2 + 1,  # *2 because skill + correct*num_skills\n",
        "            embed_dim,\n",
        "            padding_idx=num_skills * 2\n",
        "        )\n",
        "\n",
        "        # Exercise/skill embedding: Maps skills to vectors for queries\n",
        "        self.skill_embed = nn.Embedding(\n",
        "            num_skills + 1,\n",
        "            embed_dim,\n",
        "            padding_idx=0\n",
        "            )\n",
        "\n",
        "        # Positional embedding: Adds temporal information\n",
        "        self.pos_embed = nn.Embedding(1000, embed_dim)  # max sequence length 1000\n",
        "\n",
        "        # Multi-head attention layer\n",
        "        self.attention = nn.MultiheadAttention(\n",
        "            embed_dim,\n",
        "            num_heads,\n",
        "            dropout=dropout,\n",
        "            batch_first=True  # Important: batch dimension first\n",
        "        )\n",
        "\n",
        "        # Layer normalization\n",
        "        self.layer_norm1 = nn.LayerNorm(embed_dim)\n",
        "        self.layer_norm2 = nn.LayerNorm(embed_dim)\n",
        "\n",
        "        # Feed-forward network\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(embed_dim, embed_dim * 4),  # Paper uses 4x hidden size\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(embed_dim * 4, embed_dim)\n",
        "        )\n",
        "\n",
        "        # Output prediction layer\n",
        "        self.pred = nn.Linear(embed_dim, 1)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, interactions, skills):\n",
        "        \"\"\"\n",
        "        Forward pass of SAKT\n",
        "\n",
        "        Args:\n",
        "            interactions: [batch_size, seq_len] - past interactions (skill + correct*num_skills)\n",
        "            skills: [batch_size, seq_len] - skills to predict performance on\n",
        "\n",
        "        Returns:\n",
        "            predictions: [batch_size, seq_len] - probability of correct answer\n",
        "        \"\"\"\n",
        "        batch_size, seq_len = interactions.shape\n",
        "\n",
        "        # Create position indices\n",
        "        positions = torch.arange(seq_len, device=interactions.device).unsqueeze(0).expand(batch_size, -1)\n",
        "\n",
        "        # Embed interactions (for Keys and Values in attention)\n",
        "        interaction_embeds = self.interaction_embed(interactions)  # [batch, seq_len, embed_dim]\n",
        "\n",
        "        # Embed skills (for Queries in attention)\n",
        "        skill_embeds = self.skill_embed(skills)  # [batch, seq_len, embed_dim]\n",
        "\n",
        "        # Add positional embeddings\n",
        "        interaction_embeds = interaction_embeds + self.pos_embed(positions)\n",
        "        skill_embeds = skill_embeds + self.pos_embed(positions)\n",
        "\n",
        "        # Create attention mask (causal mask - can't see future)\n",
        "        attn_mask = torch.triu(\n",
        "            torch.ones(seq_len, seq_len, device=interactions.device) * float('-inf'),\n",
        "            diagonal=1\n",
        "        )\n",
        "\n",
        "        # Apply self-attention\n",
        "        # Query: what skill we're predicting\n",
        "        # Key & Value: past interaction history\n",
        "        attended, _ = self.attention(\n",
        "            query=skill_embeds,\n",
        "            key=interaction_embeds,\n",
        "            value=interaction_embeds,\n",
        "            attn_mask=attn_mask,\n",
        "            need_weights=False\n",
        "        )\n",
        "\n",
        "        # Residual connection and layer norm\n",
        "        attended = self.layer_norm1(skill_embeds + self.dropout(attended))\n",
        "\n",
        "        # Feed-forward network with residual\n",
        "        ffn_out = self.ffn(attended)\n",
        "        ffn_out = self.layer_norm2(attended + self.dropout(ffn_out))\n",
        "\n",
        "        # Predict probability of correct answer\n",
        "        pred = self.pred(ffn_out).squeeze(-1)  # [batch, seq_len]\n",
        "        return torch.sigmoid(pred)\n",
        "\n",
        "# Test the model\n",
        "print(\"Testing SAKT model...\")\n",
        "model = SAKT(num_skills=145)\n",
        "\n",
        "# Create dummy batch\n",
        "batch_interactions = torch.randint(0, 290, (2, 50))  # 2 sequences, length 50\n",
        "batch_skills = torch.randint(0, 145, (2, 50))\n",
        "\n",
        "# Forward pass\n",
        "output = model(batch_interactions, batch_skills)\n",
        "print(f\"Model output shape: {output.shape}\")\n",
        "print(f\"Output range: [{output.min():.3f}, {output.max():.3f}]\")\n",
        "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")"
      ],
      "metadata": {
        "id": "YYZUAsjHEmZQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e54c1a19-762e-44d4-b9fd-68e9aab53ac6"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing SAKT model...\n",
            "Model output shape: torch.Size([2, 50])\n",
            "Output range: [0.165, 0.769]\n",
            "Model parameters: 265,101\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CELL 11: Create PyTorch Dataset and DataLoaders"
      ],
      "metadata": {
        "id": "0aIb8Y30EtBq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 11: Create PyTorch Dataset and DataLoaders\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "\n",
        "class SAKTDataset(Dataset):\n",
        "    \"\"\"Dataset class for SAKT - Corrected version\"\"\"\n",
        "\n",
        "    def __init__(self, sequences, max_seq_len=100, num_skills=145):\n",
        "        self.max_seq_len = max_seq_len\n",
        "        self.num_skills = num_skills\n",
        "        self.padding_interaction = num_skills * 2  # Padding token\n",
        "\n",
        "        # Process sequences: split long ones\n",
        "        self.data = []\n",
        "        for seq in sequences:\n",
        "            interactions = seq['interactions']\n",
        "            skills = seq['skill_indices']\n",
        "            corrects = seq['corrects']\n",
        "\n",
        "            # If sequence is longer than max_seq_len, split it\n",
        "            if len(interactions) > max_seq_len:\n",
        "                for i in range(0, len(interactions), max_seq_len):\n",
        "                    end_idx = min(i + max_seq_len, len(interactions))\n",
        "                    self.data.append({\n",
        "                        'interactions': interactions[i:end_idx],\n",
        "                        'skills': skills[i:end_idx],\n",
        "                        'corrects': corrects[i:end_idx]\n",
        "                    })\n",
        "            else:\n",
        "                self.data.append({\n",
        "                    'interactions': interactions,\n",
        "                    'skills': skills,\n",
        "                    'corrects': corrects\n",
        "                })\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        seq = self.data[idx]\n",
        "        seq_len = len(seq['interactions'])\n",
        "\n",
        "        interactions = list(seq['interactions'])\n",
        "        skills = list(seq['skills'])\n",
        "        corrects = list(seq['corrects'])\n",
        "\n",
        "        shifted_interactions = []\n",
        "        for i in range(seq_len):\n",
        "            if i == 0:\n",
        "                # First position: use a special START token (same as padding)\n",
        "                shifted_interactions.append(self.padding_interaction)\n",
        "            else:\n",
        "                # Use the PREVIOUS interaction\n",
        "                shifted_interactions.append(interactions[i-1])\n",
        "\n",
        "        # Use shifted_interactions instead of interactions\n",
        "        interactions = shifted_interactions\n",
        "\n",
        "        # Pad to the LEFT if sequence is shorter than max_seq_len\n",
        "        if seq_len < self.max_seq_len:\n",
        "            pad_len = self.max_seq_len - seq_len\n",
        "\n",
        "            # Pad to the LEFT\n",
        "            interactions = [self.padding_interaction] * pad_len + interactions\n",
        "            skills = [0] * pad_len + skills  # 0 for padding\n",
        "            corrects = [0] * pad_len + corrects\n",
        "\n",
        "            # Create mask (0 for padding, 1 for real data)\n",
        "            mask = [0] * pad_len + [1] * seq_len\n",
        "        else:\n",
        "            mask = [1] * self.max_seq_len\n",
        "\n",
        "        return {\n",
        "            'interactions': torch.tensor(interactions, dtype=torch.long),\n",
        "            'skills': torch.tensor(skills, dtype=torch.long),\n",
        "            'targets': torch.tensor(corrects, dtype=torch.float),\n",
        "            'mask': torch.tensor(mask, dtype=torch.float)\n",
        "        }\n",
        "\n",
        "# Load preprocessed data\n",
        "import pickle\n",
        "with open('sakt_preprocessed_data.pkl', 'rb') as f:\n",
        "    data = pickle.load(f)\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = SAKTDataset(data['train'], max_seq_len=100, num_skills=data['num_skills'])\n",
        "val_dataset = SAKTDataset(data['val'], max_seq_len=100, num_skills=data['num_skills'])\n",
        "test_dataset = SAKTDataset(data['test'], max_seq_len=100, num_skills=data['num_skills'])\n",
        "\n",
        "# Create dataloaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "# Test dataloader\n",
        "batch = next(iter(train_loader))\n",
        "print(f\"Batch keys: {batch.keys()}\")\n",
        "print(f\"Interactions shape: {batch['interactions'].shape}\")\n",
        "print(f\"Skills shape: {batch['skills'].shape}\")\n",
        "print(f\"Targets shape: {batch['targets'].shape}\")\n",
        "print(f\"Mask shape: {batch['mask'].shape}\")"
      ],
      "metadata": {
        "id": "RnIpiDMGEvyu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6692c7f3-8ba9-4440-fe76-a06dcfc01452"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch keys: dict_keys(['interactions', 'skills', 'targets', 'mask'])\n",
            "Interactions shape: torch.Size([64, 100])\n",
            "Skills shape: torch.Size([64, 100])\n",
            "Targets shape: torch.Size([64, 100])\n",
            "Mask shape: torch.Size([64, 100])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "G3iDyuwvE2GR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CELL 12: Test Data Loaders and Verify Data Format"
      ],
      "metadata": {
        "id": "ZBUnkJ5uE4Ox"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 12: Test Data Loaders and Verify Data Format\n",
        "\n",
        "import pickle\n",
        "\n",
        "# Load the preprocessed data we saved in Cell 9\n",
        "# 'rb' means read in binary mode (pickle files are binary)\n",
        "with open('sakt_preprocessed_data.pkl', 'rb') as f:\n",
        "    data = pickle.load(f)  # Converts file back to Python dictionary\n",
        "\n",
        "print(f\"Loaded data with {data['num_skills']} unique skills\")\n",
        "\n",
        "# Create PyTorch datasets from the sequences\n",
        "# SAKTDataset handles padding and creating input/target pairs\n",
        "train_dataset = SAKTDataset(data['train'], max_seq_len=100)\n",
        "val_dataset = SAKTDataset(data['val'], max_seq_len=100)\n",
        "test_dataset = SAKTDataset(data['test'], max_seq_len=100)\n",
        "\n",
        "print(f\"\\nDataset sizes:\")\n",
        "print(f\"Train: {len(train_dataset)} students\")\n",
        "print(f\"Val: {len(val_dataset)} students\")\n",
        "print(f\"Test: {len(test_dataset)} students\")\n",
        "\n",
        "# Create data loaders that will feed batches to my model\n",
        "# batch_size=64 means process 64 students at once\n",
        "# shuffle=True randomizes order each epoch (important for training)\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "# Test by loading one batch to verify everything works\n",
        "print(f\"\\nTesting data loader with one batch:\")\n",
        "batch = next(iter(train_loader))  # Get first batch\n",
        "\n",
        "# Check what's in each batch\n",
        "print(f\"Batch contains: {list(batch.keys())}\")\n",
        "print(f\"Interactions shape: {batch['interactions'].shape}\")  # [64, 99]\n",
        "print(f\"Skills shape: {batch['skills'].shape}\")              # [64, 99]\n",
        "print(f\"Targets shape: {batch['targets'].shape}\")            # [64, 99]\n",
        "print(f\"Mask shape: {batch['mask'].shape}\")                  # [64, 99]\n",
        "\n",
        "# Look at one student's data to understand format\n",
        "print(f\"\\nExample from first student in batch:\")\n",
        "first_seq_len = batch['mask'][0].sum().int()  # Count non-padded positions\n",
        "print(f\"Actual sequence length: {first_seq_len}\")\n",
        "print(f\"First 5 interactions: {batch['interactions'][0][:5].tolist()}\")\n",
        "print(f\"First 5 skills to predict: {batch['skills'][0][:5].tolist()}\")\n",
        "print(f\"First 5 correct/incorrect: {batch['targets'][0][:5].tolist()}\")\n",
        "print(f\"First 5 mask values: {batch['mask'][0][:5].tolist()}\")  # 1=real, 0=padding"
      ],
      "metadata": {
        "id": "Oho9RKT4E5QD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a9e6fa3-bd65-499e-b87f-acc64db74106"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded data with 145 unique skills\n",
            "\n",
            "Dataset sizes:\n",
            "Train: 4534 students\n",
            "Val: 4534 students\n",
            "Test: 1134 students\n",
            "\n",
            "Testing data loader with one batch:\n",
            "Batch contains: ['interactions', 'skills', 'targets', 'mask']\n",
            "Interactions shape: torch.Size([64, 100])\n",
            "Skills shape: torch.Size([64, 100])\n",
            "Targets shape: torch.Size([64, 100])\n",
            "Mask shape: torch.Size([64, 100])\n",
            "\n",
            "Example from first student in batch:\n",
            "Actual sequence length: 21\n",
            "First 5 interactions: [290, 290, 290, 290, 290]\n",
            "First 5 skills to predict: [0, 0, 0, 0, 0]\n",
            "First 5 correct/incorrect: [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "First 5 mask values: [0.0, 0.0, 0.0, 0.0, 0.0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CELL 13: Training Functions for SAKT"
      ],
      "metadata": {
        "id": "KSAwA5DuE7nl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 13: Training Functions for SAKT\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def train_epoch(model, train_loader, optimizer, criterion, device):\n",
        "    \"\"\"\n",
        "    Train the model for one epoch\n",
        "\n",
        "    Args:\n",
        "        model: SAKT model\n",
        "        train_loader: DataLoader with training data\n",
        "        optimizer: Adam optimizer\n",
        "        criterion: BCELoss function\n",
        "        device: cuda or cpu\n",
        "\n",
        "    Returns:\n",
        "        epoch_loss: Average loss for this epoch\n",
        "        epoch_auc: AUC score for this epoch\n",
        "    \"\"\"\n",
        "    model.train()  # Enable dropout and batch norm training behavior\n",
        "    total_loss = 0\n",
        "    all_predictions = []\n",
        "    all_targets = []\n",
        "\n",
        "    # Progress bar to track training\n",
        "    pbar = tqdm(train_loader, desc='Training', leave=False)\n",
        "\n",
        "    for batch in pbar:\n",
        "        # Move all tensors to GPU if available\n",
        "        interactions = batch['interactions'].to(device)\n",
        "        skills = batch['skills'].to(device)\n",
        "        targets = batch['targets'].to(device)\n",
        "        mask = batch['mask'].to(device)\n",
        "\n",
        "        # Clear gradients from previous batch\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass: get predictions from model\n",
        "        predictions = model(interactions, skills)\n",
        "\n",
        "        # Calculate loss only on non-padded positions\n",
        "        # criterion returns loss for each position\n",
        "        # Compute loss with masking\n",
        "        loss = criterion(predictions, targets)  # This gives loss per element\n",
        "        masked_loss = (loss * mask).sum() / mask.sum()  # Apply mask and average\n",
        "\n",
        "\n",
        "        # Backward pass: compute gradients\n",
        "        masked_loss.backward()\n",
        "\n",
        "        # Clip gradients to prevent explosion\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "        # Update model parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        # Collect predictions for metrics\n",
        "        total_loss += masked_loss.item()\n",
        "\n",
        "        # Extract only valid (non-padded) predictions\n",
        "        valid_idx = mask == 1\n",
        "        valid_predictions = predictions[valid_idx].detach().cpu().numpy()\n",
        "        valid_targets = targets[valid_idx].detach().cpu().numpy()\n",
        "\n",
        "        all_predictions.extend(valid_predictions)\n",
        "        all_targets.extend(valid_targets)\n",
        "\n",
        "        # Update progress bar with current loss\n",
        "        pbar.set_postfix({'loss': f'{masked_loss.item():.4f}'})\n",
        "\n",
        "    # Calculate epoch metrics\n",
        "    epoch_loss = total_loss / len(train_loader)\n",
        "    epoch_auc = roc_auc_score(all_targets, all_predictions)\n",
        "\n",
        "    return epoch_loss, epoch_auc\n",
        "\n",
        "def validate_epoch(model, val_loader, criterion, device):\n",
        "    \"\"\"\n",
        "    Validate the model (no gradient updates)\n",
        "\n",
        "    Args:\n",
        "        model: SAKT model\n",
        "        val_loader: DataLoader with validation data\n",
        "        criterion: BCELoss function\n",
        "        device: cuda or cpu\n",
        "\n",
        "    Returns:\n",
        "        val_loss: Average validation loss\n",
        "        val_auc: Validation AUC score\n",
        "    \"\"\"\n",
        "    model.eval()  # Disable dropout and use batch norm statistics\n",
        "    total_loss = 0\n",
        "    all_predictions = []\n",
        "    all_targets = []\n",
        "\n",
        "    # No gradient computation needed for validation\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(val_loader, desc='Validating', leave=False):\n",
        "            # Move to device\n",
        "            interactions = batch['interactions'].to(device)\n",
        "            skills = batch['skills'].to(device)\n",
        "            targets = batch['targets'].to(device)\n",
        "            mask = batch['mask'].to(device)\n",
        "\n",
        "            # Get predictions\n",
        "            predictions = model(interactions, skills)\n",
        "\n",
        "            # Calculate loss\n",
        "            # Compute loss with masking\n",
        "            loss = criterion(predictions, targets)  # This gives loss per element\n",
        "            masked_loss = (loss * mask).sum() / mask.sum()  # Apply mask and average\n",
        "\n",
        "            # Collect for metrics\n",
        "            total_loss += masked_loss.item()\n",
        "\n",
        "            # Extract valid predictions\n",
        "            valid_idx = mask == 1\n",
        "            valid_predictions = predictions[valid_idx].cpu().numpy()\n",
        "            valid_targets = targets[valid_idx].cpu().numpy()\n",
        "\n",
        "            all_predictions.extend(valid_predictions)\n",
        "            all_targets.extend(valid_targets)\n",
        "\n",
        "    # Calculate validation metrics\n",
        "    val_loss = total_loss / len(val_loader)\n",
        "    val_auc = roc_auc_score(all_targets, all_predictions)\n",
        "\n",
        "    return val_loss, val_auc"
      ],
      "metadata": {
        "id": "y6BM04A2E-iF"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CELL 14: Train SAKT Model"
      ],
      "metadata": {
        "id": "uvoiybrsFCFk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 14: Train SAKT Model\n",
        "\n",
        "# Check if GPU is available and use it\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "if device.type == 'cuda':\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "\n",
        "# Initialize model with paper's hyperparameters\n",
        "model = SAKT(\n",
        "    num_skills=145,  # From my dataset analysis\n",
        "    embed_dim=10,   # Paper: d=[50, 100, 150, 200]\n",
        "    num_heads=5,     # Paper: h=5\n",
        "    dropout=0.2      # Paper: dropout=0.2\n",
        ").to(device)\n",
        "\n",
        "# Count model parameters\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"Total parameters: {total_params:,}\")\n",
        "print(f\"Trainable parameters: {trainable_params:,}\")\n",
        "\n",
        "# Initialize optimizer (Adam with paper's learning rate)\n",
        "optimizer = optim.Adam(\n",
        "    model.parameters(),\n",
        "    lr=0.001,           # Paper uses 0.001\n",
        "    weight_decay=0.0001  # L2 regularization\n",
        ")\n",
        "\n",
        "# Loss function for binary classification\n",
        "# reduction='none' returns loss per element (needed for masking)\n",
        "criterion = nn.BCELoss(reduction='none')\n",
        "\n",
        "\n",
        "# Training configuration\n",
        "NUM_EPOCHS = 50\n",
        "#best_val_auc = 0\n",
        "\n",
        "# Track metrics for visualization\n",
        "history = {\n",
        "    'train_loss': [], 'train_auc': [],\n",
        "#    'val_loss': [], 'val_auc': []\n",
        "}\n",
        "\n",
        "print(f\"\\nStarting training for max {NUM_EPOCHS} epochs...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Main training loop\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    print(f\"\\nEpoch {epoch+1}/{NUM_EPOCHS}\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    # Train for one epoch\n",
        "    train_loss, train_auc = train_epoch(\n",
        "        model, train_loader, optimizer, criterion, device\n",
        "    )\n",
        "\n",
        "    # Validate\n",
        "    #val_loss, val_auc = validate_epoch(\n",
        "    #    model, val_loader, criterion, device\n",
        "    #)\n",
        "\n",
        "    # Store history\n",
        "    history['train_loss'].append(train_loss)\n",
        "    history['train_auc'].append(train_auc)\n",
        "    #history['val_loss'].append(val_loss)\n",
        "    #history['val_auc'].append(val_auc)\n",
        "\n",
        "    # Print metrics\n",
        "    print(f\"Train - Loss: {train_loss:.4f}, AUC: {train_auc:.4f}\")\n",
        "    #print(f\"Valid - Loss: {val_loss:.4f}, AUC: {val_auc:.4f}\")\n",
        "\n",
        "    # Save best model\n",
        "    #if val_auc > best_val_auc:\n",
        "    #    best_val_auc = val_auc\n",
        "\n",
        "    if epoch + 1 == NUM_EPOCHS:\n",
        "        # Save checkpoint\n",
        "        checkpoint = {\n",
        "            'epoch': epoch + 1,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            #'best_val_auc': best_val_auc,\n",
        "            'train_auc': train_auc,\n",
        "            'history': history\n",
        "        }\n",
        "        torch.save(checkpoint, 'best_sakt_model.pth')\n",
        "        #print(f\"âœ“ New best model saved! (AUC: {best_val_auc:.4f})\")\n",
        "        print(f\"âœ“ Final model saved! (Train AUC: {train_auc:.4f})\")\n",
        "\n",
        "print(f\"\\nTraining complete!\")\n",
        "#print(f\"Best validation AUC: {best_val_auc:.4f}\")\n",
        "print(f\"Final training AUC: {train_auc:.4f}\")"
      ],
      "metadata": {
        "id": "jqPTIgbjFC37",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a679edc2-d84e-4169-b9ce-bb751337e10f"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu\n",
            "Total parameters: 770,201\n",
            "Trainable parameters: 770,201\n",
            "\n",
            "Starting training for max 100 epochs...\n",
            "============================================================\n",
            "\n",
            "Epoch 1/100\n",
            "----------------------------------------\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train - Loss: 0.6074, AUC: 0.6555\n",
            "\n",
            "Epoch 2/100\n",
            "----------------------------------------\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train - Loss: 0.5670, AUC: 0.7242\n",
            "\n",
            "Epoch 3/100\n",
            "----------------------------------------\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train - Loss: 0.5531, AUC: 0.7421\n",
            "\n",
            "Epoch 4/100\n",
            "----------------------------------------\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train - Loss: 0.5436, AUC: 0.7535\n",
            "\n",
            "Epoch 5/100\n",
            "----------------------------------------\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train - Loss: 0.5381, AUC: 0.7595\n",
            "\n",
            "Epoch 6/100\n",
            "----------------------------------------\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train - Loss: 0.5331, AUC: 0.7667\n",
            "\n",
            "Epoch 7/100\n",
            "----------------------------------------\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train - Loss: 0.5284, AUC: 0.7715\n",
            "\n",
            "Epoch 8/100\n",
            "----------------------------------------\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train - Loss: 0.5244, AUC: 0.7754\n",
            "\n",
            "Epoch 9/100\n",
            "----------------------------------------\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train - Loss: 0.5205, AUC: 0.7796\n",
            "\n",
            "Epoch 10/100\n",
            "----------------------------------------\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train - Loss: 0.5178, AUC: 0.7830\n",
            "\n",
            "Epoch 11/100\n",
            "----------------------------------------\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train - Loss: 0.5158, AUC: 0.7847\n",
            "\n",
            "Epoch 12/100\n",
            "----------------------------------------\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train - Loss: 0.5131, AUC: 0.7879\n",
            "\n",
            "Epoch 13/100\n",
            "----------------------------------------\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train - Loss: 0.5108, AUC: 0.7900\n",
            "\n",
            "Epoch 14/100\n",
            "----------------------------------------\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train - Loss: 0.5098, AUC: 0.7904\n",
            "\n",
            "Epoch 15/100\n",
            "----------------------------------------\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train - Loss: 0.5076, AUC: 0.7933\n",
            "\n",
            "Epoch 16/100\n",
            "----------------------------------------\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train - Loss: 0.5070, AUC: 0.7934\n",
            "\n",
            "Epoch 17/100\n",
            "----------------------------------------\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train - Loss: 0.5055, AUC: 0.7949\n",
            "\n",
            "Epoch 18/100\n",
            "----------------------------------------\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train - Loss: 0.5054, AUC: 0.7948\n",
            "\n",
            "Epoch 19/100\n",
            "----------------------------------------\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train - Loss: 0.5051, AUC: 0.7951\n",
            "\n",
            "Epoch 20/100\n",
            "----------------------------------------\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train - Loss: 0.5056, AUC: 0.7952\n",
            "\n",
            "Epoch 21/100\n",
            "----------------------------------------\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train - Loss: 0.5041, AUC: 0.7966\n",
            "\n",
            "Epoch 22/100\n",
            "----------------------------------------\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train - Loss: 0.5038, AUC: 0.7970\n",
            "\n",
            "Epoch 23/100\n",
            "----------------------------------------\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train - Loss: 0.5036, AUC: 0.7969\n",
            "\n",
            "Epoch 24/100\n",
            "----------------------------------------\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train - Loss: 0.5037, AUC: 0.7969\n",
            "\n",
            "Epoch 25/100\n",
            "----------------------------------------\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train - Loss: 0.5042, AUC: 0.7966\n",
            "\n",
            "Epoch 26/100\n",
            "----------------------------------------\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train - Loss: 0.5036, AUC: 0.7966\n",
            "\n",
            "Epoch 27/100\n",
            "----------------------------------------\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train - Loss: 0.5040, AUC: 0.7966\n",
            "\n",
            "Epoch 28/100\n",
            "----------------------------------------\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train - Loss: 0.5039, AUC: 0.7963\n",
            "\n",
            "Epoch 29/100\n",
            "----------------------------------------\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train - Loss: 0.5040, AUC: 0.7962\n",
            "\n",
            "Epoch 30/100\n",
            "----------------------------------------\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train - Loss: 0.5036, AUC: 0.7964\n",
            "\n",
            "Epoch 31/100\n",
            "----------------------------------------\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train - Loss: 0.5051, AUC: 0.7954\n",
            "\n",
            "Epoch 32/100\n",
            "----------------------------------------\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train - Loss: 0.5072, AUC: 0.7935\n",
            "\n",
            "Epoch 33/100\n",
            "----------------------------------------\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train - Loss: 0.5048, AUC: 0.7951\n",
            "\n",
            "Epoch 34/100\n",
            "----------------------------------------\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train - Loss: 0.5056, AUC: 0.7942\n",
            "\n",
            "Epoch 35/100\n",
            "----------------------------------------\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train - Loss: 0.5055, AUC: 0.7946\n",
            "\n",
            "Epoch 36/100\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train - Loss: 0.5051, AUC: 0.7950\n",
            "\n",
            "Epoch 37/100\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train - Loss: 0.5066, AUC: 0.7933\n",
            "\n",
            "Epoch 38/100\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train - Loss: 0.5072, AUC: 0.7929\n",
            "\n",
            "Epoch 39/100\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train - Loss: 0.5068, AUC: 0.7933\n",
            "\n",
            "Epoch 40/100\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train - Loss: 0.5075, AUC: 0.7926\n",
            "\n",
            "Epoch 41/100\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train - Loss: 0.5080, AUC: 0.7921\n",
            "\n",
            "Epoch 42/100\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train - Loss: 0.5091, AUC: 0.7911\n",
            "\n",
            "Epoch 43/100\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train - Loss: 0.5087, AUC: 0.7910\n",
            "\n",
            "Epoch 44/100\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train - Loss: 0.5089, AUC: 0.7911\n",
            "\n",
            "Epoch 45/100\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train - Loss: 0.5095, AUC: 0.7909\n",
            "\n",
            "Epoch 46/100\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train - Loss: 0.5120, AUC: 0.7882\n",
            "\n",
            "Epoch 47/100\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train - Loss: 0.5096, AUC: 0.7907\n",
            "\n",
            "Epoch 48/100\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train - Loss: 0.5100, AUC: 0.7904\n",
            "\n",
            "Epoch 49/100\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train - Loss: 0.5107, AUC: 0.7896\n",
            "\n",
            "Epoch 50/100\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train - Loss: 0.5102, AUC: 0.7898\n",
            "\n",
            "Epoch 51/100\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train - Loss: 0.5104, AUC: 0.7894\n",
            "\n",
            "Epoch 52/100\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train - Loss: 0.5107, AUC: 0.7898\n",
            "\n",
            "Epoch 53/100\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train - Loss: 0.5116, AUC: 0.7883\n",
            "\n",
            "Epoch 54/100\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train - Loss: 0.5113, AUC: 0.7883\n",
            "\n",
            "Epoch 55/100\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train - Loss: 0.5104, AUC: 0.7892\n",
            "\n",
            "Epoch 56/100\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train - Loss: 0.5109, AUC: 0.7890\n",
            "\n",
            "Epoch 57/100\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train - Loss: 0.5115, AUC: 0.7887\n",
            "\n",
            "Epoch 58/100\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train - Loss: 0.5104, AUC: 0.7894\n",
            "\n",
            "Epoch 59/100\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train - Loss: 0.5110, AUC: 0.7885\n",
            "\n",
            "Epoch 60/100\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train - Loss: 0.5108, AUC: 0.7891\n",
            "\n",
            "Epoch 61/100\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train - Loss: 0.5108, AUC: 0.7886\n",
            "\n",
            "Epoch 62/100\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train - Loss: 0.5106, AUC: 0.7891\n",
            "\n",
            "Epoch 63/100\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train - Loss: 0.5105, AUC: 0.7889\n",
            "\n",
            "Epoch 64/100\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train - Loss: 0.5106, AUC: 0.7892\n",
            "\n",
            "Epoch 65/100\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train - Loss: 0.5109, AUC: 0.7892\n",
            "\n",
            "Epoch 66/100\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train - Loss: 0.5103, AUC: 0.7890\n",
            "\n",
            "Epoch 67/100\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train - Loss: 0.5095, AUC: 0.7903\n",
            "\n",
            "Epoch 68/100\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train - Loss: 0.5105, AUC: 0.7898\n",
            "\n",
            "Epoch 69/100\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train - Loss: 0.5099, AUC: 0.7901\n",
            "\n",
            "Epoch 70/100\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train - Loss: 0.5099, AUC: 0.7899\n",
            "\n",
            "Epoch 71/100\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train - Loss: 0.5088, AUC: 0.7914\n",
            "\n",
            "Epoch 72/100\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train - Loss: 0.5105, AUC: 0.7894\n",
            "\n",
            "Epoch 73/100\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 35/71 [01:11<01:13,  2.03s/it, loss=0.5172]"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CELL 15: Plot Training History"
      ],
      "metadata": {
        "id": "Vp4XSTwZFEzB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 15: Plot Training History\n",
        "\n",
        "# Create figure with two subplots\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "# Subplot 1: Loss curves\n",
        "plt.subplot(1, 2, 1)\n",
        "epochs = range(1, len(history['train_loss']) + 1)\n",
        "plt.plot(epochs, history['train_loss'], 'b-', label='Train Loss', marker='o', markersize=4)\n",
        "plt.plot(epochs, history['val_loss'], 'r-', label='Val Loss', marker='s', markersize=4)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Subplot 2: AUC curves\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs, history['train_auc'], 'b-', label='Train AUC', marker='o', markersize=4)\n",
        "plt.plot(epochs, history['val_auc'], 'r-', label='Val AUC', marker='s', markersize=4)\n",
        "plt.axhline(y=0.82, color='g', linestyle='--', label='Target AUC (0.82)')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('AUC')\n",
        "plt.title('Training and Validation AUC')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print summary statistics\n",
        "print(\"Training Summary:\")\n",
        "print(\"-\" * 40)\n",
        "print(f\"Total epochs trained: {len(history['train_loss'])}\")\n",
        "print(f\"Best validation AUC: {max(history['val_auc']):.4f}\")\n",
        "print(f\"Final train AUC: {history['train_auc'][-1]:.4f}\")\n",
        "print(f\"Final validation AUC: {history['val_auc'][-1]:.4f}\")\n",
        "\n",
        "# Check for overfitting\n",
        "final_gap = history['train_auc'][-1] - history['val_auc'][-1]\n",
        "print(f\"\\nTrain-Val gap: {final_gap:.4f}\")\n",
        "if final_gap > 0.05:\n",
        "    print(\"Model might be overfitting\")\n",
        "else:\n",
        "    print(\"âœ“ No significant overfitting detected\")"
      ],
      "metadata": {
        "id": "QR2gh3sLFHmN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CELL 16: Evaluate on Test Set"
      ],
      "metadata": {
        "id": "-qt2FatlFKDk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 16: Evaluate on Test Set (CORRECTED)\n",
        "\n",
        "# Load best model\n",
        "checkpoint = torch.load('best_sakt_model.pth', weights_only=False)\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "#print(f\"Loaded best model from epoch {checkpoint['epoch']} with val AUC {checkpoint['best_val_auc']:.4f}\")\n",
        "\n",
        "# Evaluate on test set\n",
        "print(\"\\nEvaluating on test set...\")\n",
        "test_loss, test_auc = validate_epoch(model, test_loader, criterion, device)\n",
        "\n",
        "print(f\"\\nTest Set Results:\")\n",
        "print(f\"Test Loss: {test_loss:.4f}\")\n",
        "print(f\"Test AUC: {test_auc:.4f}\")\n",
        "\n",
        "# Compare with validation performance\n",
        "print(f\"\\nGeneralization check:\")\n",
        "#print(f\"Validation AUC: {checkpoint['best_val_auc']:.4f}\")\n",
        "print(f\"Test AUC: {test_auc:.4f}\")\n",
        "#print(f\"Val-Test gap: {abs(checkpoint['best_val_auc'] - test_auc):.4f}\")\n",
        "\n",
        "# Additional metrics\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Get predictions for additional metrics\n",
        "model.eval()\n",
        "all_predictions = []\n",
        "all_targets = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        interactions = batch['interactions'].to(device)\n",
        "        skills = batch['skills'].to(device)\n",
        "        targets = batch['targets'].to(device)\n",
        "        mask = batch['mask'].to(device)\n",
        "\n",
        "        predictions = model(interactions, skills)\n",
        "\n",
        "        # Get actual batch size (last batch might be smaller)\n",
        "        current_batch_size = interactions.shape[0]\n",
        "\n",
        "        # Extract valid predictions for each sequence in batch\n",
        "        for i in range(current_batch_size):\n",
        "            mask_i = mask[i].cpu().numpy()\n",
        "            pred_i = predictions[i].cpu().numpy()\n",
        "            target_i = targets[i].cpu().numpy()\n",
        "\n",
        "            # Only add non-padded values\n",
        "            valid_idx = mask_i == 1\n",
        "            all_predictions.extend(pred_i[valid_idx].tolist())\n",
        "            all_targets.extend(target_i[valid_idx].tolist())\n",
        "\n",
        "# Convert to numpy arrays\n",
        "all_predictions = np.array(all_predictions)\n",
        "all_targets = np.array(all_targets)\n",
        "\n",
        "# Calculate additional metrics using 0.5 threshold\n",
        "binary_predictions = (all_predictions > 0.5).astype(int)\n",
        "\n",
        "accuracy = accuracy_score(all_targets, binary_predictions)\n",
        "precision = precision_score(all_targets, binary_predictions)\n",
        "recall = recall_score(all_targets, binary_predictions)\n",
        "#f1 = f1_score(all_targets, binary_predictions)\n",
        "\n",
        "print(f\"\\nAdditional Test Metrics:\")\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "#print(f\"F1-Score: {f1:.4f}\")"
      ],
      "metadata": {
        "id": "fpUEBkydFMc5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CELL 17: Save Final Model and Results"
      ],
      "metadata": {
        "id": "CohmA6OQFQKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 17: Save Final Model and Results\n",
        "\n",
        "import json\n",
        "from datetime import datetime\n",
        "\n",
        "# Create results summary\n",
        "results = {\n",
        "    'model_name': 'SAKT',\n",
        "    'dataset': 'ASSISTments2009',\n",
        "    'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
        "    'hyperparameters': {\n",
        "        'num_skills': 145,\n",
        "        'embed_dim': 128,\n",
        "        'num_heads': 5,\n",
        "        'dropout': 0.2,\n",
        "        'learning_rate': 0.001,\n",
        "        'batch_size': 64,\n",
        "        'max_seq_len': 100\n",
        "    },\n",
        "    'results': {\n",
        "        'best_val_auc': float(checkpoint['best_val_auc']),\n",
        "        'test_auc': float(test_auc),\n",
        "        'test_accuracy': float(accuracy),\n",
        "        'test_precision': float(precision),\n",
        "        'test_recall': float(recall),\n",
        "        'test_f1': float(f1),\n",
        "        'total_epochs': len(history['train_loss']),\n",
        "        'best_epoch': checkpoint['epoch']\n",
        "    },\n",
        "    'data_stats': {\n",
        "        'train_students': len(data['train']),\n",
        "        'val_students': len(data['val']),\n",
        "        'test_students': len(data['test']),\n",
        "        'unique_skills': data['num_skills']\n",
        "    }\n",
        "}\n",
        "\n",
        "# Save results to JSON\n",
        "with open('sakt_results.json', 'w') as f:\n",
        "    json.dump(results, f, indent=4)\n",
        "\n",
        "# Save final model with all information\n",
        "final_save = {\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'optimizer_state_dict': optimizer.state_dict(),\n",
        "    'results': results,\n",
        "    'history': history,\n",
        "    'skill_to_idx': data['skill_to_idx']\n",
        "}\n",
        "\n",
        "torch.save(final_save, 'sakt_final_model.pth')\n",
        "\n",
        "print(\"Model and results saved!\")\n",
        "print(f\"\\nSummary for thesis:\")\n",
        "print(f\"- SAKT achieved {test_auc:.4f} AUC on ASSISTments2009\")\n",
        "print(f\"- Model has {sum(p.numel() for p in model.parameters()):,} parameters\")\n",
        "print(f\"- Training took {len(history['train_loss'])} epochs\")"
      ],
      "metadata": {
        "id": "K2aDuaUnFSN6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Created this cell to perform a Hyperparameter search for the improving the AUC of the SAKT paper"
      ],
      "metadata": {
        "id": "IsIdj8XHdL5B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL: Hyperparameter Search for SAKT\n",
        "\n",
        "import itertools\n",
        "from datetime import datetime\n",
        "import json\n",
        "\n",
        "# Define hyperparameter search space\n",
        "param_grid = {\n",
        "    'embed_dim': [100, 150, 200],  # Paper tested these\n",
        "    'num_heads': [5],  # Keep at 5 as per paper\n",
        "    'dropout': [0.1, 0.2, 0.3],\n",
        "    'lr': [5e-4, 1e-3, 2e-3],\n",
        "    'weight_decay': [0, 1e-5, 1e-4],\n",
        "    'batch_size': [64, 128],  # Paper mentions 128 for ASSIST2009\n",
        "}\n",
        "\n",
        "# Generate all combinations\n",
        "param_combinations = list(itertools.product(*param_grid.values()))\n",
        "param_names = list(param_grid.keys())\n",
        "\n",
        "print(f\"Total hyperparameter combinations to test: {len(param_combinations)}\")\n",
        "\n",
        "# Function to train with specific hyperparameters\n",
        "def train_with_params(params_dict, max_epochs=20, patience=5):\n",
        "    \"\"\"\n",
        "    Train SAKT with given hyperparameters and return best validation AUC\n",
        "    \"\"\"\n",
        "    # Create model with specified parameters\n",
        "    model = SAKT(\n",
        "        num_skills=145,\n",
        "        embed_dim=params_dict['embed_dim'],\n",
        "        num_heads=params_dict['num_heads'],\n",
        "        dropout=params_dict['dropout']\n",
        "    ).to(device)\n",
        "\n",
        "    # Create optimizer with specified learning rate\n",
        "    optimizer = optim.Adam(\n",
        "        model.parameters(),\n",
        "        lr=params_dict['lr'],\n",
        "        weight_decay=params_dict['weight_decay']\n",
        "    )\n",
        "\n",
        "    # Recreate data loaders with specified batch size\n",
        "    train_loader_hp = DataLoader(train_dataset, batch_size=params_dict['batch_size'], shuffle=True)\n",
        "    val_loader_hp = DataLoader(val_dataset, batch_size=params_dict['batch_size'], shuffle=False)\n",
        "\n",
        "    # Loss function\n",
        "    criterion = nn.BCELoss(reduction='none')\n",
        "\n",
        "    # Training loop with early stopping\n",
        "    best_val_auc = 0\n",
        "    patience_counter = 0\n",
        "\n",
        "    for epoch in range(max_epochs):\n",
        "        # Train epoch\n",
        "        train_loss, train_auc = train_epoch(model, train_loader_hp, optimizer, criterion, device)\n",
        "\n",
        "        # Validate\n",
        "        val_loss, val_auc = validate_epoch(model, val_loader_hp, criterion, device)\n",
        "\n",
        "        # Check if improved\n",
        "        if val_auc > best_val_auc:\n",
        "            best_val_auc = val_auc\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "\n",
        "        # Early stopping\n",
        "        if patience_counter >= patience:\n",
        "            break\n",
        "\n",
        "    return best_val_auc, epoch + 1\n",
        "\n",
        "# Run hyperparameter search\n",
        "results = []\n",
        "best_auc = 0\n",
        "best_params = None\n",
        "\n",
        "print(\"\\nStarting hyperparameter search...\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Test a subset first (remove this limit for full search)\n",
        "# For full search, remove the [:10] to test all combinations\n",
        "for i, param_values in enumerate(param_combinations):  # Testing all combinations\n",
        "    params_dict = dict(zip(param_names, param_values))\n",
        "\n",
        "    print(f\"\\nTesting combination {i+1}/{len(param_combinations)}\")\n",
        "    print(f\"Parameters: {params_dict}\")\n",
        "\n",
        "    try:\n",
        "        # Train with these parameters\n",
        "        val_auc, epochs_trained = train_with_params(params_dict, max_epochs=15, patience=3)\n",
        "\n",
        "        # Store results\n",
        "        result = {\n",
        "            'params': params_dict,\n",
        "            'val_auc': val_auc,\n",
        "            'epochs': epochs_trained\n",
        "        }\n",
        "        results.append(result)\n",
        "\n",
        "        print(f\"Validation AUC: {val_auc:.4f} (trained for {epochs_trained} epochs)\")\n",
        "\n",
        "        # Track best\n",
        "        if val_auc > best_auc:\n",
        "            best_auc = val_auc\n",
        "            best_params = params_dict\n",
        "            print(f\"âœ“ New best AUC!\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error with parameters {params_dict}: {str(e)}\")\n",
        "        continue\n",
        "\n",
        "# Sort results by AUC\n",
        "results.sort(key=lambda x: x['val_auc'], reverse=True)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"HYPERPARAMETER SEARCH RESULTS\")\n",
        "print(\"=\"*80)\n",
        "print(f\"\\nBest parameters found:\")\n",
        "print(f\"AUC: {best_auc:.4f}\")\n",
        "for param, value in best_params.items():\n",
        "    print(f\"{param}: {value}\")\n",
        "\n",
        "print(\"\\nTop 5 configurations:\")\n",
        "for i, result in enumerate(results[:5]):\n",
        "    print(f\"\\n{i+1}. AUC: {result['val_auc']:.4f}\")\n",
        "    for param, value in result['params'].items():\n",
        "        print(f\"   {param}: {value}\")\n",
        "\n",
        "# Save results\n",
        "with open('hyperparameter_search_results.json', 'w') as f:\n",
        "    json.dump(results, f, indent=2)"
      ],
      "metadata": {
        "id": "YckB_GGPW2L7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}