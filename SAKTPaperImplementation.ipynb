{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO0CFqiwe2UvDA9gPQqpWsz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AmurdAmzer/SAKT-Paper_Implementation/blob/main/SAKTPaperImplementation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cell 1: Uploading data via Colab's File Browser"
      ],
      "metadata": {
        "id": "r3Mjf06XlwkI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 1: Uploading data via Colab's File Browser\n",
        "\n",
        "# os stands for \"Operating System\" - it's like a special toolkit that lets your Python code talk to your computer's file system.\n",
        "import os\n",
        "print(\"Files in current directory:\")\n",
        "for file in os.listdir():\n",
        "  if file.endswith(\".csv\"):\n",
        "    print(f\" - {file}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LacccNggl2gy",
        "outputId": "be65944c-d932-491d-92c6-6b20b146b42f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files in current directory:\n",
            " - skill_builder_data_corrected_collapsed.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Cell 2: Imports and File upload"
      ],
      "metadata": {
        "id": "soB_jKNd5A3M"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "l7PycBC01iR8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65bf3670-0ef2-41d4-9687-41199c04b8a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (145400, 31)\n",
            "\n",
            "Column names: ['Unnamed: 0', 'order_id', 'assignment_id', 'user_id', 'assistment_id', 'problem_id', 'original', 'correct', 'attempt_count', 'ms_first_response', 'tutor_mode', 'answer_type', 'sequence_id', 'student_class_id', 'position', 'type', 'base_sequence_id', 'skill_id', 'skill_name', 'teacher_id', 'school_id', 'hint_count', 'hint_total', 'overlap_time', 'template_id', 'answer_id', 'answer_text', 'first_action', 'bottom_hint', 'opportunity', 'opportunity_original']\n",
            "\n",
            "First 5 rows:    Unnamed: 0  order_id  assignment_id  user_id  assistment_id  problem_id  \\\n",
            "0           1  33022537         277618    64525          33139       51424   \n",
            "1           2  33022709         277618    64525          33150       51435   \n",
            "2           3  35450204         220674    70363          33159       51444   \n",
            "3           4  35450295         220674    70363          33110       51395   \n",
            "4           5  35450311         220674    70363          33196       51481   \n",
            "\n",
            "   original  correct  attempt_count  ms_first_response  ... hint_count  \\\n",
            "0         1        1              1              32454  ...        0.0   \n",
            "1         1        1              1               4922  ...        0.0   \n",
            "2         1        0              2              25390  ...        0.0   \n",
            "3         1        1              1               4859  ...        0.0   \n",
            "4         1        0             14              19813  ...        3.0   \n",
            "\n",
            "  hint_total  overlap_time  template_id  answer_id answer_text  first_action  \\\n",
            "0        3.0       32454.0      30799.0        NaN          26           0.0   \n",
            "1        3.0        4922.0      30799.0        NaN          55           0.0   \n",
            "2        3.0       42000.0      30799.0        NaN          88           0.0   \n",
            "3        3.0        4859.0      30059.0        NaN          41           0.0   \n",
            "4        4.0      124564.0      30060.0        NaN          65           0.0   \n",
            "\n",
            "  bottom_hint opportunity  opportunity_original  \n",
            "0         NaN         1.0                   1.0  \n",
            "1         NaN         2.0                   2.0  \n",
            "2         NaN         1.0                   1.0  \n",
            "3         NaN         2.0                   2.0  \n",
            "4         0.0         3.0                   3.0  \n",
            "\n",
            "[5 rows x 31 columns]\n"
          ]
        }
      ],
      "source": [
        "# Cell 2: Imports and File upload (df = pd.read_csv(....)) opens the spreadsheet(data) and puts it into a \"DataFrame\"\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Load the CSV file (Remember to use the actual file name)\n",
        "# encoding='ISO-8859-1' handles special characters in the data\n",
        "# low_memory=False prevents dtype warnings for mixed types\n",
        "\n",
        "# I choose to call my loaded data df, df can mean DataFrame, df is just a variable name, I can choose to name my uploaded data anything.\n",
        "# low_memory=False tells the computer \"take your time reading this properly, don't rush\"\n",
        "# encoding='ISO-8859-1' = Like telling your computer \"hey, this file might have special characters like currency symbols ($ etc.)\".\n",
        "#  Real-Life Analogy\n",
        "\"\"\"\n",
        "It's like telling your computer:\n",
        "When you read this file, treat these bytes or symbols as Latin-style letters â€” not random gibberish.\"\n",
        "\n",
        "If you don't specify the correct encoding, Python might fail to read the file\n",
        "\"\"\"\n",
        "\n",
        "df = pd.read_csv('skill_builder_data_corrected_collapsed.csv', encoding='ISO-8859-1', low_memory=False)\n",
        "\n",
        "# Show basic information about the dataset\n",
        "print(f\"Dataset shape: {df.shape}\") # (rows, columns)\n",
        "print(f\"\\nColumn names: {list(df.columns)}\") # all column names\n",
        "print(f\"\\nFirst 5 rows: {df.head()}\") # preview first 5 rows. head() has a default parameter built in df.head(n=5). you could specify by df.head(5) etc.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cell3: Key Statistics - Understanding the dataset size and scope"
      ],
      "metadata": {
        "id": "Ir9cX3Pr5Ncc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: Key Statistics - Understanding the dataset size and scope\n",
        "\n",
        "print(\"===DATASET OVERVIEW===\")\n",
        "\n",
        "# Count total number of student interactions = number of rows\n",
        "print(f\"Toatal interactions: {len(df)}\")\n",
        "\n",
        "# Count unique students - each student has a unique user_id\n",
        "print(f\"Unique students: {df['user_id'].nunique()}\") # df['user_id] = access the user id column. nunique = count how many unique values are in that column\n",
        "\n",
        "# Count unique problems - individual questions students attempted\n",
        "print(f\"Unique problems: {df['problem_id'].nunique()}\")\n",
        "\n",
        "# Count unique skills - knowledge concepts being tested. this is crucial because I will create embeddings for each skill\n",
        "print(f\"Unique skills: {df['skill_id'].nunique()}\")\n",
        "\n",
        "# Calculate overall performance - percentage of correct answers\n",
        "print(f\"\\nCorrect rate: {df['correct'].mean():.2%}\")  # .2% means format the number as percentage with two decimal places, so for eg. 0.825641 becomes 82.56%\n",
        "\n",
        "# Check data completeness for skill_id (Critical for SAKT)\n",
        "# NB. SAKT needs skill_id to work - rows without the skill_id must be removed\n",
        "print(f\"Rows with skill_id: {df['skill_id'].notna().sum()}\")\n",
        "print(f\"Rows missing skill_id: {df['skill_id'].isna().sum()} ({df['skill_id'].isna().mean():.1%})\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cqUdL-mz24hc",
        "outputId": "c1c0bb1e-b862-44e6-ff5d-c3c51d0d2e49"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===DATASET OVERVIEW===\n",
            "Toatal interactions: 145400\n",
            "Unique students: 3389\n",
            "Unique problems: 8952\n",
            "Unique skills: 78\n",
            "\n",
            "Correct rate: 66.80%\n",
            "Rows with skill_id: 145399\n",
            "Rows missing skill_id: 1 (0.0%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CELL 4: Examine One Student's Learning Journey (Creating a case study).\n",
        "# Before SAKT learns from all students, you want to see what one student's learning path looks like\n",
        "# This helps to understand the sequential nature of the data"
      ],
      "metadata": {
        "id": "QhKU22wsrOMe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4: Examine One Student's (random) Learning Journey\n",
        "# This helps us understand the sequential nature of the data\n",
        "\n",
        "# Find students sorted by number of attempts (most active students)\n",
        "student_activity = df['user_id'].value_counts()\n",
        "\n",
        "# Pick the 11th most active student (avoid outliers)\n",
        "student_id = student_activity.index[20]\n",
        "\n",
        "# Get all data for this student, sorted by time\n",
        "# Order_id represents the sequence of attempsts\n",
        "\n",
        "student_data = df[df['user_id'] == student_id].sort_values('order_id')\n",
        "\n",
        "# Display student summary\n",
        "print(f\"Student {student_id} attempted {len(student_data)} problems\")\n",
        "print(f\"Skills attempted: {student_data['skill_id'].nunique()}\")\n",
        "print(f\"Correct rate: {student_data['correct'].mean():.2%}\")\n",
        "\n",
        "# Show their first 30 attempts to see the sequential Pattern\n",
        "print(\"\\nFirst 30 attempts:\")\n",
        "print(student_data[['order_id', 'skill_id', 'correct', 'ms_first_response']].head(30))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8nUiiGtprW7R",
        "outputId": "381e111b-e688-4575-be94-56fec30f7018"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Student 73684 attempted 414 problems\n",
            "Skills attempted: 54\n",
            "Correct rate: 77.05%\n",
            "\n",
            "First 30 attempts:\n",
            "        order_id skill_id  correct  ms_first_response\n",
            "20261   20426673    10_12        1              57694\n",
            "107452  20427695       54        0              16001\n",
            "26635   20428015       11        0              16126\n",
            "26636   20428406       11        1               8688\n",
            "26637   20428500       11        1              14033\n",
            "26638   20428580       11        1               7249\n",
            "26639   20428909    11_70        1              58632\n",
            "26640   20429211    11_70        0              28855\n",
            "26641   20430188       11        0              67154\n",
            "26642   20430544       11        1               8031\n",
            "26643   20430595       11        1              13531\n",
            "26644   20430623       11        1               8702\n",
            "10253   20549826        4        0              30007\n",
            "10254   20549848        4        0              67603\n",
            "135     20549937     1_13        0              17364\n",
            "121342  21406452       67        1              20499\n",
            "109200  21406745       58        1              17139\n",
            "107453  21407553       54        1               9407\n",
            "51696   21407633       24        1              26126\n",
            "20262   22833906    10_12        1              77160\n",
            "20263   22833957    10_12        1              77145\n",
            "20264   22834002    10_12        1              36667\n",
            "20265   22834043    10_12        1              83711\n",
            "136     22834125     1_13        1               5930\n",
            "137     22834134     1_13        1               5914\n",
            "109201  22834179       58        0              18101\n",
            "109202  22834204       58        1              46644\n",
            "109203  22834221       58        1             190687\n",
            "109204  22834275       58        1              61240\n",
            "104337  24605547       51        0              13391\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CELL 5: Visualize Key Patterns to Understand The Data Better"
      ],
      "metadata": {
        "id": "j4AOb7nX5wK_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 5: Visualing Key Patterns to Understand the Data Better\n",
        "\n",
        "# Calculate seuence length for each student (how many problems/quenstions/skills did each student attempt)\n",
        "\n",
        "seq_lengths = df.groupby('user_id').size()\n",
        "\n",
        "# seq_len for the Top 20 students\n",
        "print(\"top 20 Most Active Students:\")\n",
        "print(\"user_id: sequence length\")\n",
        "sorted_seq = seq_lengths.sort_values(ascending=False)\n",
        "for user_id, length in sorted_seq.head(20).items():\n",
        "  print(f\"{user_id}:     {length}\")\n",
        "\n",
        "print(\"\\n\")\n",
        "\n",
        "\n",
        "# seq_len for the Buttom 20 students\n",
        "print(\"Buttom 20 Least Active Students:\")\n",
        "print(\"user_id: sequence length\")\n",
        "sorted_seq = seq_lengths.sort_values(ascending=False)\n",
        "for user_id, length in sorted_seq.tail(20).items():\n",
        "  print(f\"{user_id}:     {length}\")\n",
        "\n",
        "print(\"\\n\")\n",
        "\n",
        "# Random 20 students sorted by length\n",
        "print(\"20 Random students sorted by length\")\n",
        "print(\"user_id: sequence length\")\n",
        "random_sample = seq_lengths.sample(20).sort_values() # If you remove the ascending argument, pandas uses the default, which is: ascending=True\n",
        "for user_id, length in random_sample.items():\n",
        "  print(f\"{user_id}:     {length}\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WPbKjUOS5vWK",
        "outputId": "411652a6-a933-4f83-fd5b-70a3f7e2c630"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "top 20 Most Active Students:\n",
            "user_id: sequence length\n",
            "75169:     623\n",
            "78970:     563\n",
            "78978:     538\n",
            "79021:     528\n",
            "71881:     518\n",
            "79019:     496\n",
            "79029:     489\n",
            "79032:     468\n",
            "96235:     447\n",
            "80807:     446\n",
            "96274:     445\n",
            "96243:     445\n",
            "79034:     443\n",
            "78989:     442\n",
            "96265:     434\n",
            "79013:     432\n",
            "70746:     431\n",
            "78980:     420\n",
            "78673:     415\n",
            "96271:     415\n",
            "\n",
            "\n",
            "Buttom 20 Least Active Students:\n",
            "user_id: sequence length\n",
            "88915:     1\n",
            "88914:     1\n",
            "88912:     1\n",
            "88908:     1\n",
            "90687:     1\n",
            "78252:     1\n",
            "74701:     1\n",
            "80110:     1\n",
            "71091:     1\n",
            "80275:     1\n",
            "80997:     1\n",
            "90192:     1\n",
            "52613:     1\n",
            "89665:     1\n",
            "71163:     1\n",
            "80359:     1\n",
            "89985:     1\n",
            "89983:     1\n",
            "89982:     1\n",
            "89981:     1\n",
            "\n",
            "\n",
            "20 Random students sorted by length\n",
            "user_id: sequence length\n",
            "89665:     1\n",
            "88647:     1\n",
            "80127:     2\n",
            "87401:     4\n",
            "84221:     5\n",
            "84039:     6\n",
            "91875:     7\n",
            "78275:     9\n",
            "91866:     14\n",
            "88830:     23\n",
            "91986:     23\n",
            "86310:     30\n",
            "85969:     36\n",
            "84139:     63\n",
            "78994:     69\n",
            "96238:     70\n",
            "78282:     71\n",
            "79777:     92\n",
            "96233:     97\n",
            "70729:     389\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CELL 6: Data Quality Check - Critical for Reliable Model Training"
      ],
      "metadata": {
        "id": "ATuGUM4053dg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 6: Data Quality Check - Critical for Reliable Model Training\n",
        "\n",
        "print(\"Data Quality Report:\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# Check 1: Duplicate rows (same data appearing multiple times)\n",
        "duplicate_count = df.duplicated().sum()\n",
        "print(f\"Duplicate rows: {duplicate_count}\")\n",
        "if duplicate_count > 0:\n",
        "    print(\"Need to remove duplicates!\")\n",
        "\n",
        "# Check 2: Students with very few attempts\n",
        "# SAKT needs sufficient history to learn patterns\n",
        "too_few = (seq_lengths < 5).sum()\n",
        "print(f\"Students with <5 attempts: {too_few} ({too_few/len(seq_lengths)*100:.1f}%)\")\n",
        "\n",
        "# Check 3: Students with too many attempts (potential outliers)\n",
        "too_many = (seq_lengths > 500).sum()\n",
        "print(f\"Students with >500 attempts: {too_many}\")\n",
        "\n",
        "# Check 4: Temporal ordering validation\n",
        "# Sort by user and order_id to check sequence integrity\n",
        "df_sorted = df.sort_values(['user_id', 'order_id'])\n",
        "# For each user, check if order_id always increases\n",
        "is_ordered = df_sorted.groupby('user_id')['order_id'].apply(\n",
        "    lambda x: (x.diff().dropna() > 0).all()  # diff() calculates difference between consecutive values\n",
        ").all()\n",
        "print(f\"All sequences properly ordered: {is_ordered}\")\n",
        "\n",
        "# Check 5: Original vs scaffolding problems\n",
        "# Original = main problem, scaffolding = hints/sub-problems\n",
        "original_count = (df['original'] == 1).sum()\n",
        "scaffold_count = (df['original'] == 0).sum()\n",
        "print(f\"\\nOriginal problems: {original_count} ({original_count/len(df)*100:.1f}%)\")\n",
        "print(f\"Scaffolding problems: {scaffold_count} ({scaffold_count/len(df)*100:.1f}%)\")\n",
        "print(\"SAKT paper uses only original problems\")\n",
        "\n",
        "# Check 6: Answer distribution\n",
        "print(f\"\\nAnswer distribution:\")\n",
        "print(f\"Correct: {(df['correct'] == 1).sum()} ({df['correct'].mean()*100:.1f}%)\")\n",
        "print(f\"Incorrect: {(df['correct'] == 0).sum()} ({(1-df['correct'].mean())*100:.1f}%)\")\n",
        "\n",
        "# Check 7: Critical missing data for SAKT\n",
        "print(f\"\\nMissing data analysis:\")\n",
        "print(f\"Missing skill_id: {df['skill_id'].isna().sum()} rows ({df['skill_id'].isna().mean()*100:.1f}%)\")\n",
        "print(f\"Missing user_id: {df['user_id'].isna().sum()} rows\")\n",
        "print(f\"Missing correct: {df['correct'].isna().sum()} rows\")\n",
        "print(\"SAKT requires skill_id, user_id, and correct to be present\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nh6WRXAH56qi",
        "outputId": "c2014a19-af09-4866-e77c-3eb973209b9e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data Quality Report:\n",
            "----------------------------------------\n",
            "Duplicate rows: 0\n",
            "Students with <5 attempts: 645 (19.0%)\n",
            "Students with >500 attempts: 5\n",
            "All sequences properly ordered: True\n",
            "\n",
            "Original problems: 142369 (97.9%)\n",
            "Scaffolding problems: 3031 (2.1%)\n",
            "SAKT paper uses only original problems\n",
            "\n",
            "Answer distribution:\n",
            "Correct: 97133 (66.8%)\n",
            "Incorrect: 48267 (33.2%)\n",
            "\n",
            "Missing data analysis:\n",
            "Missing skill_id: 1 rows (0.0%)\n",
            "Missing user_id: 0 rows\n",
            "Missing correct: 0 rows\n",
            "SAKT requires skill_id, user_id, and correct to be present\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CELL 7: Data Preprocessing for SAKT"
      ],
      "metadata": {
        "id": "OKW2EPOJ7Y3Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 7: Data Preprocessing for SAKT\n",
        "\n",
        "print(\"Starting data preprocessing...\")\n",
        "print(f\"Original data shape: {df.shape}\")\n",
        "\n",
        "# Step 1: Keep only original problems (main problems, not hints)\n",
        "# SAKT paper specifies using only original problems\n",
        "df_clean = df[df['original'] == 1].copy()\n",
        "print(f\"\\nAfter keeping only original problems: {df_clean.shape}\")\n",
        "\n",
        "# Step 2: Remove rows with missing skill_id\n",
        "# SAKT requires skill_id to create embeddings\n",
        "df_clean = df_clean.dropna(subset=['skill_id'])\n",
        "print(f\"After removing missing skill_id: {df_clean.shape}\")\n",
        "\n",
        "# Step 3: Convert skill_id to integer (it might be float due to NaN values)\n",
        "df_clean['skill_id'] = df_clean['skill_id'].astype(int)\n",
        "\n",
        "# Step 4: Calculate sequence lengths per student\n",
        "student_seq_lengths = df_clean.groupby('user_id').size()\n",
        "\n",
        "# Step 5: Keep only students with >= 0 attempts\n",
        "# Too few attempts don't provide enough learning history\n",
        "valid_students = student_seq_lengths[student_seq_lengths >= 0].index\n",
        "df_clean = df_clean[df_clean['user_id'].isin(valid_students)]\n",
        "print(f\"After removing students with <5 attempts: {df_clean.shape}\")\n",
        "\n",
        "# Step 6: Sort by user_id and order_id (temporal order)\n",
        "df_clean = df_clean.sort_values(['user_id', 'order_id'])\n",
        "\n",
        "print(f\"\\nFinal clean dataset:\")\n",
        "print(f\"- Total interactions: {len(df_clean)}\")\n",
        "print(f\"- Unique students: {df_clean['user_id'].nunique()}\")\n",
        "print(f\"- Unique skills: {df_clean['skill_id'].nunique()}\")\n",
        "print(f\"- Average correct rate: {df_clean['correct'].mean():.2%}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gLCecolb7fwX",
        "outputId": "99b9bf7d-f8ec-4ee4-c01b-9850c475956c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting data preprocessing...\n",
            "Original data shape: (145400, 31)\n",
            "\n",
            "After keeping only original problems: (142369, 31)\n",
            "After removing missing skill_id: (142368, 31)\n",
            "After removing students with <5 attempts: (142368, 31)\n",
            "\n",
            "Final clean dataset:\n",
            "- Total interactions: 142368\n",
            "- Unique students: 3389\n",
            "- Unique skills: 76\n",
            "- Average correct rate: 67.34%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CELL 8: Transform Data into SAKT Input Format"
      ],
      "metadata": {
        "id": "D9i4en2K7iJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 8: Transform Data into SAKT Input Format\n",
        "\n",
        "# Get unique skills and create mapping\n",
        "unique_skills = sorted(df_clean['skill_id'].unique())\n",
        "num_skills = len(unique_skills)\n",
        "\n",
        "# Create skill_id to index mapping (0 to num_skills-1)\n",
        "skill_to_idx = {skill: idx for idx, skill in enumerate(unique_skills)}\n",
        "\n",
        "print(f\"Number of unique skills: {num_skills}\")\n",
        "print(f\"Skill IDs range: {min(unique_skills)} to {max(unique_skills)}\")\n",
        "\n",
        "# Function to create sequences for each student\n",
        "def create_student_sequences(df_clean, skill_to_idx, num_skills):\n",
        "    \"\"\"\n",
        "    Convert student interactions into SAKT format:\n",
        "    - interaction = skill_idx + (correct * num_skills)\n",
        "    \"\"\"\n",
        "    sequences = []\n",
        "\n",
        "    # Process each student\n",
        "    for user_id, user_data in df_clean.groupby('user_id'):\n",
        "        # Get student's attempt history\n",
        "        skills = user_data['skill_id'].values\n",
        "        corrects = user_data['correct'].values\n",
        "\n",
        "        # Convert skill_id to indices\n",
        "        skill_indices = [skill_to_idx[skill] for skill in skills]\n",
        "\n",
        "        # Create interaction sequence (SAKT encoding)\n",
        "        # interaction = skill_index + (correct * num_skills)\n",
        "        interactions = []\n",
        "        for skill_idx, correct in zip(skill_indices, corrects):\n",
        "            interaction = skill_idx + (correct * num_skills)\n",
        "            interactions.append(interaction)\n",
        "\n",
        "        sequences.append({\n",
        "            'user_id': user_id,\n",
        "            'skill_indices': skill_indices,\n",
        "            'corrects': corrects,\n",
        "            'interactions': interactions,\n",
        "            'length': len(interactions)\n",
        "        })\n",
        "\n",
        "    return sequences\n",
        "\n",
        "# Create sequences\n",
        "print(\"\\nCreating student sequences...\")\n",
        "sequences = create_student_sequences(df_clean, skill_to_idx, num_skills)\n",
        "\n",
        "# Show example sequence\n",
        "print(f\"\\nExample sequence (first student):\")\n",
        "example = sequences[0]\n",
        "print(f\"User ID: {example['user_id']}\")\n",
        "print(f\"Sequence length: {example['length']}\")\n",
        "print(f\"First 5 skills attempted: {example['skill_indices'][:5]}\")\n",
        "print(f\"First 5 responses (0=wrong, 1=correct): {example['corrects'][:5]}\")\n",
        "print(f\"First 5 interaction encodings: {example['interactions'][:5]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "osY3ties7k2s",
        "outputId": "b59d2b8e-399b-414d-deec-bebd8d898dfc"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of unique skills: 76\n",
            "Skill IDs range: 1 to 48637079\n",
            "\n",
            "Creating student sequences...\n",
            "\n",
            "Example sequence (first student):\n",
            "User ID: 14\n",
            "Sequence length: 19\n",
            "First 5 skills attempted: [67, 67, 67, 67, 67]\n",
            "First 5 responses (0=wrong, 1=correct): [0 1 0 0 0]\n",
            "First 5 interaction encodings: [np.int64(67), np.int64(143), np.int64(67), np.int64(67), np.int64(67)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "HPht-lxO7mZO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CELL 9: Split Data for Training (Student-Level Split)"
      ],
      "metadata": {
        "id": "ZW62OMAt7oKR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 9: Split Data for Training (Student-Level Split)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split at student level (not interaction level) to prevent data leakage\n",
        "# Each student's full sequence goes into either train, val, or test\n",
        "\n",
        "# Use 80/20 split as in the paper (no separate validation)\n",
        "train_sequences, test_sequences = train_test_split(\n",
        "    sequences,\n",
        "    test_size=0.2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# For hyperparameter tuning, create a small validation set from training\n",
        "#train_sequences, val_sequences = train_test_split(\n",
        "#    train_sequences,\n",
        "#    test_size=0.1,  # 10% of training data for validation\n",
        "#    random_state=42\n",
        "#)\n",
        "\n",
        "print(f\"Dataset splits (matching paper):\")\n",
        "print(f\"- Train: {len(train_sequences)} students (80%)\")\n",
        "#print(f\"- Val: {len(val_sequences)} students\")\n",
        "print(f\"- Test: {len(test_sequences)} students (20%)\")\n",
        "\n",
        "# Calculate total interactions per split\n",
        "train_interactions = sum(seq['length'] for seq in train_sequences)\n",
        "#val_interactions = sum(seq['length'] for seq in val_sequences)\n",
        "test_interactions = sum(seq['length'] for seq in test_sequences)\n",
        "\n",
        "print(f\"\\nTotal interactions per split:\")\n",
        "print(f\"- Train: {train_interactions:,}\")\n",
        "#print(f\"- Val: {val_interactions:,}\")\n",
        "print(f\"- Test: {test_interactions:,}\")\n",
        "\n",
        "# Save the processed data\n",
        "import pickle\n",
        "\n",
        "save_data = {\n",
        "    'train': train_sequences,\n",
        "    'val': train_sequences, #change to val_sequences when you want to create a validation data. currently I am using trainin data as validation data for compatibility with the theSAKT paper\n",
        "    'test': test_sequences,\n",
        "    'num_skills': num_skills,\n",
        "    'skill_to_idx': skill_to_idx\n",
        "}\n",
        "\n",
        "with open('sakt_preprocessed_data.pkl', 'wb') as f:\n",
        "    pickle.dump(save_data, f)\n",
        "\n",
        "print(\"\\nData preprocessing complete! Saved to 'sakt_preprocessed_data.pkl'\")"
      ],
      "metadata": {
        "id": "eV3IUXSR7qwt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7a709a1-32e5-4212-e9b7-17ed9e1b6199"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset splits (matching paper):\n",
            "- Train: 2711 students (80%)\n",
            "- Test: 678 students (20%)\n",
            "\n",
            "Total interactions per split:\n",
            "- Train: 112,991\n",
            "- Test: 29,377\n",
            "\n",
            "Data preprocessing complete! Saved to 'sakt_preprocessed_data.pkl'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CELL 10: SAKT Model Implementation"
      ],
      "metadata": {
        "id": "JzMvcACxEkYx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 10: SAKT Model Implementation\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F #importing the functional module from PyTorch's torch.nn package, and giving it a shorter alias: F.\n",
        "import math\n",
        "\n",
        "class SAKT(nn.Module): # Class SAKT model that inherits nn.module\n",
        "    def __init__(self, num_skills, embed_dim=100, num_heads=5, dropout=0.2):\n",
        "        \"\"\"\n",
        "        SAKT Model matching the paper implementation\n",
        "\n",
        "        Args:\n",
        "            num_skills: Number of unique skills (145 in this case)\n",
        "            embed_dim: Embedding dimension (paper uses 128)\n",
        "            num_heads: Number of attention heads (paper uses 5)\n",
        "            dropout: Dropout rate (paper uses 0.2)\n",
        "        \"\"\"\n",
        "        super(SAKT, self).__init__()\n",
        "\n",
        "        self.num_skills = num_skills\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "        # Embedding layers\n",
        "        # Interaction embedding: Maps (skill + correct*num_skills) to vectors\n",
        "        self.interaction_embed = nn.Embedding(\n",
        "            num_skills * 2 + 1,  # *2 because skill + correct*num_skills\n",
        "            embed_dim,\n",
        "            padding_idx=num_skills * 2\n",
        "        )\n",
        "\n",
        "        # Exercise/skill embedding: Maps skills to vectors for queries\n",
        "        self.skill_embed = nn.Embedding(\n",
        "            num_skills + 1,\n",
        "            embed_dim,\n",
        "            padding_idx=0\n",
        "            )\n",
        "\n",
        "        # Positional embedding: Adds temporal information\n",
        "        self.pos_embed = nn.Embedding(1000, embed_dim)  # max sequence length 1000\n",
        "\n",
        "        # Multi-head attention layer\n",
        "        self.attention = nn.MultiheadAttention(\n",
        "            embed_dim,\n",
        "            num_heads,\n",
        "            dropout=dropout,\n",
        "            batch_first=True  # Important: batch dimension first\n",
        "        )\n",
        "\n",
        "        # Layer normalization\n",
        "        self.layer_norm1 = nn.LayerNorm(embed_dim)\n",
        "        self.layer_norm2 = nn.LayerNorm(embed_dim)\n",
        "\n",
        "        # Feed-forward network\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(embed_dim, embed_dim * 4),  # Paper uses 4x hidden size\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(embed_dim * 4, embed_dim)\n",
        "        )\n",
        "\n",
        "        # Output prediction layer\n",
        "        self.pred = nn.Linear(embed_dim, 1)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, interactions, skills):\n",
        "        \"\"\"\n",
        "        Forward pass of SAKT\n",
        "\n",
        "        Args:\n",
        "            interactions: [batch_size, seq_len] - past interactions (skill + correct*num_skills)\n",
        "            skills: [batch_size, seq_len] - skills to predict performance on\n",
        "\n",
        "        Returns:\n",
        "            predictions: [batch_size, seq_len] - probability of correct answer\n",
        "        \"\"\"\n",
        "        batch_size, seq_len = interactions.shape\n",
        "\n",
        "        # Create position indices\n",
        "        positions = torch.arange(seq_len, device=interactions.device).unsqueeze(0).expand(batch_size, -1)\n",
        "\n",
        "        # Embed interactions (for Keys and Values in attention)\n",
        "        interaction_embeds = self.interaction_embed(interactions)  # [batch, seq_len, embed_dim]\n",
        "\n",
        "        # Embed skills (for Queries in attention)\n",
        "        skill_embeds = self.skill_embed(skills)  # [batch, seq_len, embed_dim]\n",
        "\n",
        "        # Add positional embeddings\n",
        "        interaction_embeds = interaction_embeds + self.pos_embed(positions)\n",
        "        skill_embeds = skill_embeds + self.pos_embed(positions)\n",
        "\n",
        "        # Create attention mask (causal mask - can't see future)\n",
        "        attn_mask = torch.triu(\n",
        "            torch.ones(seq_len, seq_len, device=interactions.device) * float('-inf'),\n",
        "            diagonal=1\n",
        "        )\n",
        "\n",
        "        # Apply self-attention\n",
        "        # Query: what skill we're predicting\n",
        "        # Key & Value: past interaction history\n",
        "        attended, _ = self.attention(\n",
        "            query=skill_embeds,\n",
        "            key=interaction_embeds,\n",
        "            value=interaction_embeds,\n",
        "            attn_mask=attn_mask,\n",
        "            need_weights=False\n",
        "        )\n",
        "\n",
        "        # Residual connection and layer norm\n",
        "        attended = self.layer_norm1(skill_embeds + self.dropout(attended))\n",
        "\n",
        "        # Feed-forward network with residual\n",
        "        ffn_out = self.ffn(attended)\n",
        "        ffn_out = self.layer_norm2(attended + self.dropout(ffn_out))\n",
        "\n",
        "        # Predict probability of correct answer\n",
        "        pred = self.pred(ffn_out).squeeze(-1)  # [batch, seq_len]\n",
        "        return torch.sigmoid(pred)\n",
        "\n",
        "# Test the model\n",
        "print(\"Testing SAKT model...\")\n",
        "model = SAKT(num_skills=145)\n",
        "\n",
        "# Create dummy batch\n",
        "batch_interactions = torch.randint(0, 290, (2, 50))  # 2 sequences, length 50\n",
        "batch_skills = torch.randint(0, 145, (2, 50))\n",
        "\n",
        "# Forward pass\n",
        "output = model(batch_interactions, batch_skills)\n",
        "print(f\"Model output shape: {output.shape}\")\n",
        "print(f\"Output range: [{output.min():.3f}, {output.max():.3f}]\")\n",
        "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")"
      ],
      "metadata": {
        "id": "YYZUAsjHEmZQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25dcc9b5-9c90-49c9-a97f-8d113fd43155"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing SAKT model...\n",
            "Model output shape: torch.Size([2, 50])\n",
            "Output range: [0.210, 0.779]\n",
            "Model parameters: 265,101\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CELL 11: Create PyTorch Dataset and DataLoaders"
      ],
      "metadata": {
        "id": "0aIb8Y30EtBq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 11: Create PyTorch Dataset and DataLoaders\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "\n",
        "class SAKTDataset(Dataset):\n",
        "    \"\"\"Dataset class for SAKT - Corrected version\"\"\"\n",
        "\n",
        "    def __init__(self, sequences, max_seq_len=100, num_skills=145):\n",
        "        self.max_seq_len = max_seq_len\n",
        "        self.num_skills = num_skills\n",
        "        self.padding_interaction = num_skills * 2  # Padding token\n",
        "\n",
        "        # Process sequences: split long ones\n",
        "        self.data = []\n",
        "        for seq in sequences:\n",
        "            interactions = seq['interactions']\n",
        "            skills = seq['skill_indices']\n",
        "            corrects = seq['corrects']\n",
        "\n",
        "            # If sequence is longer than max_seq_len, split it\n",
        "            if len(interactions) > max_seq_len:\n",
        "                for i in range(0, len(interactions), max_seq_len):\n",
        "                    end_idx = min(i + max_seq_len, len(interactions))\n",
        "                    self.data.append({\n",
        "                        'interactions': interactions[i:end_idx],\n",
        "                        'skills': skills[i:end_idx],\n",
        "                        'corrects': corrects[i:end_idx]\n",
        "                    })\n",
        "            else:\n",
        "                self.data.append({\n",
        "                    'interactions': interactions,\n",
        "                    'skills': skills,\n",
        "                    'corrects': corrects\n",
        "                })\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        seq = self.data[idx]\n",
        "        seq_len = len(seq['interactions'])\n",
        "\n",
        "        interactions = list(seq['interactions'])\n",
        "        skills = list(seq['skills'])\n",
        "        corrects = list(seq['corrects'])\n",
        "\n",
        "        shifted_interactions = []\n",
        "        for i in range(seq_len):\n",
        "            if i == 0:\n",
        "                # First position: use a special START token (same as padding)\n",
        "                shifted_interactions.append(self.padding_interaction)\n",
        "            else:\n",
        "                # Use the PREVIOUS interaction\n",
        "                shifted_interactions.append(interactions[i-1])\n",
        "\n",
        "        # Use shifted_interactions instead of interactions\n",
        "        interactions = shifted_interactions\n",
        "\n",
        "        # Pad to the LEFT if sequence is shorter than max_seq_len\n",
        "        if seq_len < self.max_seq_len:\n",
        "            pad_len = self.max_seq_len - seq_len\n",
        "\n",
        "            # Pad to the LEFT\n",
        "            interactions = [self.padding_interaction] * pad_len + interactions\n",
        "            skills = [0] * pad_len + skills  # 0 for padding\n",
        "            corrects = [0] * pad_len + corrects\n",
        "\n",
        "            # Create mask (0 for padding, 1 for real data)\n",
        "            mask = [0] * pad_len + [1] * seq_len\n",
        "        else:\n",
        "            mask = [1] * self.max_seq_len\n",
        "\n",
        "        return {\n",
        "            'interactions': torch.tensor(interactions, dtype=torch.long),\n",
        "            'skills': torch.tensor(skills, dtype=torch.long),\n",
        "            'targets': torch.tensor(corrects, dtype=torch.float),\n",
        "            'mask': torch.tensor(mask, dtype=torch.float)\n",
        "        }\n",
        "\n",
        "# Load preprocessed data\n",
        "import pickle\n",
        "with open('sakt_preprocessed_data.pkl', 'rb') as f:\n",
        "    data = pickle.load(f)\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = SAKTDataset(data['train'], max_seq_len=100, num_skills=data['num_skills']) # paper uses max_seq_length = 100 and 50\n",
        "val_dataset = SAKTDataset(data['val'], max_seq_len=100, num_skills=data['num_skills'])     # paper uses max_seq_length = 100 and 50\n",
        "test_dataset = SAKTDataset(data['test'], max_seq_len=100, num_skills=data['num_skills'])   # paper uses max_seq_length = 100 and 50\n",
        "\n",
        "# Create dataloaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)  # paper uses batch_size = 128\n",
        "val_loader = DataLoader(val_dataset, batch_size=128, shuffle=False)     # paper uses batch_size = 128\n",
        "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)   # paper uses batch_size = 128\n",
        "\n",
        "# Test dataloader\n",
        "batch = next(iter(train_loader))\n",
        "print(f\"Batch keys: {batch.keys()}\")\n",
        "print(f\"Interactions shape: {batch['interactions'].shape}\")\n",
        "print(f\"Skills shape: {batch['skills'].shape}\")\n",
        "print(f\"Targets shape: {batch['targets'].shape}\")\n",
        "print(f\"Mask shape: {batch['mask'].shape}\")"
      ],
      "metadata": {
        "id": "RnIpiDMGEvyu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc29bcc4-a247-41a2-f00d-e3495cbf9762"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch keys: dict_keys(['interactions', 'skills', 'targets', 'mask'])\n",
            "Interactions shape: torch.Size([128, 100])\n",
            "Skills shape: torch.Size([128, 100])\n",
            "Targets shape: torch.Size([128, 100])\n",
            "Mask shape: torch.Size([128, 100])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "G3iDyuwvE2GR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CELL 12: Test Data Loaders and Verify Data Format"
      ],
      "metadata": {
        "id": "ZBUnkJ5uE4Ox"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 12: Test Data Loaders and Verify Data Format\n",
        "\n",
        "import pickle\n",
        "\n",
        "# Load the preprocessed data we saved in Cell 9\n",
        "# 'rb' means read in binary mode (pickle files are binary)\n",
        "with open('sakt_preprocessed_data.pkl', 'rb') as f:\n",
        "    data = pickle.load(f)  # Converts file back to Python dictionary\n",
        "\n",
        "print(f\"Loaded data with {data['num_skills']} unique skills\")\n",
        "\n",
        "# Create PyTorch datasets from the sequences\n",
        "# SAKTDataset handles padding and creating input/target pairs\n",
        "train_dataset = SAKTDataset(data['train'], max_seq_len=100)\n",
        "val_dataset = SAKTDataset(data['val'], max_seq_len=100)\n",
        "test_dataset = SAKTDataset(data['test'], max_seq_len=100)\n",
        "\n",
        "print(f\"\\nDataset sizes:\")\n",
        "print(f\"Train: {len(train_dataset)} students\")\n",
        "print(f\"Val: {len(val_dataset)} students\")\n",
        "print(f\"Test: {len(test_dataset)} students\")\n",
        "\n",
        "# Create data loaders that will feed batches to my model\n",
        "# batch_size=64 means process 64 students at once\n",
        "# shuffle=True randomizes order each epoch (important for training)\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "# Test by loading one batch to verify everything works\n",
        "print(f\"\\nTesting data loader with one batch:\")\n",
        "batch = next(iter(train_loader))  # Get first batch\n",
        "\n",
        "# Check what's in each batch\n",
        "print(f\"Batch contains: {list(batch.keys())}\")\n",
        "print(f\"Interactions shape: {batch['interactions'].shape}\")  # [64, 99]\n",
        "print(f\"Skills shape: {batch['skills'].shape}\")              # [64, 99]\n",
        "print(f\"Targets shape: {batch['targets'].shape}\")            # [64, 99]\n",
        "print(f\"Mask shape: {batch['mask'].shape}\")                  # [64, 99]\n",
        "\n",
        "# Look at one student's data to understand format\n",
        "print(f\"\\nExample from first student in batch:\")\n",
        "first_seq_len = batch['mask'][0].sum().int()  # Count non-padded positions\n",
        "print(f\"Actual sequence length: {first_seq_len}\")\n",
        "print(f\"First 5 interactions: {batch['interactions'][0][:5].tolist()}\")\n",
        "print(f\"First 5 skills to predict: {batch['skills'][0][:5].tolist()}\")\n",
        "print(f\"First 5 correct/incorrect: {batch['targets'][0][:5].tolist()}\")\n",
        "print(f\"First 5 mask values: {batch['mask'][0][:5].tolist()}\")  # 1=real, 0=padding"
      ],
      "metadata": {
        "id": "Oho9RKT4E5QD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94fdec63-bd6b-4218-82de-f18a36e9d7aa"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded data with 76 unique skills\n",
            "\n",
            "Dataset sizes:\n",
            "Train: 3232 students\n",
            "Val: 3232 students\n",
            "Test: 818 students\n",
            "\n",
            "Testing data loader with one batch:\n",
            "Batch contains: ['interactions', 'skills', 'targets', 'mask']\n",
            "Interactions shape: torch.Size([64, 100])\n",
            "Skills shape: torch.Size([64, 100])\n",
            "Targets shape: torch.Size([64, 100])\n",
            "Mask shape: torch.Size([64, 100])\n",
            "\n",
            "Example from first student in batch:\n",
            "Actual sequence length: 10\n",
            "First 5 interactions: [290, 290, 290, 290, 290]\n",
            "First 5 skills to predict: [0, 0, 0, 0, 0]\n",
            "First 5 correct/incorrect: [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "First 5 mask values: [0.0, 0.0, 0.0, 0.0, 0.0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CELL 13: Training Functions for SAKT"
      ],
      "metadata": {
        "id": "KSAwA5DuE7nl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 13: Training Functions for SAKT\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def train_epoch(model, train_loader, optimizer, criterion, device):\n",
        "    \"\"\"\n",
        "    Train the model for one epoch\n",
        "\n",
        "    Args:\n",
        "        model: SAKT model\n",
        "        train_loader: DataLoader with training data\n",
        "        optimizer: Adam optimizer\n",
        "        criterion: BCELoss function\n",
        "        device: cuda or cpu\n",
        "\n",
        "    Returns:\n",
        "        epoch_loss: Average loss for this epoch\n",
        "        epoch_auc: AUC score for this epoch\n",
        "    \"\"\"\n",
        "    model.train()  # Enable dropout and batch norm training behavior\n",
        "    total_loss = 0\n",
        "    all_predictions = []\n",
        "    all_targets = []\n",
        "\n",
        "    # Progress bar to track training\n",
        "    pbar = tqdm(train_loader, desc='Training', leave=False)\n",
        "\n",
        "    for batch in pbar:\n",
        "        # Move all tensors to GPU if available\n",
        "        interactions = batch['interactions'].to(device)\n",
        "        skills = batch['skills'].to(device)\n",
        "        targets = batch['targets'].to(device)\n",
        "        mask = batch['mask'].to(device)\n",
        "\n",
        "        # Clear gradients from previous batch\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass: get predictions from model\n",
        "        predictions = model(interactions, skills)\n",
        "\n",
        "        # Calculate loss only on non-padded positions\n",
        "        # criterion returns loss for each position\n",
        "        # Compute loss with masking\n",
        "        loss = criterion(predictions, targets)  # This gives loss per element\n",
        "        masked_loss = (loss * mask).sum() / mask.sum()  # Apply mask and average\n",
        "\n",
        "\n",
        "        # Backward pass: compute gradients\n",
        "        masked_loss.backward()\n",
        "\n",
        "        # Clip gradients to prevent explosion\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "        # Update model parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        # Collect predictions for metrics\n",
        "        total_loss += masked_loss.item()\n",
        "\n",
        "        # Extract only valid (non-padded) predictions\n",
        "        valid_idx = mask == 1\n",
        "        valid_predictions = predictions[valid_idx].detach().cpu().numpy()\n",
        "        valid_targets = targets[valid_idx].detach().cpu().numpy()\n",
        "\n",
        "        all_predictions.extend(valid_predictions)\n",
        "        all_targets.extend(valid_targets)\n",
        "\n",
        "        # Update progress bar with current loss\n",
        "        pbar.set_postfix({'loss': f'{masked_loss.item():.4f}'})\n",
        "\n",
        "    # Calculate epoch metrics\n",
        "    epoch_loss = total_loss / len(train_loader)\n",
        "    epoch_auc = roc_auc_score(all_targets, all_predictions)\n",
        "\n",
        "    return epoch_loss, epoch_auc\n",
        "\n",
        "def validate_epoch(model, val_loader, criterion, device):\n",
        "    \"\"\"\n",
        "    Validate the model (no gradient updates)\n",
        "\n",
        "    Args:\n",
        "        model: SAKT model\n",
        "        val_loader: DataLoader with validation data\n",
        "        criterion: BCELoss function\n",
        "        device: cuda or cpu\n",
        "\n",
        "    Returns:\n",
        "        val_loss: Average validation loss\n",
        "        val_auc: Validation AUC score\n",
        "    \"\"\"\n",
        "    model.eval()  # Disable dropout and use batch norm statistics\n",
        "    total_loss = 0\n",
        "    all_predictions = []\n",
        "    all_targets = []\n",
        "\n",
        "    # No gradient computation needed for validation\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(val_loader, desc='Validating', leave=False):\n",
        "            # Move to device\n",
        "            interactions = batch['interactions'].to(device)\n",
        "            skills = batch['skills'].to(device)\n",
        "            targets = batch['targets'].to(device)\n",
        "            mask = batch['mask'].to(device)\n",
        "\n",
        "            # Get predictions\n",
        "            predictions = model(interactions, skills)\n",
        "\n",
        "            # Calculate loss\n",
        "            # Compute loss with masking\n",
        "            loss = criterion(predictions, targets)  # This gives loss per element\n",
        "            masked_loss = (loss * mask).sum() / mask.sum()  # Apply mask and average\n",
        "\n",
        "            # Collect for metrics\n",
        "            total_loss += masked_loss.item()\n",
        "\n",
        "            # Extract valid predictions\n",
        "            valid_idx = mask == 1\n",
        "            valid_predictions = predictions[valid_idx].cpu().numpy()\n",
        "            valid_targets = targets[valid_idx].cpu().numpy()\n",
        "\n",
        "            all_predictions.extend(valid_predictions)\n",
        "            all_targets.extend(valid_targets)\n",
        "\n",
        "    # Calculate validation metrics\n",
        "    val_loss = total_loss / len(val_loader)\n",
        "    val_auc = roc_auc_score(all_targets, all_predictions)\n",
        "\n",
        "    return val_loss, val_auc"
      ],
      "metadata": {
        "id": "y6BM04A2E-iF"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CELL 14: Train SAKT Model"
      ],
      "metadata": {
        "id": "uvoiybrsFCFk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 14: Train SAKT Model\n",
        "\n",
        "# Check if GPU is available and use it\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "if device.type == 'cuda':\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "\n",
        "# Initialize model with paper's hyperparameters\n",
        "model = SAKT(\n",
        "    num_skills=145,  # From my dataset analysis\n",
        "    embed_dim=100,   # Paper: d=[50, 100, 150, 200]\n",
        "    num_heads=5,     # Paper: h=5\n",
        "    dropout=0.2      # Paper: dropout=0.2\n",
        ").to(device)\n",
        "\n",
        "# Count model parameters\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"Total parameters: {total_params:,}\")\n",
        "print(f\"Trainable parameters: {trainable_params:,}\")\n",
        "\n",
        "# Initialize optimizer (Adam with paper's learning rate)\n",
        "optimizer = optim.Adam(\n",
        "    model.parameters(),\n",
        "    lr=0.001,           # Paper uses 0.001\n",
        "    weight_decay=0.0001  # L2 regularization\n",
        ")\n",
        "\n",
        "# Loss function for binary classification\n",
        "# reduction='none' returns loss per element (needed for masking)\n",
        "criterion = nn.BCELoss(reduction='none')\n",
        "\n",
        "\n",
        "# Training configuration\n",
        "NUM_EPOCHS = 50\n",
        "#best_val_auc = 0\n",
        "\n",
        "# Track metrics for visualization\n",
        "history = {\n",
        "    'train_loss': [], 'train_auc': [],\n",
        "#    'val_loss': [], 'val_auc': []\n",
        "}\n",
        "\n",
        "print(f\"\\nStarting training for max {NUM_EPOCHS} epochs...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Main training loop\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    print(f\"\\nEpoch {epoch+1}/{NUM_EPOCHS}\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    # Train for one epoch\n",
        "    train_loss, train_auc = train_epoch(\n",
        "        model, train_loader, optimizer, criterion, device\n",
        "    )\n",
        "\n",
        "    # Validate\n",
        "    #val_loss, val_auc = validate_epoch(\n",
        "    #    model, val_loader, criterion, device\n",
        "    #)\n",
        "\n",
        "    # Store history\n",
        "    history['train_loss'].append(train_loss)\n",
        "    history['train_auc'].append(train_auc)\n",
        "    #history['val_loss'].append(val_loss)\n",
        "    #history['val_auc'].append(val_auc)\n",
        "\n",
        "    # Print metrics\n",
        "    print(f\"Train - Loss: {train_loss:.4f}, AUC: {train_auc:.4f}\")\n",
        "    #print(f\"Valid - Loss: {val_loss:.4f}, AUC: {val_auc:.4f}\")\n",
        "\n",
        "    # Save best model\n",
        "    #if val_auc > best_val_auc:\n",
        "    #    best_val_auc = val_auc\n",
        "\n",
        "    if epoch + 1 == NUM_EPOCHS:\n",
        "        # Save checkpoint\n",
        "        checkpoint = {\n",
        "            'epoch': epoch + 1,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            #'best_val_auc': best_val_auc,\n",
        "            'train_auc': train_auc,\n",
        "            'history': history\n",
        "        }\n",
        "        torch.save(checkpoint, 'best_sakt_model.pth')\n",
        "        #print(f\"âœ“ New best model saved! (AUC: {best_val_auc:.4f})\")\n",
        "        print(f\"âœ“ Final model saved! (Train AUC: {train_auc:.4f})\")\n",
        "\n",
        "print(f\"\\nTraining complete!\")\n",
        "#print(f\"Best validation AUC: {best_val_auc:.4f}\")\n",
        "print(f\"Final training AUC: {train_auc:.4f}\")"
      ],
      "metadata": {
        "id": "jqPTIgbjFC37",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8dfb6631-b15a-4da8-8fa6-5d080362ea83"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "Total parameters: 265,101\n",
            "Trainable parameters: 265,101\n",
            "\n",
            "Starting training for max 50 epochs...\n",
            "============================================================\n",
            "\n",
            "Epoch 1/50\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train - Loss: 0.6185, AUC: 0.6045\n",
            "\n",
            "Epoch 2/50\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train - Loss: 0.5820, AUC: 0.6887\n",
            "\n",
            "Epoch 3/50\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train - Loss: 0.5682, AUC: 0.7104\n",
            "\n",
            "Epoch 4/50\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train - Loss: 0.5579, AUC: 0.7259\n",
            "\n",
            "Epoch 5/50\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train - Loss: 0.5513, AUC: 0.7346\n",
            "\n",
            "Epoch 6/50\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train - Loss: 0.5456, AUC: 0.7426\n",
            "\n",
            "Epoch 7/50\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train - Loss: 0.5404, AUC: 0.7476\n",
            "\n",
            "Epoch 8/50\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train - Loss: 0.5371, AUC: 0.7526\n",
            "\n",
            "Epoch 9/50\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train - Loss: 0.5339, AUC: 0.7567\n",
            "\n",
            "Epoch 10/50\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train - Loss: 0.5315, AUC: 0.7594\n",
            "\n",
            "Epoch 11/50\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train - Loss: 0.5287, AUC: 0.7630\n",
            "\n",
            "Epoch 12/50\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train - Loss: 0.5277, AUC: 0.7643\n",
            "\n",
            "Epoch 13/50\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train - Loss: 0.5251, AUC: 0.7669\n",
            "\n",
            "Epoch 14/50\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train - Loss: 0.5222, AUC: 0.7696\n",
            "\n",
            "Epoch 15/50\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train - Loss: 0.5207, AUC: 0.7720\n",
            "\n",
            "Epoch 16/50\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train - Loss: 0.5191, AUC: 0.7735\n",
            "\n",
            "Epoch 17/50\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train - Loss: 0.5179, AUC: 0.7744\n",
            "\n",
            "Epoch 18/50\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train - Loss: 0.5151, AUC: 0.7773\n",
            "\n",
            "Epoch 19/50\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train - Loss: 0.5144, AUC: 0.7776\n",
            "\n",
            "Epoch 20/50\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train - Loss: 0.5150, AUC: 0.7779\n",
            "\n",
            "Epoch 21/50\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train - Loss: 0.5139, AUC: 0.7797\n",
            "\n",
            "Epoch 22/50\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train - Loss: 0.5124, AUC: 0.7813\n",
            "\n",
            "Epoch 23/50\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train - Loss: 0.5108, AUC: 0.7824\n",
            "\n",
            "Epoch 24/50\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train - Loss: 0.5106, AUC: 0.7823\n",
            "\n",
            "Epoch 25/50\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train - Loss: 0.5089, AUC: 0.7848\n",
            "\n",
            "Epoch 26/50\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train - Loss: 0.5090, AUC: 0.7851\n",
            "\n",
            "Epoch 27/50\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train - Loss: 0.5076, AUC: 0.7864\n",
            "\n",
            "Epoch 28/50\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train - Loss: 0.5061, AUC: 0.7874\n",
            "\n",
            "Epoch 29/50\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train - Loss: 0.5048, AUC: 0.7887\n",
            "\n",
            "Epoch 30/50\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train - Loss: 0.5056, AUC: 0.7885\n",
            "\n",
            "Epoch 31/50\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train - Loss: 0.5034, AUC: 0.7900\n",
            "\n",
            "Epoch 32/50\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train - Loss: 0.5036, AUC: 0.7899\n",
            "\n",
            "Epoch 33/50\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train - Loss: 0.5017, AUC: 0.7917\n",
            "\n",
            "Epoch 34/50\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train - Loss: 0.5032, AUC: 0.7923\n",
            "\n",
            "Epoch 35/50\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train - Loss: 0.5017, AUC: 0.7926\n",
            "\n",
            "Epoch 36/50\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train - Loss: 0.4997, AUC: 0.7940\n",
            "\n",
            "Epoch 37/50\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train - Loss: 0.5009, AUC: 0.7932\n",
            "\n",
            "Epoch 38/50\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train - Loss: 0.4994, AUC: 0.7949\n",
            "\n",
            "Epoch 39/50\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train - Loss: 0.5002, AUC: 0.7942\n",
            "\n",
            "Epoch 40/50\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train - Loss: 0.4991, AUC: 0.7950\n",
            "\n",
            "Epoch 41/50\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train - Loss: 0.4985, AUC: 0.7956\n",
            "\n",
            "Epoch 42/50\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train - Loss: 0.4954, AUC: 0.7978\n",
            "\n",
            "Epoch 43/50\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train - Loss: 0.4950, AUC: 0.7985\n",
            "\n",
            "Epoch 44/50\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train - Loss: 0.4954, AUC: 0.7981\n",
            "\n",
            "Epoch 45/50\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train - Loss: 0.4967, AUC: 0.7985\n",
            "\n",
            "Epoch 46/50\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train - Loss: 0.4949, AUC: 0.7988\n",
            "\n",
            "Epoch 47/50\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train - Loss: 0.4941, AUC: 0.8006\n",
            "\n",
            "Epoch 48/50\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train - Loss: 0.4940, AUC: 0.8003\n",
            "\n",
            "Epoch 49/50\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train - Loss: 0.4924, AUC: 0.8013\n",
            "\n",
            "Epoch 50/50\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                                                      "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train - Loss: 0.4923, AUC: 0.8026\n",
            "âœ“ Final model saved! (Train AUC: 0.8026)\n",
            "\n",
            "Training complete!\n",
            "Final training AUC: 0.8026\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CELL 15: Plot Training History"
      ],
      "metadata": {
        "id": "Vp4XSTwZFEzB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 15: Plot Training History\n",
        "\n",
        "# Create figure with two subplots\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "# Subplot 1: Loss curves\n",
        "plt.subplot(1, 2, 1)\n",
        "epochs = range(1, len(history['train_loss']) + 1)\n",
        "plt.plot(epochs, history['train_loss'], 'b-', label='Train Loss', marker='o', markersize=4)\n",
        "plt.plot(epochs, history['val_loss'], 'r-', label='Val Loss', marker='s', markersize=4)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Subplot 2: AUC curves\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs, history['train_auc'], 'b-', label='Train AUC', marker='o', markersize=4)\n",
        "plt.plot(epochs, history['val_auc'], 'r-', label='Val AUC', marker='s', markersize=4)\n",
        "plt.axhline(y=0.82, color='g', linestyle='--', label='Target AUC (0.82)')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('AUC')\n",
        "plt.title('Training and Validation AUC')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print summary statistics\n",
        "print(\"Training Summary:\")\n",
        "print(\"-\" * 40)\n",
        "print(f\"Total epochs trained: {len(history['train_loss'])}\")\n",
        "print(f\"Best validation AUC: {max(history['val_auc']):.4f}\")\n",
        "print(f\"Final train AUC: {history['train_auc'][-1]:.4f}\")\n",
        "print(f\"Final validation AUC: {history['val_auc'][-1]:.4f}\")\n",
        "\n",
        "# Check for overfitting\n",
        "final_gap = history['train_auc'][-1] - history['val_auc'][-1]\n",
        "print(f\"\\nTrain-Val gap: {final_gap:.4f}\")\n",
        "if final_gap > 0.05:\n",
        "    print(\"Model might be overfitting\")\n",
        "else:\n",
        "    print(\"âœ“ No significant overfitting detected\")"
      ],
      "metadata": {
        "id": "QR2gh3sLFHmN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 562
        },
        "outputId": "08f68ae5-35cd-481d-c77b-9661004f2f30"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'val_loss'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-21-4252565987.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'b-'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Train Loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmarker\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'o'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmarkersize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r-'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Val Loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmarker\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m's'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmarkersize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Epoch'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'val_loss'"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CELL 16: Evaluate on Test Set"
      ],
      "metadata": {
        "id": "-qt2FatlFKDk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 16: Evaluate on Test Set\n",
        "\n",
        "# Load best model\n",
        "checkpoint = torch.load('best_sakt_model.pth', weights_only=False)\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "#print(f\"Loaded best model from epoch {checkpoint['epoch']} with val AUC {checkpoint['best_val_auc']:.4f}\")\n",
        "\n",
        "# Evaluate on test set\n",
        "print(\"\\nEvaluating on test set...\")\n",
        "test_loss, test_auc = validate_epoch(model, test_loader, criterion, device)\n",
        "\n",
        "print(f\"\\nTest Set Results:\")\n",
        "print(f\"Test Loss: {test_loss:.4f}\")\n",
        "print(f\"Test AUC: {test_auc:.4f}\")\n",
        "\n",
        "# Compare with validation performance\n",
        "print(f\"\\nGeneralization check:\")\n",
        "#print(f\"Validation AUC: {checkpoint['best_val_auc']:.4f}\")\n",
        "print(f\"Test AUC: {test_auc:.4f}\")\n",
        "#print(f\"Val-Test gap: {abs(checkpoint['best_val_auc'] - test_auc):.4f}\")\n",
        "\n",
        "# Additional metrics\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Get predictions for additional metrics\n",
        "model.eval()\n",
        "all_predictions = []\n",
        "all_targets = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        interactions = batch['interactions'].to(device)\n",
        "        skills = batch['skills'].to(device)\n",
        "        targets = batch['targets'].to(device)\n",
        "        mask = batch['mask'].to(device)\n",
        "\n",
        "        predictions = model(interactions, skills)\n",
        "\n",
        "        # Get actual batch size (last batch might be smaller)\n",
        "        current_batch_size = interactions.shape[0]\n",
        "\n",
        "        # Extract valid predictions for each sequence in batch\n",
        "        for i in range(current_batch_size):\n",
        "            mask_i = mask[i].cpu().numpy()\n",
        "            pred_i = predictions[i].cpu().numpy()\n",
        "            target_i = targets[i].cpu().numpy()\n",
        "\n",
        "            # Only add non-padded values\n",
        "            valid_idx = mask_i == 1\n",
        "            all_predictions.extend(pred_i[valid_idx].tolist())\n",
        "            all_targets.extend(target_i[valid_idx].tolist())\n",
        "\n",
        "# Convert to numpy arrays\n",
        "all_predictions = np.array(all_predictions)\n",
        "all_targets = np.array(all_targets)\n",
        "\n",
        "# Calculate additional metrics using 0.5 threshold\n",
        "binary_predictions = (all_predictions > 0.5).astype(int)\n",
        "\n",
        "accuracy = accuracy_score(all_targets, binary_predictions)\n",
        "precision = precision_score(all_targets, binary_predictions)\n",
        "recall = recall_score(all_targets, binary_predictions)\n",
        "#f1 = f1_score(all_targets, binary_predictions)\n",
        "\n",
        "print(f\"\\nAdditional Test Metrics:\")\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "#print(f\"F1-Score: {f1:.4f}\")"
      ],
      "metadata": {
        "id": "fpUEBkydFMc5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e22a8072-c123-4861-e36a-8f408e2d46ca"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluating on test set...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test Set Results:\n",
            "Test Loss: 0.5761\n",
            "Test AUC: 0.7178\n",
            "\n",
            "Generalization check:\n",
            "Test AUC: 0.7178\n",
            "\n",
            "Additional Test Metrics:\n",
            "Accuracy: 0.7151\n",
            "Precision: 0.7346\n",
            "Recall: 0.9017\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CELL 17: Save Final Model and Results"
      ],
      "metadata": {
        "id": "CohmA6OQFQKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 17: Save Final Model and Results\n",
        "\n",
        "import json\n",
        "from datetime import datetime\n",
        "\n",
        "# Create results summary\n",
        "results = {\n",
        "    'model_name': 'SAKT',\n",
        "    'dataset': 'ASSISTments2009',\n",
        "    'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
        "    'hyperparameters': {\n",
        "        'num_skills': 145,\n",
        "        'embed_dim': 128,\n",
        "        'num_heads': 5,\n",
        "        'dropout': 0.2,\n",
        "        'learning_rate': 0.001,\n",
        "        'batch_size': 64,\n",
        "        'max_seq_len': 100\n",
        "    },\n",
        "    'results': {\n",
        "        'best_val_auc': float(checkpoint['best_val_auc']),\n",
        "        'test_auc': float(test_auc),\n",
        "        'test_accuracy': float(accuracy),\n",
        "        'test_precision': float(precision),\n",
        "        'test_recall': float(recall),\n",
        "        'test_f1': float(f1),\n",
        "        'total_epochs': len(history['train_loss']),\n",
        "        'best_epoch': checkpoint['epoch']\n",
        "    },\n",
        "    'data_stats': {\n",
        "        'train_students': len(data['train']),\n",
        "        'val_students': len(data['val']),\n",
        "        'test_students': len(data['test']),\n",
        "        'unique_skills': data['num_skills']\n",
        "    }\n",
        "}\n",
        "\n",
        "# Save results to JSON\n",
        "with open('sakt_results.json', 'w') as f:\n",
        "    json.dump(results, f, indent=4)\n",
        "\n",
        "# Save final model with all information\n",
        "final_save = {\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'optimizer_state_dict': optimizer.state_dict(),\n",
        "    'results': results,\n",
        "    'history': history,\n",
        "    'skill_to_idx': data['skill_to_idx']\n",
        "}\n",
        "\n",
        "torch.save(final_save, 'sakt_final_model.pth')\n",
        "\n",
        "print(\"Model and results saved!\")\n",
        "print(f\"\\nSummary for thesis:\")\n",
        "print(f\"- SAKT achieved {test_auc:.4f} AUC on ASSISTments2009\")\n",
        "print(f\"- Model has {sum(p.numel() for p in model.parameters()):,} parameters\")\n",
        "print(f\"- Training took {len(history['train_loss'])} epochs\")"
      ],
      "metadata": {
        "id": "K2aDuaUnFSN6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Created this cell to perform a Hyperparameter search for the improving the AUC of the SAKT paper"
      ],
      "metadata": {
        "id": "IsIdj8XHdL5B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL: Hyperparameter Search for SAKT\n",
        "\n",
        "import itertools\n",
        "from datetime import datetime\n",
        "import json\n",
        "\n",
        "# Define hyperparameter search space\n",
        "param_grid = {\n",
        "    'embed_dim': [50, 100, 150, 200],  # Paper tested these\n",
        "    'num_heads': [5],  # Keep at 5 as per paper\n",
        "    'dropout': [0.1, 0.2, 0.3],\n",
        "    'lr': [5e-4, 1e-3, 2e-3],\n",
        "    'weight_decay': [0, 1e-5, 1e-4],\n",
        "    'batch_size': [64, 128],  # Paper mentions 128 for ASSIST2009\n",
        "}\n",
        "\n",
        "# Generate all combinations\n",
        "param_combinations = list(itertools.product(*param_grid.values()))\n",
        "param_names = list(param_grid.keys())\n",
        "\n",
        "print(f\"Total hyperparameter combinations to test: {len(param_combinations)}\")\n",
        "\n",
        "# Function to train with specific hyperparameters\n",
        "def train_with_params(params_dict, max_epochs=20, patience=5):\n",
        "    \"\"\"\n",
        "    Train SAKT with given hyperparameters and return best validation AUC\n",
        "    \"\"\"\n",
        "    # Create model with specified parameters\n",
        "    model = SAKT(\n",
        "        num_skills=145,\n",
        "        embed_dim=params_dict['embed_dim'],\n",
        "        num_heads=params_dict['num_heads'],\n",
        "        dropout=params_dict['dropout']\n",
        "    ).to(device)\n",
        "\n",
        "    # Create optimizer with specified learning rate\n",
        "    optimizer = optim.Adam(\n",
        "        model.parameters(),\n",
        "        lr=params_dict['lr'],\n",
        "        weight_decay=params_dict['weight_decay']\n",
        "    )\n",
        "\n",
        "    # Recreate data loaders with specified batch size\n",
        "    train_loader_hp = DataLoader(train_dataset, batch_size=params_dict['batch_size'], shuffle=True)\n",
        "    val_loader_hp = DataLoader(val_dataset, batch_size=params_dict['batch_size'], shuffle=False)\n",
        "\n",
        "    # Loss function\n",
        "    criterion = nn.BCELoss(reduction='none')\n",
        "\n",
        "    # Training loop with early stopping\n",
        "    best_val_auc = 0\n",
        "    patience_counter = 0\n",
        "\n",
        "    for epoch in range(max_epochs):\n",
        "        # Train epoch\n",
        "        train_loss, train_auc = train_epoch(model, train_loader_hp, optimizer, criterion, device)\n",
        "\n",
        "        # Validate\n",
        "        val_loss, val_auc = validate_epoch(model, val_loader_hp, criterion, device)\n",
        "\n",
        "        # Check if improved\n",
        "        if val_auc > best_val_auc:\n",
        "            best_val_auc = val_auc\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "\n",
        "        # Early stopping\n",
        "        if patience_counter >= patience:\n",
        "            break\n",
        "\n",
        "    return best_val_auc, epoch + 1\n",
        "\n",
        "# Run hyperparameter search\n",
        "results = []\n",
        "best_auc = 0\n",
        "best_params = None\n",
        "\n",
        "print(\"\\nStarting hyperparameter search...\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Test a subset first (remove this limit for full search)\n",
        "# For full search, remove the [:10] to test all combinations\n",
        "for i, param_values in enumerate(param_combinations):  # Testing all combinations\n",
        "    params_dict = dict(zip(param_names, param_values))\n",
        "\n",
        "    print(f\"\\nTesting combination {i+1}/{len(param_combinations)}\")\n",
        "    print(f\"Parameters: {params_dict}\")\n",
        "\n",
        "    try:\n",
        "        # Train with these parameters\n",
        "        val_auc, epochs_trained = train_with_params(params_dict, max_epochs=15, patience=3)\n",
        "\n",
        "        # Store results\n",
        "        result = {\n",
        "            'params': params_dict,\n",
        "            'val_auc': val_auc,\n",
        "            'epochs': epochs_trained\n",
        "        }\n",
        "        results.append(result)\n",
        "\n",
        "        print(f\"Validation AUC: {val_auc:.4f} (trained for {epochs_trained} epochs)\")\n",
        "\n",
        "        # Track best\n",
        "        if val_auc > best_auc:\n",
        "            best_auc = val_auc\n",
        "            best_params = params_dict\n",
        "            print(f\"âœ“ New best AUC!\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error with parameters {params_dict}: {str(e)}\")\n",
        "        continue\n",
        "\n",
        "# Sort results by AUC\n",
        "results.sort(key=lambda x: x['val_auc'], reverse=True)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"HYPERPARAMETER SEARCH RESULTS\")\n",
        "print(\"=\"*80)\n",
        "print(f\"\\nBest parameters found:\")\n",
        "print(f\"AUC: {best_auc:.4f}\")\n",
        "for param, value in best_params.items():\n",
        "    print(f\"{param}: {value}\")\n",
        "\n",
        "print(\"\\nTop 5 configurations:\")\n",
        "for i, result in enumerate(results[:5]):\n",
        "    print(f\"\\n{i+1}. AUC: {result['val_auc']:.4f}\")\n",
        "    for param, value in result['params'].items():\n",
        "        print(f\"   {param}: {value}\")\n",
        "\n",
        "# Save results\n",
        "with open('hyperparameter_search_results.json', 'w') as f:\n",
        "    json.dump(results, f, indent=2)"
      ],
      "metadata": {
        "id": "YckB_GGPW2L7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 17: Examine Available Features for Enhancement\n",
        "\n",
        "# Load your data and check columns\n",
        "print(\"Available columns in ASSIST2009:\")\n",
        "print(df.columns.tolist())\n",
        "\n",
        "# Check for key features we want to use\n",
        "key_features = ['ms_first_response', 'hint_count', 'attempt_count', 'overlap_time']\n",
        "available_features = [f for f in key_features if f in df.columns]\n",
        "print(f\"\\nAvailable enhancement features: {available_features}\")\n",
        "\n",
        "# Check data completeness for these features\n",
        "for feature in available_features:\n",
        "    if feature in df.columns:\n",
        "        missing_pct = df[feature].isna().sum() / len(df) * 100\n",
        "        print(f\"{feature}: {missing_pct:.1f}% missing\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tG8IQBhjCUfS",
        "outputId": "68fa45e1-9f0f-45c2-d0bc-63ad2c52b73c"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Available columns in ASSIST2009:\n",
            "['Unnamed: 0', 'order_id', 'assignment_id', 'user_id', 'assistment_id', 'problem_id', 'original', 'correct', 'attempt_count', 'ms_first_response', 'tutor_mode', 'answer_type', 'sequence_id', 'student_class_id', 'position', 'type', 'base_sequence_id', 'skill_id', 'skill_name', 'teacher_id', 'school_id', 'hint_count', 'hint_total', 'overlap_time', 'template_id', 'answer_id', 'answer_text', 'first_action', 'bottom_hint', 'opportunity', 'opportunity_original']\n",
            "\n",
            "Available enhancement features: ['ms_first_response', 'hint_count', 'attempt_count', 'overlap_time']\n",
            "ms_first_response: 0.0% missing\n",
            "hint_count: 0.0% missing\n",
            "attempt_count: 0.0% missing\n",
            "overlap_time: 0.0% missing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 18: Enhanced Data Preprocessing for DA-SAKT\n",
        "\n",
        "def create_enhanced_sequences(df_clean, skill_to_idx, num_skills):\n",
        "    \"\"\"Create sequences with additional features for enhanced SAKT\"\"\"\n",
        "    sequences = []\n",
        "\n",
        "    for user_id, user_data in df_clean.groupby('user_id'):\n",
        "        # Basic features\n",
        "        skills = user_data['skill_id'].values\n",
        "        corrects = user_data['correct'].values\n",
        "        skill_indices = [skill_to_idx[skill] for skill in skills]\n",
        "\n",
        "        # Enhanced features\n",
        "        # Response time (normalize to 0-1 range)\n",
        "        if 'ms_first_response' in user_data.columns:\n",
        "            response_times = user_data['ms_first_response'].fillna(user_data['ms_first_response'].median()).values\n",
        "            # Clip extreme values and normalize\n",
        "            response_times = np.clip(response_times, 0, 300000)  # Max 5 minutes\n",
        "            response_times = response_times / 300000.0\n",
        "        else:\n",
        "            response_times = np.ones(len(skills)) * 0.5  # Default middle difficulty\n",
        "\n",
        "        # Hint usage (normalize by max hints)\n",
        "        if 'hint_count' in user_data.columns:\n",
        "            hint_counts = user_data['hint_count'].fillna(0).values\n",
        "            hint_counts = np.clip(hint_counts, 0, 5) / 5.0\n",
        "        else:\n",
        "            hint_counts = np.zeros(len(skills))\n",
        "\n",
        "        # Attempt count\n",
        "        if 'attempt_count' in user_data.columns:\n",
        "            attempt_counts = user_data['attempt_count'].fillna(1).values\n",
        "            attempt_counts = np.clip(attempt_counts, 1, 5) / 5.0\n",
        "        else:\n",
        "            attempt_counts = np.ones(len(skills)) * 0.2\n",
        "\n",
        "        # Create interactions\n",
        "        interactions = []\n",
        "        for skill_idx, correct in zip(skill_indices, corrects):\n",
        "            interaction = skill_idx + (correct * num_skills)\n",
        "            interactions.append(interaction)\n",
        "\n",
        "        sequences.append({\n",
        "            'user_id': user_id,\n",
        "            'skill_indices': skill_indices,\n",
        "            'corrects': corrects,\n",
        "            'interactions': interactions,\n",
        "            'response_times': response_times,\n",
        "            'hint_counts': hint_counts,\n",
        "            'attempt_counts': attempt_counts,\n",
        "            'length': len(interactions)\n",
        "        })\n",
        "\n",
        "    return sequences\n",
        "\n",
        "# Create enhanced sequences\n",
        "print(\"Creating enhanced sequences...\")\n",
        "enhanced_sequences = create_enhanced_sequences(df_clean, skill_to_idx, num_skills)\n",
        "\n",
        "# Split data (80/20)\n",
        "from sklearn.model_selection import train_test_split\n",
        "train_sequences_enh, test_sequences_enh = train_test_split(\n",
        "    enhanced_sequences,\n",
        "    test_size=0.2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "print(f\"Enhanced sequences created:\")\n",
        "print(f\"- Train: {len(train_sequences_enh)} students\")\n",
        "print(f\"- Test: {len(test_sequences_enh)} students\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NdnNGQqUCbWj",
        "outputId": "5727d81a-70b8-4037-d280-bc49efb50c9c"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating enhanced sequences...\n",
            "Enhanced sequences created:\n",
            "- Train: 2711 students\n",
            "- Test: 678 students\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 19: Difficulty-Aware SAKT Model\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class DifficultyAwareSAKT(nn.Module):\n",
        "    def __init__(self, num_skills, embed_dim=100, num_heads=5, dropout=0.2, num_features=3):\n",
        "        \"\"\"\n",
        "        Enhanced SAKT with difficulty awareness and multi-feature integration\n",
        "\n",
        "        Args:\n",
        "            num_features: Number of additional features (response_time, hints, attempts)\n",
        "        \"\"\"\n",
        "        super(DifficultyAwareSAKT, self).__init__()\n",
        "\n",
        "        self.num_skills = num_skills\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "        # Original SAKT embeddings\n",
        "        self.interaction_embed = nn.Embedding(\n",
        "            num_skills * 2 + 1,\n",
        "            embed_dim,\n",
        "            padding_idx=num_skills * 2\n",
        "        )\n",
        "        self.skill_embed = nn.Embedding(\n",
        "            num_skills + 1,\n",
        "            embed_dim,\n",
        "            padding_idx=0\n",
        "        )\n",
        "        self.pos_embed = nn.Embedding(1000, embed_dim)\n",
        "\n",
        "        # NEW: Feature embeddings\n",
        "        self.feature_proj = nn.Linear(num_features, embed_dim // 4)\n",
        "\n",
        "        # NEW: Difficulty-aware projection\n",
        "        self.difficulty_proj = nn.Linear(embed_dim + embed_dim // 4, embed_dim)\n",
        "\n",
        "        # Enhanced attention with difficulty awareness\n",
        "        self.attention = nn.MultiheadAttention(\n",
        "            embed_dim,\n",
        "            num_heads,\n",
        "            dropout=dropout,\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        # NEW: Sparse attention gate (learns which interactions to focus on)\n",
        "        self.attention_gate = nn.Sequential(\n",
        "            nn.Linear(embed_dim * 2, embed_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(embed_dim, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "        # Layer normalization\n",
        "        self.layer_norm1 = nn.LayerNorm(embed_dim)\n",
        "        self.layer_norm2 = nn.LayerNorm(embed_dim)\n",
        "\n",
        "        # Feed-forward network\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(embed_dim, embed_dim * 4),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(embed_dim * 4, embed_dim)\n",
        "        )\n",
        "\n",
        "        # Output prediction with difficulty awareness\n",
        "        self.pred = nn.Linear(embed_dim, 1)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, interactions, skills, features):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            interactions: [batch_size, seq_len]\n",
        "            skills: [batch_size, seq_len]\n",
        "            features: [batch_size, seq_len, num_features] - response_time, hints, attempts\n",
        "        \"\"\"\n",
        "        batch_size, seq_len = interactions.shape\n",
        "\n",
        "        # Position indices\n",
        "        positions = torch.arange(seq_len, device=interactions.device).unsqueeze(0).expand(batch_size, -1)\n",
        "\n",
        "        # Embed interactions and skills\n",
        "        interaction_embeds = self.interaction_embed(interactions)\n",
        "        skill_embeds = self.skill_embed(skills)\n",
        "\n",
        "        # NEW: Project features and combine with embeddings\n",
        "        feature_embeds = self.feature_proj(features)  # [batch, seq_len, embed_dim//4]\n",
        "\n",
        "        # Enhance interaction embeddings with features (difficulty-aware)\n",
        "        enhanced_interactions = torch.cat([interaction_embeds, feature_embeds], dim=-1)\n",
        "        enhanced_interactions = self.difficulty_proj(enhanced_interactions)\n",
        "\n",
        "        # Add positional embeddings\n",
        "        enhanced_interactions = enhanced_interactions + self.pos_embed(positions)\n",
        "        skill_embeds = skill_embeds + self.pos_embed(positions)\n",
        "\n",
        "        # Causal mask\n",
        "        attn_mask = torch.triu(\n",
        "            torch.ones(seq_len, seq_len, device=interactions.device) * float('-inf'),\n",
        "            diagonal=1\n",
        "        )\n",
        "\n",
        "        # Apply attention\n",
        "        attended, attn_weights = self.attention(\n",
        "            query=skill_embeds,\n",
        "            key=enhanced_interactions,\n",
        "            value=enhanced_interactions,\n",
        "            attn_mask=attn_mask,\n",
        "            need_weights=True\n",
        "        )\n",
        "\n",
        "        # NEW: Apply sparse attention gate\n",
        "        gate_input = torch.cat([skill_embeds, attended], dim=-1)\n",
        "        gate_weights = self.attention_gate(gate_input)\n",
        "        attended = attended * gate_weights\n",
        "\n",
        "        # Residual connection and layer norm\n",
        "        attended = self.layer_norm1(skill_embeds + self.dropout(attended))\n",
        "\n",
        "        # Feed-forward network\n",
        "        ffn_out = self.ffn(attended)\n",
        "        ffn_out = self.layer_norm2(attended + self.dropout(ffn_out))\n",
        "\n",
        "        # Predict\n",
        "        pred = self.pred(ffn_out).squeeze(-1)\n",
        "        return torch.sigmoid(pred)\n",
        "\n",
        "# Test the enhanced model\n",
        "print(\"Testing Difficulty-Aware SAKT...\")\n",
        "model_enhanced = DifficultyAwareSAKT(num_skills=145)\n",
        "print(f\"Enhanced model parameters: {sum(p.numel() for p in model_enhanced.parameters()):,}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "14ir5gkkCjyb",
        "outputId": "278ce0e2-0ac7-45cc-a2ee-533c591acd62"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing Difficulty-Aware SAKT...\n",
            "Enhanced model parameters: 298,002\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 20: Enhanced Dataset for DA-SAKT\n",
        "\n",
        "class EnhancedSAKTDataset(Dataset):\n",
        "    def __init__(self, sequences, max_seq_len=100, num_skills=145):\n",
        "        self.max_seq_len = max_seq_len\n",
        "        self.num_skills = num_skills\n",
        "        self.padding_interaction = num_skills * 2\n",
        "\n",
        "        # Process sequences\n",
        "        self.data = []\n",
        "        for seq in sequences:\n",
        "            if len(seq['interactions']) > max_seq_len:\n",
        "                # Split long sequences\n",
        "                for i in range(0, len(seq['interactions']), max_seq_len):\n",
        "                    end_idx = min(i + max_seq_len, len(seq['interactions']))\n",
        "                    self.data.append({\n",
        "                        'interactions': seq['interactions'][i:end_idx],\n",
        "                        'skills': seq['skill_indices'][i:end_idx],\n",
        "                        'corrects': seq['corrects'][i:end_idx],\n",
        "                        'response_times': seq['response_times'][i:end_idx],\n",
        "                        'hint_counts': seq['hint_counts'][i:end_idx],\n",
        "                        'attempt_counts': seq['attempt_counts'][i:end_idx]\n",
        "                    })\n",
        "            else:\n",
        "                self.data.append({\n",
        "                    'interactions': seq['interactions'],\n",
        "                    'skills': seq['skill_indices'],\n",
        "                    'corrects': seq['corrects'],\n",
        "                    'response_times': seq['response_times'],\n",
        "                    'hint_counts': seq['hint_counts'],\n",
        "                    'attempt_counts': seq['attempt_counts']\n",
        "                })\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        seq = self.data[idx]\n",
        "        seq_len = len(seq['interactions'])\n",
        "\n",
        "        # Convert to lists and apply shifting\n",
        "        interactions = list(seq['interactions'])\n",
        "        skills = list(seq['skills'])\n",
        "        corrects = list(seq['corrects'])\n",
        "        response_times = list(seq['response_times'])\n",
        "        hint_counts = list(seq['hint_counts'])\n",
        "        attempt_counts = list(seq['attempt_counts'])\n",
        "\n",
        "        # Shift interactions\n",
        "        shifted_interactions = []\n",
        "        shifted_features = []\n",
        "        for i in range(seq_len):\n",
        "            if i == 0:\n",
        "                shifted_interactions.append(self.padding_interaction)\n",
        "                shifted_features.append([0.5, 0.0, 0.2])  # Default features\n",
        "            else:\n",
        "                shifted_interactions.append(interactions[i-1])\n",
        "                shifted_features.append([\n",
        "                    response_times[i-1],\n",
        "                    hint_counts[i-1],\n",
        "                    attempt_counts[i-1]\n",
        "                ])\n",
        "\n",
        "        interactions = shifted_interactions\n",
        "        features = shifted_features\n",
        "\n",
        "        # Pad to the LEFT\n",
        "        if seq_len < self.max_seq_len:\n",
        "            pad_len = self.max_seq_len - seq_len\n",
        "\n",
        "            interactions = [self.padding_interaction] * pad_len + interactions\n",
        "            skills = [0] * pad_len + skills\n",
        "            corrects = [0] * pad_len + corrects\n",
        "            features = [[0.5, 0.0, 0.2]] * pad_len + features\n",
        "            mask = [0] * pad_len + [1] * seq_len\n",
        "        else:\n",
        "            mask = [1] * self.max_seq_len\n",
        "\n",
        "        return {\n",
        "            'interactions': torch.tensor(interactions, dtype=torch.long),\n",
        "            'skills': torch.tensor(skills, dtype=torch.long),\n",
        "            'targets': torch.tensor(corrects, dtype=torch.float),\n",
        "            'features': torch.tensor(features, dtype=torch.float),\n",
        "            'mask': torch.tensor(mask, dtype=torch.float)\n",
        "        }\n",
        "\n",
        "# Create enhanced datasets\n",
        "train_dataset_enh = EnhancedSAKTDataset(train_sequences_enh, max_seq_len=100, num_skills=num_skills)\n",
        "test_dataset_enh = EnhancedSAKTDataset(test_sequences_enh, max_seq_len=100, num_skills=num_skills)\n",
        "\n",
        "# Create dataloaders\n",
        "train_loader_enh = DataLoader(train_dataset_enh, batch_size=64, shuffle=True)\n",
        "test_loader_enh = DataLoader(test_dataset_enh, batch_size=64, shuffle=False)\n",
        "\n",
        "print(f\"Enhanced datasets created:\")\n",
        "print(f\"Train batches: {len(train_loader_enh)}\")\n",
        "print(f\"Test batches: {len(test_loader_enh)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t96ORb09Cq2f",
        "outputId": "38200087-3910-479d-c472-fe068fe16e5b"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enhanced datasets created:\n",
            "Train batches: 51\n",
            "Test batches: 13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 21: Train and Compare Models\n",
        "\n",
        "# Modified training functions for enhanced model\n",
        "def train_epoch_enhanced(model, train_loader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    all_predictions = []\n",
        "    all_targets = []\n",
        "\n",
        "    for batch in tqdm(train_loader, desc='Training', leave=False):\n",
        "        interactions = batch['interactions'].to(device)\n",
        "        skills = batch['skills'].to(device)\n",
        "        targets = batch['targets'].to(device)\n",
        "        features = batch['features'].to(device)\n",
        "        mask = batch['mask'].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass with features\n",
        "        predictions = model(interactions, skills, features)\n",
        "\n",
        "        # Masked loss\n",
        "        loss = criterion(predictions, targets)\n",
        "        masked_loss = (loss * mask).sum() / mask.sum()\n",
        "\n",
        "        masked_loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += masked_loss.item()\n",
        "\n",
        "        # Collect predictions\n",
        "        valid_idx = mask == 1\n",
        "        valid_predictions = predictions[valid_idx].detach().cpu().numpy()\n",
        "        valid_targets = targets[valid_idx].detach().cpu().numpy()\n",
        "\n",
        "        all_predictions.extend(valid_predictions)\n",
        "        all_targets.extend(valid_targets)\n",
        "\n",
        "    epoch_loss = total_loss / len(train_loader)\n",
        "    epoch_auc = roc_auc_score(all_targets, all_predictions)\n",
        "\n",
        "    return epoch_loss, epoch_auc\n",
        "\n",
        "# Train enhanced model\n",
        "print(\"Training Difficulty-Aware SAKT...\")\n",
        "model_enhanced = DifficultyAwareSAKT(\n",
        "    num_skills=145,\n",
        "    embed_dim=100,\n",
        "    num_heads=5,\n",
        "    dropout=0.2,\n",
        "    num_features=3\n",
        ").to(device)\n",
        "\n",
        "optimizer_enh = optim.Adam(model_enhanced.parameters(), lr=1e-3, weight_decay=1e-4)\n",
        "criterion = nn.BCELoss(reduction='none')\n",
        "\n",
        "# Training loop\n",
        "NUM_EPOCHS = 50\n",
        "best_test_auc = 0\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    train_loss, train_auc = train_epoch_enhanced(\n",
        "        model_enhanced, train_loader_enh, optimizer_enh, criterion, device\n",
        "    )\n",
        "\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        print(f\"Epoch {epoch+1}: Train Loss={train_loss:.4f}, Train AUC={train_auc:.4f}\")\n",
        "\n",
        "    # Save final model\n",
        "    if epoch + 1 == NUM_EPOCHS:\n",
        "        torch.save({\n",
        "            'model_state_dict': model_enhanced.state_dict(),\n",
        "            'train_auc': train_auc\n",
        "        }, 'enhanced_sakt_model.pth')\n",
        "\n",
        "# Evaluate both models\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"MODEL COMPARISON\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Load baseline model\n",
        "baseline_checkpoint = torch.load('best_sakt_model.pth', weights_only=False)\n",
        "model_baseline = SAKT(num_skills=145, embed_dim=100, num_heads=5, dropout=0.2).to(device)\n",
        "model_baseline.load_state_dict(baseline_checkpoint['model_state_dict'])\n",
        "\n",
        "# Evaluate baseline\n",
        "print(\"\\nBaseline SAKT:\")\n",
        "baseline_test_loss, baseline_test_auc = validate_epoch(\n",
        "    model_baseline, test_loader, criterion, device\n",
        ")\n",
        "print(f\"Test AUC: {baseline_test_auc:.4f}\")\n",
        "\n",
        "# Evaluate enhanced model\n",
        "print(\"\\nDifficulty-Aware SAKT:\")\n",
        "# Create a validation function for enhanced model\n",
        "def validate_enhanced(model, test_loader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    all_predictions = []\n",
        "    all_targets = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            interactions = batch['interactions'].to(device)\n",
        "            skills = batch['skills'].to(device)\n",
        "            targets = batch['targets'].to(device)\n",
        "            features = batch['features'].to(device)\n",
        "            mask = batch['mask'].to(device)\n",
        "\n",
        "            predictions = model(interactions, skills, features)\n",
        "\n",
        "            loss = criterion(predictions, targets)\n",
        "            masked_loss = (loss * mask).sum() / mask.sum()\n",
        "            total_loss += masked_loss.item()\n",
        "\n",
        "            valid_idx = mask == 1\n",
        "            valid_predictions = predictions[valid_idx].cpu().numpy()\n",
        "            valid_targets = targets[valid_idx].cpu().numpy()\n",
        "\n",
        "            all_predictions.extend(valid_predictions)\n",
        "            all_targets.extend(valid_targets)\n",
        "\n",
        "    test_loss = total_loss / len(test_loader)\n",
        "    test_auc = roc_auc_score(all_targets, all_predictions)\n",
        "\n",
        "    return test_loss, test_auc\n",
        "\n",
        "enhanced_test_loss, enhanced_test_auc = validate_enhanced(\n",
        "    model_enhanced, test_loader_enh, criterion, device\n",
        ")\n",
        "print(f\"Test AUC: {enhanced_test_auc:.4f}\")\n",
        "\n",
        "# Calculate improvement\n",
        "improvement = (enhanced_test_auc - baseline_test_auc) / baseline_test_auc * 100\n",
        "print(f\"\\nImprovement: {improvement:+.2f}%\")\n",
        "print(f\"Absolute gain: {enhanced_test_auc - baseline_test_auc:+.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v1UKWO09CvuQ",
        "outputId": "1babae32-058b-4521-ad8d-c9540925cfd1"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Difficulty-Aware SAKT...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10: Train Loss=0.5309, Train AUC=0.7580\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 20: Train Loss=0.5141, Train AUC=0.7769\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 30: Train Loss=0.5042, Train AUC=0.7877\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 40: Train Loss=0.4960, Train AUC=0.7962\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 50: Train Loss=0.4896, Train AUC=0.8025\n",
            "\n",
            "============================================================\n",
            "MODEL COMPARISON\n",
            "============================================================\n",
            "\n",
            "Baseline SAKT:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test AUC: 0.7178\n",
            "\n",
            "Difficulty-Aware SAKT:\n",
            "Test AUC: 0.7280\n",
            "\n",
            "Improvement: +1.41%\n",
            "Absolute gain: +0.0101\n"
          ]
        }
      ]
    }
  ]
}