{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AmurdAmzer/SAKT-Paper_Implementation/blob/main/SAKTPaperImplementation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3Mjf06XlwkI"
      },
      "source": [
        "# Cell 1: Uploading data via Colab's File Browser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LacccNggl2gy",
        "outputId": "dd3215b6-2f3d-4699-9886-2f267357c121"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files in current directory:\n",
            " - skill_builder_data_corrected_collapsed.csv\n"
          ]
        }
      ],
      "source": [
        "# Cell 1: Uploading data via Colab's File Browser\n",
        "\n",
        "# os stands for \"Operating System\" - it's like a special toolkit that lets your Python code talk to your computer's file system.\n",
        "import os\n",
        "print(\"Files in current directory:\")\n",
        "for file in os.listdir():\n",
        "  if file.endswith(\".csv\"):\n",
        "    print(f\" - {file}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "soB_jKNd5A3M"
      },
      "source": [
        "#Cell 2: Imports and File upload"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l7PycBC01iR8",
        "outputId": "a1386d28-58d5-408e-fe07-eda5a9a30adf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (346860, 31)\n",
            "\n",
            "Column names: ['Unnamed: 0', 'order_id', 'assignment_id', 'user_id', 'assistment_id', 'problem_id', 'original', 'correct', 'attempt_count', 'ms_first_response', 'tutor_mode', 'answer_type', 'sequence_id', 'student_class_id', 'position', 'type', 'base_sequence_id', 'skill_id', 'skill_name', 'teacher_id', 'school_id', 'hint_count', 'hint_total', 'overlap_time', 'template_id', 'answer_id', 'answer_text', 'first_action', 'bottom_hint', 'opportunity', 'opportunity_original']\n",
            "\n",
            "First 5 rows:    Unnamed: 0  order_id  assignment_id  user_id  assistment_id  problem_id  \\\n",
            "0           1  33022537         277618    64525          33139       51424   \n",
            "1           2  33022709         277618    64525          33150       51435   \n",
            "2           3  35450204         220674    70363          33159       51444   \n",
            "3           4  35450295         220674    70363          33110       51395   \n",
            "4           5  35450311         220674    70363          33196       51481   \n",
            "\n",
            "   original  correct  attempt_count  ms_first_response  ... hint_count  \\\n",
            "0         1        1              1              32454  ...          0   \n",
            "1         1        1              1               4922  ...          0   \n",
            "2         1        0              2              25390  ...          0   \n",
            "3         1        1              1               4859  ...          0   \n",
            "4         1        0             14              19813  ...          3   \n",
            "\n",
            "  hint_total  overlap_time  template_id  answer_id answer_text  first_action  \\\n",
            "0          3         32454        30799        NaN          26             0   \n",
            "1          3          4922        30799        NaN          55             0   \n",
            "2          3         42000        30799        NaN          88             0   \n",
            "3          3          4859        30059        NaN          41             0   \n",
            "4          4        124564        30060        NaN          65             0   \n",
            "\n",
            "  bottom_hint opportunity  opportunity_original  \n",
            "0         NaN           1                   1.0  \n",
            "1         NaN           2                   2.0  \n",
            "2         NaN           1                   1.0  \n",
            "3         NaN           2                   2.0  \n",
            "4         0.0           3                   3.0  \n",
            "\n",
            "[5 rows x 31 columns]\n"
          ]
        }
      ],
      "source": [
        "# Cell 2: Imports and File upload (df = pd.read_csv(....)) opens the spreadsheet(data) and puts it into a \"DataFrame\"\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Load the CSV file (Remember to use the actual file name)\n",
        "# encoding='ISO-8859-1' handles special characters in the data\n",
        "# low_memory=False prevents dtype warnings for mixed types\n",
        "\n",
        "# I choose to call my loaded data df, df can mean DataFrame, df is just a variable name, I can choose to name my uploaded data anything.\n",
        "# low_memory=False tells the computer \"take your time reading this properly, don't rush\"\n",
        "# encoding='ISO-8859-1' = Like telling your computer \"hey, this file might have special characters like currency symbols ($ etc.)\".\n",
        "#  Real-Life Analogy\n",
        "\"\"\"\n",
        "It's like telling your computer:\n",
        "When you read this file, treat these bytes or symbols as Latin-style letters â€” not random gibberish.\"\n",
        "\n",
        "If you don't specify the correct encoding, Python might fail to read the file\n",
        "\"\"\"\n",
        "\n",
        "df = pd.read_csv('skill_builder_data_corrected_collapsed.csv', encoding='ISO-8859-1', low_memory=False)\n",
        "\n",
        "# Show basic information about the dataset\n",
        "print(f\"Dataset shape: {df.shape}\") # (rows, columns)\n",
        "print(f\"\\nColumn names: {list(df.columns)}\") # all column names\n",
        "print(f\"\\nFirst 5 rows: {df.head()}\") # preview first 5 rows. head() has a default parameter built in df.head(n=5). you could specify by df.head(5) etc.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ir9cX3Pr5Ncc"
      },
      "source": [
        "# Cell3: Key Statistics - Understanding the dataset size and scope"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cqUdL-mz24hc",
        "outputId": "d4ed96d0-0d5a-418f-9eb6-f72a6aad7e7c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===DATASET OVERVIEW===\n",
            "Toatal interactions: 346860\n",
            "Unique students: 4217\n",
            "Unique problems: 26688\n",
            "Unique skills: 149\n",
            "\n",
            "Correct rate: 64.53%\n",
            "Rows with skill_id: 283105\n",
            "Rows missing skill_id: 63755 (18.4%)\n"
          ]
        }
      ],
      "source": [
        "# Cell 3: Key Statistics - Understanding the dataset size and scope\n",
        "\n",
        "print(\"===DATASET OVERVIEW===\")\n",
        "\n",
        "# Count total number of student interactions = number of rows\n",
        "print(f\"Toatal interactions: {len(df)}\")\n",
        "\n",
        "# Count unique students - each student has a unique user_id\n",
        "print(f\"Unique students: {df['user_id'].nunique()}\") # df['user_id] = access the user id column. nunique = count how many unique values are in that column\n",
        "\n",
        "# Count unique problems - individual questions students attempted\n",
        "print(f\"Unique problems: {df['problem_id'].nunique()}\")\n",
        "\n",
        "# Count unique skills - knowledge concepts being tested. this is crucial because I will create embeddings for each skill\n",
        "print(f\"Unique skills: {df['skill_id'].nunique()}\")\n",
        "\n",
        "# Calculate overall performance - percentage of correct answers\n",
        "print(f\"\\nCorrect rate: {df['correct'].mean():.2%}\")  # .2% means format the number as percentage with two decimal places, so for eg. 0.825641 becomes 82.56%\n",
        "\n",
        "# Check data completeness for skill_id (Critical for SAKT)\n",
        "# NB. SAKT needs skill_id to work - rows without the skill_id must be removed\n",
        "print(f\"Rows with skill_id: {df['skill_id'].notna().sum()}\")\n",
        "print(f\"Rows missing skill_id: {df['skill_id'].isna().sum()} ({df['skill_id'].isna().mean():.1%})\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QhKU22wsrOMe"
      },
      "source": [
        "# CELL 4: Examine One Student's Learning Journey (Creating a case study).\n",
        "# Before SAKT learns from all students, you want to see what one student's learning path looks like\n",
        "# This helps to understand the sequential nature of the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8nUiiGtprW7R",
        "outputId": "e8f37e1b-59b1-4893-d712-546420e7c5dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Student 96235 attempted 977 problems\n",
            "Skills attempted: 87\n",
            "Correct rate: 58.03%\n",
            "\n",
            "First 30 attempts:\n",
            "        order_id skill_id  correct  ms_first_response\n",
            "96959   38171250       49        0               6228\n",
            "96960   38171251       49        1               4544\n",
            "96961   38171252       49        1               6059\n",
            "96962   38171253       49        0               6154\n",
            "96963   38171254       49        0              11896\n",
            "96964   38171255       49        1              11180\n",
            "96965   38171256       49        0               5603\n",
            "96966   38171257       49        0               5152\n",
            "96967   38171258       49        0               8184\n",
            "96968   38171259       49        0               3551\n",
            "96969   38171260       49        1               7122\n",
            "96970   38171261       49        1               4576\n",
            "96971   38171262       49        0               4139\n",
            "96972   38171263       49        0               6022\n",
            "96973   38171264       49        0              14956\n",
            "96974   38171265       49        1               9418\n",
            "96975   38171266       49        1              11435\n",
            "96976   38171267       49        0               6624\n",
            "96977   38171268       49        0               6217\n",
            "96978   38171269       49        1               9203\n",
            "259830  38171273      311        1             105282\n",
            "229704  38171275      307        1              15819\n",
            "259831  38171300      311        1              80594\n",
            "282263  38171301      368        0              64103\n",
            "282264  38171302      368        0             113526\n",
            "282265  38171303      368        1              67698\n",
            "282266  38171304      368        1              48215\n",
            "282267  38171305      368        1              92317\n",
            "60818   38171315       32        1              12908\n",
            "11447   38171316        4        0              15694\n"
          ]
        }
      ],
      "source": [
        "# Cell 4: Examine One Student's (random) Learning Journey\n",
        "# This helps us understand the sequential nature of the data\n",
        "\n",
        "# Find students sorted by number of attempts (most active students)\n",
        "student_activity = df['user_id'].value_counts()\n",
        "\n",
        "# Pick the 11th most active student (avoid outliers)\n",
        "student_id = student_activity.index[20]\n",
        "\n",
        "# Get all data for this student, sorted by time\n",
        "# Order_id represents the sequence of attempsts\n",
        "\n",
        "student_data = df[df['user_id'] == student_id].sort_values('order_id')\n",
        "\n",
        "# Display student summary\n",
        "print(f\"Student {student_id} attempted {len(student_data)} problems\")\n",
        "print(f\"Skills attempted: {student_data['skill_id'].nunique()}\")\n",
        "print(f\"Correct rate: {student_data['correct'].mean():.2%}\")\n",
        "\n",
        "# Show their first 30 attempts to see the sequential Pattern\n",
        "print(\"\\nFirst 30 attempts:\")\n",
        "print(student_data[['order_id', 'skill_id', 'correct', 'ms_first_response']].head(30))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4AOb7nX5wK_"
      },
      "source": [
        "# CELL 5: Visualize Key Patterns to Understand The Data Better"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WPbKjUOS5vWK",
        "outputId": "fe1af3e7-1413-4401-e579-463b1d68b454"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "top 20 Most Active Students:\n",
            "user_id: sequence length\n",
            "78978:     1383\n",
            "78970:     1334\n",
            "79032:     1264\n",
            "79021:     1239\n",
            "96274:     1236\n",
            "78979:     1163\n",
            "96244:     1149\n",
            "75169:     1129\n",
            "79013:     1124\n",
            "78989:     1115\n",
            "79029:     1112\n",
            "78980:     1112\n",
            "79019:     1095\n",
            "71881:     1089\n",
            "78987:     1084\n",
            "96243:     1083\n",
            "79031:     1064\n",
            "79018:     1041\n",
            "96265:     1014\n",
            "79012:     1005\n",
            "\n",
            "\n",
            "Buttom 20 Least Active Students:\n",
            "user_id: sequence length\n",
            "87330:     1\n",
            "87336:     1\n",
            "87337:     1\n",
            "91464:     1\n",
            "91439:     1\n",
            "87376:     1\n",
            "92527:     1\n",
            "85549:     1\n",
            "85553:     1\n",
            "85554:     1\n",
            "85555:     1\n",
            "85556:     1\n",
            "85558:     1\n",
            "85367:     1\n",
            "77725:     1\n",
            "74701:     1\n",
            "71163:     1\n",
            "84309:     1\n",
            "51933:     1\n",
            "84305:     1\n",
            "\n",
            "\n",
            "20 Random students sorted by length\n",
            "user_id: sequence length\n",
            "80957:     1\n",
            "90484:     3\n",
            "84962:     3\n",
            "87257:     5\n",
            "90703:     6\n",
            "83239:     11\n",
            "81225:     12\n",
            "86346:     19\n",
            "84118:     25\n",
            "88097:     30\n",
            "91961:     30\n",
            "87873:     46\n",
            "89208:     57\n",
            "78038:     64\n",
            "78356:     69\n",
            "89048:     74\n",
            "88777:     144\n",
            "78510:     146\n",
            "85730:     267\n",
            "78922:     719\n"
          ]
        }
      ],
      "source": [
        "# CELL 5: Visualing Key Patterns to Understand the Data Better\n",
        "\n",
        "# Calculate seuence length for each student (how many problems/quenstions/skills did each student attempt)\n",
        "\n",
        "seq_lengths = df.groupby('user_id').size()\n",
        "\n",
        "# seq_len for the Top 20 students\n",
        "print(\"top 20 Most Active Students:\")\n",
        "print(\"user_id: sequence length\")\n",
        "sorted_seq = seq_lengths.sort_values(ascending=False)\n",
        "for user_id, length in sorted_seq.head(20).items():\n",
        "  print(f\"{user_id}:     {length}\")\n",
        "\n",
        "print(\"\\n\")\n",
        "\n",
        "\n",
        "# seq_len for the Buttom 20 students\n",
        "print(\"Buttom 20 Least Active Students:\")\n",
        "print(\"user_id: sequence length\")\n",
        "sorted_seq = seq_lengths.sort_values(ascending=False)\n",
        "for user_id, length in sorted_seq.tail(20).items():\n",
        "  print(f\"{user_id}:     {length}\")\n",
        "\n",
        "print(\"\\n\")\n",
        "\n",
        "# Random 20 students sorted by length\n",
        "print(\"20 Random students sorted by length\")\n",
        "print(\"user_id: sequence length\")\n",
        "random_sample = seq_lengths.sample(20).sort_values() # If you remove the ascending argument, pandas uses the default, which is: ascending=True\n",
        "for user_id, length in random_sample.items():\n",
        "  print(f\"{user_id}:     {length}\")\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ATuGUM4053dg"
      },
      "source": [
        "# CELL 6: Data Quality Check - Critical for Reliable Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nh6WRXAH56qi",
        "outputId": "7410b50b-2ca6-454f-e067-c5c86d2bae9c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data Quality Report:\n",
            "----------------------------------------\n",
            "Duplicate rows: 0\n",
            "Students with <5 attempts: 473 (11.2%)\n",
            "Students with >500 attempts: 181\n",
            "All sequences properly ordered: True\n",
            "\n",
            "Original problems: 275458 (79.4%)\n",
            "Scaffolding problems: 71402 (20.6%)\n",
            "SAKT paper uses only original problems\n",
            "\n",
            "Answer distribution:\n",
            "Correct: 223818 (64.5%)\n",
            "Incorrect: 123042 (35.5%)\n",
            "\n",
            "Missing data analysis:\n",
            "Missing skill_id: 63755 rows (18.4%)\n",
            "Missing user_id: 0 rows\n",
            "Missing correct: 0 rows\n",
            "SAKT requires skill_id, user_id, and correct to be present\n"
          ]
        }
      ],
      "source": [
        "# CELL 6: Data Quality Check - Critical for Reliable Model Training\n",
        "\n",
        "print(\"Data Quality Report:\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# Check 1: Duplicate rows (same data appearing multiple times)\n",
        "duplicate_count = df.duplicated().sum()\n",
        "print(f\"Duplicate rows: {duplicate_count}\")\n",
        "if duplicate_count > 0:\n",
        "    print(\"Need to remove duplicates!\")\n",
        "\n",
        "# Check 2: Students with very few attempts\n",
        "# SAKT needs sufficient history to learn patterns\n",
        "too_few = (seq_lengths < 5).sum()\n",
        "print(f\"Students with <5 attempts: {too_few} ({too_few/len(seq_lengths)*100:.1f}%)\")\n",
        "\n",
        "# Check 3: Students with too many attempts (potential outliers)\n",
        "too_many = (seq_lengths > 500).sum()\n",
        "print(f\"Students with >500 attempts: {too_many}\")\n",
        "\n",
        "# Check 4: Temporal ordering validation\n",
        "# Sort by user and order_id to check sequence integrity\n",
        "df_sorted = df.sort_values(['user_id', 'order_id'])\n",
        "# For each user, check if order_id always increases\n",
        "is_ordered = df_sorted.groupby('user_id')['order_id'].apply(\n",
        "    lambda x: (x.diff().dropna() > 0).all()  # diff() calculates difference between consecutive values\n",
        ").all()\n",
        "print(f\"All sequences properly ordered: {is_ordered}\")\n",
        "\n",
        "# Check 5: Original vs scaffolding problems\n",
        "# Original = main problem, scaffolding = hints/sub-problems\n",
        "original_count = (df['original'] == 1).sum()\n",
        "scaffold_count = (df['original'] == 0).sum()\n",
        "print(f\"\\nOriginal problems: {original_count} ({original_count/len(df)*100:.1f}%)\")\n",
        "print(f\"Scaffolding problems: {scaffold_count} ({scaffold_count/len(df)*100:.1f}%)\")\n",
        "print(\"SAKT paper uses only original problems\")\n",
        "\n",
        "# Check 6: Answer distribution\n",
        "print(f\"\\nAnswer distribution:\")\n",
        "print(f\"Correct: {(df['correct'] == 1).sum()} ({df['correct'].mean()*100:.1f}%)\")\n",
        "print(f\"Incorrect: {(df['correct'] == 0).sum()} ({(1-df['correct'].mean())*100:.1f}%)\")\n",
        "\n",
        "# Check 7: Critical missing data for SAKT\n",
        "print(f\"\\nMissing data analysis:\")\n",
        "print(f\"Missing skill_id: {df['skill_id'].isna().sum()} rows ({df['skill_id'].isna().mean()*100:.1f}%)\")\n",
        "print(f\"Missing user_id: {df['user_id'].isna().sum()} rows\")\n",
        "print(f\"Missing correct: {df['correct'].isna().sum()} rows\")\n",
        "print(\"SAKT requires skill_id, user_id, and correct to be present\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OKW2EPOJ7Y3Y"
      },
      "source": [
        "# CELL 7: Data Preprocessing for SAKT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gLCecolb7fwX",
        "outputId": "594161b0-cebf-48fc-98f8-80366a7d4bad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting data preprocessing...\n",
            "Original data shape: (346860, 31)\n",
            "\n",
            "After keeping only original problems: (275458, 31)\n",
            "After removing missing skill_id: (259399, 31)\n",
            "After removing students with <5 attempts: (259399, 31)\n",
            "\n",
            "Final clean dataset:\n",
            "- Total interactions: 259399\n",
            "- Unique students: 4163\n",
            "- Unique skills: 145\n",
            "- Average correct rate: 65.80%\n"
          ]
        }
      ],
      "source": [
        "# CELL 7: Data Preprocessing for SAKT\n",
        "\n",
        "print(\"Starting data preprocessing...\")\n",
        "print(f\"Original data shape: {df.shape}\")\n",
        "\n",
        "# Step 1: Keep only original problems (main problems, not hints)\n",
        "# SAKT paper specifies using only original problems\n",
        "df_clean = df[df['original'] == 1].copy()\n",
        "print(f\"\\nAfter keeping only original problems: {df_clean.shape}\")\n",
        "\n",
        "# Step 2: Remove rows with missing skill_id\n",
        "# SAKT requires skill_id to create embeddings\n",
        "df_clean = df_clean.dropna(subset=['skill_id'])\n",
        "print(f\"After removing missing skill_id: {df_clean.shape}\")\n",
        "\n",
        "# Step 3: Convert skill_id to integer (it might be float due to NaN values)\n",
        "df_clean['skill_id'] = df_clean['skill_id'].astype(int)\n",
        "\n",
        "# Step 4: Calculate sequence lengths per student\n",
        "student_seq_lengths = df_clean.groupby('user_id').size()\n",
        "\n",
        "# Step 5: Keep only students with >= 0 attempts\n",
        "# Too few attempts don't provide enough learning history\n",
        "valid_students = student_seq_lengths[student_seq_lengths >= 0].index\n",
        "df_clean = df_clean[df_clean['user_id'].isin(valid_students)]\n",
        "print(f\"After removing students with <5 attempts: {df_clean.shape}\")\n",
        "\n",
        "# Step 6: Sort by user_id and order_id (temporal order)\n",
        "df_clean = df_clean.sort_values(['user_id', 'order_id'])\n",
        "\n",
        "print(f\"\\nFinal clean dataset:\")\n",
        "print(f\"- Total interactions: {len(df_clean)}\")\n",
        "print(f\"- Unique students: {df_clean['user_id'].nunique()}\")\n",
        "print(f\"- Unique skills: {df_clean['skill_id'].nunique()}\")\n",
        "print(f\"- Average correct rate: {df_clean['correct'].mean():.2%}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D9i4en2K7iJ2"
      },
      "source": [
        "# CELL 8: Transform Data into SAKT Input Format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "osY3ties7k2s",
        "outputId": "79db28d2-d106-4d37-f8e6-5a4057d5eba3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of unique skills: 145\n",
            "Skill IDs range: 1 to 173190193221\n",
            "\n",
            "Creating student sequences...\n",
            "\n",
            "Example sequence (first student):\n",
            "User ID: 14\n",
            "Sequence length: 19\n",
            "First 5 skills attempted: [132, 132, 132, 132, 132]\n",
            "First 5 responses (0=wrong, 1=correct): [0 1 0 0 0]\n",
            "First 5 interaction encodings: [np.int64(132), np.int64(277), np.int64(132), np.int64(132), np.int64(132)]\n"
          ]
        }
      ],
      "source": [
        "# CELL 8: Transform Data into SAKT Input Format\n",
        "\n",
        "# Get unique skills and create mapping\n",
        "unique_skills = sorted(df_clean['skill_id'].unique())\n",
        "num_skills = len(unique_skills)\n",
        "\n",
        "# Create skill_id to index mapping (0 to num_skills-1)\n",
        "skill_to_idx = {skill: idx for idx, skill in enumerate(unique_skills)}\n",
        "\n",
        "print(f\"Number of unique skills: {num_skills}\")\n",
        "print(f\"Skill IDs range: {min(unique_skills)} to {max(unique_skills)}\")\n",
        "\n",
        "# Function to create sequences for each student\n",
        "def create_student_sequences(df_clean, skill_to_idx, num_skills):\n",
        "    \"\"\"\n",
        "    Convert student interactions into SAKT format:\n",
        "    - interaction = skill_idx + (correct * num_skills)\n",
        "    \"\"\"\n",
        "    sequences = []\n",
        "\n",
        "    # Process each student\n",
        "    for user_id, user_data in df_clean.groupby('user_id'):\n",
        "        # Get student's attempt history\n",
        "        skills = user_data['skill_id'].values\n",
        "        corrects = user_data['correct'].values\n",
        "\n",
        "        # Convert skill_id to indices\n",
        "        skill_indices = [skill_to_idx[skill] for skill in skills]\n",
        "\n",
        "        # Create interaction sequence (SAKT encoding)\n",
        "        # interaction = skill_index + (correct * num_skills)\n",
        "        interactions = []\n",
        "        for skill_idx, correct in zip(skill_indices, corrects):\n",
        "            interaction = skill_idx + (correct * num_skills)\n",
        "            interactions.append(interaction)\n",
        "\n",
        "        sequences.append({\n",
        "            'user_id': user_id,\n",
        "            'skill_indices': skill_indices,\n",
        "            'corrects': corrects,\n",
        "            'interactions': interactions,\n",
        "            'length': len(interactions)\n",
        "        })\n",
        "\n",
        "    return sequences\n",
        "\n",
        "# Create sequences\n",
        "print(\"\\nCreating student sequences...\")\n",
        "sequences = create_student_sequences(df_clean, skill_to_idx, num_skills)\n",
        "\n",
        "# Show example sequence\n",
        "print(f\"\\nExample sequence (first student):\")\n",
        "example = sequences[0]\n",
        "print(f\"User ID: {example['user_id']}\")\n",
        "print(f\"Sequence length: {example['length']}\")\n",
        "print(f\"First 5 skills attempted: {example['skill_indices'][:5]}\")\n",
        "print(f\"First 5 responses (0=wrong, 1=correct): {example['corrects'][:5]}\")\n",
        "print(f\"First 5 interaction encodings: {example['interactions'][:5]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HPht-lxO7mZO"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZW62OMAt7oKR"
      },
      "source": [
        "# CELL 9: Split Data for Training (Student-Level Split)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eV3IUXSR7qwt",
        "outputId": "4e3eecc6-8f56-4d35-eb66-e96bb6464e84"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset splits (matching paper):\n",
            "- Train: 3330 students (80%)\n",
            "- Test: 833 students (20%)\n",
            "\n",
            "Total interactions per split:\n",
            "- Train: 206,654\n",
            "- Test: 52,745\n",
            "\n",
            "Data preprocessing complete! Saved to 'sakt_preprocessed_data.pkl'\n"
          ]
        }
      ],
      "source": [
        "# CELL 9: Split Data for Training (Student-Level Split)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split at student level (not interaction level) to prevent data leakage\n",
        "# Each student's full sequence goes into either train, val, or test\n",
        "\n",
        "# Use 80/20 split as in the paper (no separate validation)\n",
        "train_sequences, test_sequences = train_test_split(\n",
        "    sequences,\n",
        "    test_size=0.2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# For hyperparameter tuning, create a small validation set from training\n",
        "#train_sequences, val_sequences = train_test_split(\n",
        "#    train_sequences,\n",
        "#    test_size=0.1,  # 10% of training data for validation\n",
        "#    random_state=42\n",
        "#)\n",
        "\n",
        "print(f\"Dataset splits (matching paper):\")\n",
        "print(f\"- Train: {len(train_sequences)} students (80%)\")\n",
        "#print(f\"- Val: {len(val_sequences)} students\")\n",
        "print(f\"- Test: {len(test_sequences)} students (20%)\")\n",
        "\n",
        "# Calculate total interactions per split\n",
        "train_interactions = sum(seq['length'] for seq in train_sequences)\n",
        "#val_interactions = sum(seq['length'] for seq in val_sequences)\n",
        "test_interactions = sum(seq['length'] for seq in test_sequences)\n",
        "\n",
        "print(f\"\\nTotal interactions per split:\")\n",
        "print(f\"- Train: {train_interactions:,}\")\n",
        "#print(f\"- Val: {val_interactions:,}\")\n",
        "print(f\"- Test: {test_interactions:,}\")\n",
        "\n",
        "# Save the processed data\n",
        "import pickle\n",
        "\n",
        "save_data = {\n",
        "    'train': train_sequences,\n",
        "    'val': train_sequences, #change to val_sequences when you want to create a validation data. currently I am using trainin data as validation data for compatibility with the theSAKT paper\n",
        "    'test': test_sequences,\n",
        "    'num_skills': num_skills,\n",
        "    'skill_to_idx': skill_to_idx\n",
        "}\n",
        "\n",
        "with open('sakt_preprocessed_data.pkl', 'wb') as f:\n",
        "    pickle.dump(save_data, f)\n",
        "\n",
        "print(\"\\nData preprocessing complete! Saved to 'sakt_preprocessed_data.pkl'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JzMvcACxEkYx"
      },
      "source": [
        "# CELL 10: SAKT Model Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YYZUAsjHEmZQ",
        "outputId": "3a435f31-c77a-4ac1-8738-f9408d720f12"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing SAKT model...\n",
            "Model output shape: torch.Size([2, 50])\n",
            "Output range: [0.200, 0.761]\n",
            "Model parameters: 265,101\n"
          ]
        }
      ],
      "source": [
        "# CELL 10: SAKT Model Implementation\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F #importing the functional module from PyTorch's torch.nn package, and giving it a shorter alias: F.\n",
        "import math\n",
        "\n",
        "class SAKT(nn.Module): # Class SAKT model that inherits nn.module\n",
        "    def __init__(self, num_skills, embed_dim=100, num_heads=5, dropout=0.3):\n",
        "        \"\"\"\n",
        "        SAKT Model matching the paper implementation\n",
        "\n",
        "        Args:\n",
        "            num_skills: Number of unique skills (145 in this case)\n",
        "            embed_dim: Embedding dimension (paper uses 128)\n",
        "            num_heads: Number of attention heads (paper uses 5)\n",
        "            dropout: Dropout rate (paper uses 0.2)\n",
        "        \"\"\"\n",
        "        super(SAKT, self).__init__()\n",
        "\n",
        "        self.num_skills = num_skills\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "        # Embedding layers\n",
        "        # Interaction embedding: Maps (skill + correct*num_skills) to vectors\n",
        "        self.interaction_embed = nn.Embedding(\n",
        "            num_skills * 2 + 1,  # *2 because skill + correct*num_skills\n",
        "            embed_dim,\n",
        "            padding_idx=num_skills * 2\n",
        "        )\n",
        "\n",
        "        # Exercise/skill embedding: Maps skills to vectors for queries\n",
        "        self.skill_embed = nn.Embedding(\n",
        "            num_skills + 1,\n",
        "            embed_dim,\n",
        "            padding_idx=0\n",
        "            )\n",
        "\n",
        "        # Positional embedding: Adds temporal information\n",
        "        self.pos_embed = nn.Embedding(1000, embed_dim)  # max sequence length 1000\n",
        "\n",
        "        # Multi-head attention layer\n",
        "        self.attention = nn.MultiheadAttention(\n",
        "            embed_dim,\n",
        "            num_heads,\n",
        "            dropout=dropout,\n",
        "            batch_first=True  # Important: batch dimension first\n",
        "        )\n",
        "\n",
        "        # Layer normalization\n",
        "        self.layer_norm1 = nn.LayerNorm(embed_dim)\n",
        "        self.layer_norm2 = nn.LayerNorm(embed_dim)\n",
        "\n",
        "        # Feed-forward network\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(embed_dim, embed_dim * 4),  # Paper uses 4x hidden size\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(embed_dim * 4, embed_dim)\n",
        "        )\n",
        "\n",
        "        # Output prediction layer\n",
        "        self.pred = nn.Linear(embed_dim, 1)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, interactions, skills):\n",
        "        \"\"\"\n",
        "        Forward pass of SAKT\n",
        "\n",
        "        Args:\n",
        "            interactions: [batch_size, seq_len] - past interactions (skill + correct*num_skills)\n",
        "            skills: [batch_size, seq_len] - skills to predict performance on\n",
        "\n",
        "        Returns:\n",
        "            predictions: [batch_size, seq_len] - probability of correct answer\n",
        "        \"\"\"\n",
        "        batch_size, seq_len = interactions.shape\n",
        "\n",
        "        # Create position indices\n",
        "        positions = torch.arange(seq_len, device=interactions.device).unsqueeze(0).expand(batch_size, -1)\n",
        "\n",
        "        # Embed interactions (for Keys and Values in attention)\n",
        "        interaction_embeds = self.interaction_embed(interactions)  # [batch, seq_len, embed_dim]\n",
        "\n",
        "        # Embed skills (for Queries in attention)\n",
        "        skill_embeds = self.skill_embed(skills)  # [batch, seq_len, embed_dim]\n",
        "\n",
        "        # Add positional embeddings\n",
        "        interaction_embeds = interaction_embeds + self.pos_embed(positions)\n",
        "        skill_embeds = skill_embeds + self.pos_embed(positions)\n",
        "\n",
        "        # Create attention mask (causal mask - can't see future)\n",
        "        attn_mask = torch.triu(\n",
        "            torch.ones(seq_len, seq_len, device=interactions.device) * float('-inf'),\n",
        "            diagonal=1\n",
        "        )\n",
        "\n",
        "        # Apply self-attention\n",
        "        # Query: what skill we're predicting\n",
        "        # Key & Value: past interaction history\n",
        "        attended, _ = self.attention(\n",
        "            query=skill_embeds,\n",
        "            key=interaction_embeds,\n",
        "            value=interaction_embeds,\n",
        "            attn_mask=attn_mask,\n",
        "            need_weights=False\n",
        "        )\n",
        "\n",
        "        # Residual connection and layer norm\n",
        "        attended = self.layer_norm1(skill_embeds + self.dropout(attended))\n",
        "\n",
        "        # Feed-forward network with residual\n",
        "        ffn_out = self.ffn(attended)\n",
        "        ffn_out = self.layer_norm2(attended + self.dropout(ffn_out))\n",
        "\n",
        "        # Predict probability of correct answer\n",
        "        pred = self.pred(ffn_out).squeeze(-1)  # [batch, seq_len]\n",
        "        return torch.sigmoid(pred)\n",
        "\n",
        "# Test the model\n",
        "print(\"Testing SAKT model...\")\n",
        "model = SAKT(num_skills=145)\n",
        "\n",
        "# Create dummy batch\n",
        "batch_interactions = torch.randint(0, 290, (2, 50))  # 2 sequences, length 50\n",
        "batch_skills = torch.randint(0, 145, (2, 50))\n",
        "\n",
        "# Forward pass\n",
        "output = model(batch_interactions, batch_skills)\n",
        "print(f\"Model output shape: {output.shape}\")\n",
        "print(f\"Output range: [{output.min():.3f}, {output.max():.3f}]\")\n",
        "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aIb8Y30EtBq"
      },
      "source": [
        "# CELL 11: Create PyTorch Dataset and DataLoaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RnIpiDMGEvyu",
        "outputId": "7c836331-3cec-4a46-9398-59fea572d6d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch keys: dict_keys(['interactions', 'skills', 'targets', 'mask'])\n",
            "Interactions shape: torch.Size([128, 100])\n",
            "Skills shape: torch.Size([128, 100])\n",
            "Targets shape: torch.Size([128, 100])\n",
            "Mask shape: torch.Size([128, 100])\n"
          ]
        }
      ],
      "source": [
        "# CELL 11: Create PyTorch Dataset and DataLoaders\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "\n",
        "class SAKTDataset(Dataset):\n",
        "    \"\"\"Dataset class for SAKT - Corrected version\"\"\"\n",
        "\n",
        "    def __init__(self, sequences, max_seq_len=100, num_skills=145):\n",
        "        self.max_seq_len = max_seq_len\n",
        "        self.num_skills = num_skills\n",
        "        self.padding_interaction = num_skills * 2  # Padding token\n",
        "\n",
        "        # Process sequences: split long ones\n",
        "        self.data = []\n",
        "        for seq in sequences:\n",
        "            interactions = seq['interactions']\n",
        "            skills = seq['skill_indices']\n",
        "            corrects = seq['corrects']\n",
        "\n",
        "            # If sequence is longer than max_seq_len, split it\n",
        "            if len(interactions) > max_seq_len:\n",
        "                for i in range(0, len(interactions), max_seq_len):\n",
        "                    end_idx = min(i + max_seq_len, len(interactions))\n",
        "                    self.data.append({\n",
        "                        'interactions': interactions[i:end_idx],\n",
        "                        'skills': skills[i:end_idx],\n",
        "                        'corrects': corrects[i:end_idx]\n",
        "                    })\n",
        "            else:\n",
        "                self.data.append({\n",
        "                    'interactions': interactions,\n",
        "                    'skills': skills,\n",
        "                    'corrects': corrects\n",
        "                })\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        seq = self.data[idx]\n",
        "        seq_len = len(seq['interactions'])\n",
        "\n",
        "        interactions = list(seq['interactions'])\n",
        "        skills = list(seq['skills'])\n",
        "        corrects = list(seq['corrects'])\n",
        "\n",
        "        shifted_interactions = []\n",
        "        for i in range(seq_len):\n",
        "            if i == 0:\n",
        "                # First position: use a special START token (same as padding)\n",
        "                shifted_interactions.append(self.padding_interaction)\n",
        "            else:\n",
        "                # Use the PREVIOUS interaction\n",
        "                shifted_interactions.append(interactions[i-1])\n",
        "\n",
        "        # Use shifted_interactions instead of interactions\n",
        "        interactions = shifted_interactions\n",
        "\n",
        "        # Pad to the LEFT if sequence is shorter than max_seq_len\n",
        "        if seq_len < self.max_seq_len:\n",
        "            pad_len = self.max_seq_len - seq_len\n",
        "\n",
        "            # Pad to the LEFT\n",
        "            interactions = [self.padding_interaction] * pad_len + interactions\n",
        "            skills = [0] * pad_len + skills  # 0 for padding\n",
        "            corrects = [0] * pad_len + corrects\n",
        "\n",
        "            # Create mask (0 for padding, 1 for real data)\n",
        "            mask = [0] * pad_len + [1] * seq_len\n",
        "        else:\n",
        "            mask = [1] * self.max_seq_len\n",
        "\n",
        "        return {\n",
        "            'interactions': torch.tensor(interactions, dtype=torch.long),\n",
        "            'skills': torch.tensor(skills, dtype=torch.long),\n",
        "            'targets': torch.tensor(corrects, dtype=torch.float),\n",
        "            'mask': torch.tensor(mask, dtype=torch.float)\n",
        "        }\n",
        "\n",
        "# Load preprocessed data\n",
        "import pickle\n",
        "with open('sakt_preprocessed_data.pkl', 'rb') as f:\n",
        "    data = pickle.load(f)\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = SAKTDataset(data['train'], max_seq_len=100, num_skills=data['num_skills']) # paper uses max_seq_length = 100 and 50\n",
        "val_dataset = SAKTDataset(data['val'], max_seq_len=100, num_skills=data['num_skills'])     # paper uses max_seq_length = 100 and 50\n",
        "test_dataset = SAKTDataset(data['test'], max_seq_len=100, num_skills=data['num_skills'])   # paper uses max_seq_length = 100 and 50\n",
        "\n",
        "# Create dataloaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)  # paper uses batch_size = 128\n",
        "val_loader = DataLoader(val_dataset, batch_size=128, shuffle=False)     # paper uses batch_size = 128\n",
        "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)   # paper uses batch_size = 128\n",
        "\n",
        "# Test dataloader\n",
        "batch = next(iter(train_loader))\n",
        "print(f\"Batch keys: {batch.keys()}\")\n",
        "print(f\"Interactions shape: {batch['interactions'].shape}\")\n",
        "print(f\"Skills shape: {batch['skills'].shape}\")\n",
        "print(f\"Targets shape: {batch['targets'].shape}\")\n",
        "print(f\"Mask shape: {batch['mask'].shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G3iDyuwvE2GR"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZBUnkJ5uE4Ox"
      },
      "source": [
        "# CELL 12: Test Data Loaders and Verify Data Format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oho9RKT4E5QD",
        "outputId": "c1745d09-b5b9-4251-bd66-3b14daf1d577"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded data with 145 unique skills\n",
            "\n",
            "Dataset sizes:\n",
            "Train: 4534 students\n",
            "Val: 4534 students\n",
            "Test: 1134 students\n",
            "\n",
            "Testing data loader with one batch:\n",
            "Batch contains: ['interactions', 'skills', 'targets', 'mask']\n",
            "Interactions shape: torch.Size([64, 100])\n",
            "Skills shape: torch.Size([64, 100])\n",
            "Targets shape: torch.Size([64, 100])\n",
            "Mask shape: torch.Size([64, 100])\n",
            "\n",
            "Example from first student in batch:\n",
            "Actual sequence length: 87\n",
            "First 5 interactions: [290, 290, 290, 290, 290]\n",
            "First 5 skills to predict: [0, 0, 0, 0, 0]\n",
            "First 5 correct/incorrect: [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "First 5 mask values: [0.0, 0.0, 0.0, 0.0, 0.0]\n"
          ]
        }
      ],
      "source": [
        "# CELL 12: Test Data Loaders and Verify Data Format\n",
        "\n",
        "import pickle\n",
        "\n",
        "# Load the preprocessed data we saved in Cell 9\n",
        "# 'rb' means read in binary mode (pickle files are binary)\n",
        "with open('sakt_preprocessed_data.pkl', 'rb') as f:\n",
        "    data = pickle.load(f)  # Converts file back to Python dictionary\n",
        "\n",
        "print(f\"Loaded data with {data['num_skills']} unique skills\")\n",
        "\n",
        "# Create PyTorch datasets from the sequences\n",
        "# SAKTDataset handles padding and creating input/target pairs\n",
        "train_dataset = SAKTDataset(data['train'], max_seq_len=100)\n",
        "val_dataset = SAKTDataset(data['val'], max_seq_len=100)\n",
        "test_dataset = SAKTDataset(data['test'], max_seq_len=100)\n",
        "\n",
        "print(f\"\\nDataset sizes:\")\n",
        "print(f\"Train: {len(train_dataset)} students\")\n",
        "print(f\"Val: {len(val_dataset)} students\")\n",
        "print(f\"Test: {len(test_dataset)} students\")\n",
        "\n",
        "# Create data loaders that will feed batches to my model\n",
        "# batch_size=64 means process 64 students at once\n",
        "# shuffle=True randomizes order each epoch (important for training)\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "# Test by loading one batch to verify everything works\n",
        "print(f\"\\nTesting data loader with one batch:\")\n",
        "batch = next(iter(train_loader))  # Get first batch\n",
        "\n",
        "# Check what's in each batch\n",
        "print(f\"Batch contains: {list(batch.keys())}\")\n",
        "print(f\"Interactions shape: {batch['interactions'].shape}\")  # [64, 99]\n",
        "print(f\"Skills shape: {batch['skills'].shape}\")              # [64, 99]\n",
        "print(f\"Targets shape: {batch['targets'].shape}\")            # [64, 99]\n",
        "print(f\"Mask shape: {batch['mask'].shape}\")                  # [64, 99]\n",
        "\n",
        "# Look at one student's data to understand format\n",
        "print(f\"\\nExample from first student in batch:\")\n",
        "first_seq_len = batch['mask'][0].sum().int()  # Count non-padded positions\n",
        "print(f\"Actual sequence length: {first_seq_len}\")\n",
        "print(f\"First 5 interactions: {batch['interactions'][0][:5].tolist()}\")\n",
        "print(f\"First 5 skills to predict: {batch['skills'][0][:5].tolist()}\")\n",
        "print(f\"First 5 correct/incorrect: {batch['targets'][0][:5].tolist()}\")\n",
        "print(f\"First 5 mask values: {batch['mask'][0][:5].tolist()}\")  # 1=real, 0=padding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KSAwA5DuE7nl"
      },
      "source": [
        "# CELL 13: Training Functions for SAKT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "y6BM04A2E-iF"
      },
      "outputs": [],
      "source": [
        "# CELL 13: Training Functions for SAKT\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def train_epoch(model, train_loader, optimizer, criterion, device):\n",
        "    \"\"\"\n",
        "    Train the model for one epoch\n",
        "\n",
        "    Args:\n",
        "        model: SAKT model\n",
        "        train_loader: DataLoader with training data\n",
        "        optimizer: Adam optimizer\n",
        "        criterion: BCELoss function\n",
        "        device: cuda or cpu\n",
        "\n",
        "    Returns:\n",
        "        epoch_loss: Average loss for this epoch\n",
        "        epoch_auc: AUC score for this epoch\n",
        "    \"\"\"\n",
        "    model.train()  # Enable dropout and batch norm training behavior\n",
        "    total_loss = 0\n",
        "    all_predictions = []\n",
        "    all_targets = []\n",
        "\n",
        "    # Progress bar to track training\n",
        "    pbar = tqdm(train_loader, desc='Training', leave=False)\n",
        "\n",
        "    for batch in pbar:\n",
        "        # Move all tensors to GPU if available\n",
        "        interactions = batch['interactions'].to(device)\n",
        "        skills = batch['skills'].to(device)\n",
        "        targets = batch['targets'].to(device)\n",
        "        mask = batch['mask'].to(device)\n",
        "\n",
        "        # Clear gradients from previous batch\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass: get predictions from model\n",
        "        predictions = model(interactions, skills)\n",
        "\n",
        "        # Calculate loss only on non-padded positions\n",
        "        # criterion returns loss for each position\n",
        "        # Compute loss with masking\n",
        "        loss = criterion(predictions, targets)  # This gives loss per element\n",
        "        masked_loss = (loss * mask).sum() / mask.sum()  # Apply mask and average\n",
        "\n",
        "\n",
        "        # Backward pass: compute gradients\n",
        "        masked_loss.backward()\n",
        "\n",
        "        # Clip gradients to prevent explosion\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "        # Update model parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        # Collect predictions for metrics\n",
        "        total_loss += masked_loss.item()\n",
        "\n",
        "        # Extract only valid (non-padded) predictions\n",
        "        valid_idx = mask == 1\n",
        "        valid_predictions = predictions[valid_idx].detach().cpu().numpy()\n",
        "        valid_targets = targets[valid_idx].detach().cpu().numpy()\n",
        "\n",
        "        all_predictions.extend(valid_predictions)\n",
        "        all_targets.extend(valid_targets)\n",
        "\n",
        "        # Update progress bar with current loss\n",
        "        pbar.set_postfix({'loss': f'{masked_loss.item():.4f}'})\n",
        "\n",
        "    # Calculate epoch metrics\n",
        "    epoch_loss = total_loss / len(train_loader)\n",
        "    epoch_auc = roc_auc_score(all_targets, all_predictions)\n",
        "\n",
        "    return epoch_loss, epoch_auc\n",
        "\n",
        "def validate_epoch(model, val_loader, criterion, device):\n",
        "    \"\"\"\n",
        "    Validate the model (no gradient updates)\n",
        "\n",
        "    Args:\n",
        "        model: SAKT model\n",
        "        val_loader: DataLoader with validation data\n",
        "        criterion: BCELoss function\n",
        "        device: cuda or cpu\n",
        "\n",
        "    Returns:\n",
        "        val_loss: Average validation loss\n",
        "        val_auc: Validation AUC score\n",
        "    \"\"\"\n",
        "    model.eval()  # Disable dropout and use batch norm statistics\n",
        "    total_loss = 0\n",
        "    all_predictions = []\n",
        "    all_targets = []\n",
        "\n",
        "    # No gradient computation needed for validation\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(val_loader, desc='Validating', leave=False):\n",
        "            # Move to device\n",
        "            interactions = batch['interactions'].to(device)\n",
        "            skills = batch['skills'].to(device)\n",
        "            targets = batch['targets'].to(device)\n",
        "            mask = batch['mask'].to(device)\n",
        "\n",
        "            # Get predictions\n",
        "            predictions = model(interactions, skills)\n",
        "\n",
        "            # Calculate loss\n",
        "            # Compute loss with masking\n",
        "            loss = criterion(predictions, targets)  # This gives loss per element\n",
        "            masked_loss = (loss * mask).sum() / mask.sum()  # Apply mask and average\n",
        "\n",
        "            # Collect for metrics\n",
        "            total_loss += masked_loss.item()\n",
        "\n",
        "            # Extract valid predictions\n",
        "            valid_idx = mask == 1\n",
        "            valid_predictions = predictions[valid_idx].cpu().numpy()\n",
        "            valid_targets = targets[valid_idx].cpu().numpy()\n",
        "\n",
        "            all_predictions.extend(valid_predictions)\n",
        "            all_targets.extend(valid_targets)\n",
        "\n",
        "    # Calculate validation metrics\n",
        "    val_loss = total_loss / len(val_loader)\n",
        "    val_auc = roc_auc_score(all_targets, all_predictions)\n",
        "\n",
        "    return val_loss, val_auc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uvoiybrsFCFk"
      },
      "source": [
        "# CELL 14: Train SAKT Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jqPTIgbjFC37",
        "outputId": "24c5029c-76c1-46c8-8996-c57f6d81f89f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "Total parameters: 265,101\n",
            "Trainable parameters: 265,101\n",
            "\n",
            "Starting training for max 50 epochs...\n",
            "============================================================\n",
            "\n",
            "Epoch 1/50\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train - Loss: 0.6271, AUC: 0.6082\n",
            "\n",
            "Epoch 2/50\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train - Loss: 0.5891, AUC: 0.6903\n",
            "\n",
            "Epoch 3/50\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train - Loss: 0.5747, AUC: 0.7122\n",
            "\n",
            "Epoch 4/50\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train - Loss: 0.5680, AUC: 0.7212\n",
            "\n",
            "Epoch 5/50\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train - Loss: 0.5622, AUC: 0.7292\n",
            "\n",
            "Epoch 6/50\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train - Loss: 0.5600, AUC: 0.7331\n",
            "\n",
            "Epoch 7/50\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train - Loss: 0.5569, AUC: 0.7374\n",
            "\n",
            "Epoch 8/50\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train - Loss: 0.5537, AUC: 0.7413\n",
            "\n",
            "Epoch 9/50\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train - Loss: 0.5517, AUC: 0.7438\n",
            "\n",
            "Epoch 10/50\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train - Loss: 0.5502, AUC: 0.7459\n",
            "\n",
            "Epoch 11/50\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train - Loss: 0.5489, AUC: 0.7472\n",
            "\n",
            "Epoch 12/50\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train - Loss: 0.5474, AUC: 0.7489\n",
            "\n",
            "Epoch 13/50\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train - Loss: 0.5466, AUC: 0.7499\n",
            "\n",
            "Epoch 14/50\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train - Loss: 0.5461, AUC: 0.7511\n",
            "\n",
            "Epoch 15/50\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train - Loss: 0.5445, AUC: 0.7523\n",
            "\n",
            "Epoch 16/50\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train - Loss: 0.5435, AUC: 0.7538\n",
            "\n",
            "Epoch 17/50\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train - Loss: 0.5436, AUC: 0.7536\n",
            "\n",
            "Epoch 18/50\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train - Loss: 0.5422, AUC: 0.7553\n",
            "\n",
            "Epoch 19/50\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train - Loss: 0.5421, AUC: 0.7554\n",
            "\n",
            "Epoch 20/50\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train - Loss: 0.5404, AUC: 0.7568\n",
            "\n",
            "Epoch 21/50\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train - Loss: 0.5412, AUC: 0.7571\n",
            "\n",
            "Epoch 22/50\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train - Loss: 0.5404, AUC: 0.7574\n",
            "\n",
            "Epoch 23/50\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train - Loss: 0.5400, AUC: 0.7577\n",
            "\n",
            "Epoch 24/50\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train - Loss: 0.5400, AUC: 0.7578\n",
            "\n",
            "Epoch 25/50\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train - Loss: 0.5384, AUC: 0.7593\n",
            "\n",
            "Epoch 26/50\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train - Loss: 0.5388, AUC: 0.7589\n",
            "\n",
            "Epoch 27/50\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train - Loss: 0.5384, AUC: 0.7591\n",
            "\n",
            "Epoch 28/50\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train - Loss: 0.5384, AUC: 0.7592\n",
            "\n",
            "Epoch 29/50\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train - Loss: 0.5378, AUC: 0.7596\n",
            "\n",
            "Epoch 30/50\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train - Loss: 0.5375, AUC: 0.7603\n",
            "\n",
            "Epoch 31/50\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train - Loss: 0.5375, AUC: 0.7602\n",
            "\n",
            "Epoch 32/50\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train - Loss: 0.5368, AUC: 0.7613\n",
            "\n",
            "Epoch 33/50\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train - Loss: 0.5367, AUC: 0.7614\n",
            "\n",
            "Epoch 34/50\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train - Loss: 0.5362, AUC: 0.7615\n",
            "\n",
            "Epoch 35/50\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train - Loss: 0.5359, AUC: 0.7615\n",
            "\n",
            "Epoch 36/50\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train - Loss: 0.5357, AUC: 0.7622\n",
            "\n",
            "Epoch 37/50\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train - Loss: 0.5349, AUC: 0.7632\n",
            "\n",
            "Epoch 38/50\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train - Loss: 0.5361, AUC: 0.7621\n",
            "\n",
            "Epoch 39/50\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train - Loss: 0.5348, AUC: 0.7632\n",
            "\n",
            "Epoch 40/50\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train - Loss: 0.5350, AUC: 0.7634\n",
            "\n",
            "Epoch 41/50\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train - Loss: 0.5341, AUC: 0.7645\n",
            "\n",
            "Epoch 42/50\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train - Loss: 0.5338, AUC: 0.7635\n",
            "\n",
            "Epoch 43/50\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train - Loss: 0.5343, AUC: 0.7639\n",
            "\n",
            "Epoch 44/50\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train - Loss: 0.5330, AUC: 0.7652\n",
            "\n",
            "Epoch 45/50\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train - Loss: 0.5335, AUC: 0.7650\n",
            "\n",
            "Epoch 46/50\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train - Loss: 0.5340, AUC: 0.7644\n",
            "\n",
            "Epoch 47/50\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train - Loss: 0.5321, AUC: 0.7661\n",
            "\n",
            "Epoch 48/50\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train - Loss: 0.5322, AUC: 0.7659\n",
            "\n",
            "Epoch 49/50\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train - Loss: 0.5321, AUC: 0.7663\n",
            "\n",
            "Epoch 50/50\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                                                      "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train - Loss: 0.5312, AUC: 0.7667\n",
            "âœ“ Final model saved! (Train AUC: 0.7667)\n",
            "\n",
            "Training complete!\n",
            "Final training AUC: 0.7667\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r"
          ]
        }
      ],
      "source": [
        "# CELL 14: Train SAKT Model\n",
        "\n",
        "# Check if GPU is available and use it\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "if device.type == 'cuda':\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "\n",
        "# Initialize model with paper's hyperparameters\n",
        "model = SAKT(\n",
        "    num_skills=145,  # From my dataset analysis\n",
        "    embed_dim=100,   # Paper: d=[50, 100, 150, 200]\n",
        "    num_heads=5,     # Paper: h=5\n",
        "    dropout=0.3      # Paper: dropout=0.2 or 0.3\n",
        ").to(device)\n",
        "\n",
        "# Count model parameters\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"Total parameters: {total_params:,}\")\n",
        "print(f\"Trainable parameters: {trainable_params:,}\")\n",
        "\n",
        "# Initialize optimizer (Adam with paper's learning rate)\n",
        "optimizer = optim.Adam(\n",
        "    model.parameters(),\n",
        "    lr=0.001,           # Paper uses 0.001\n",
        "    weight_decay=0.0001  # L2 regularization\n",
        ")\n",
        "\n",
        "# Loss function for binary classification\n",
        "# reduction='none' returns loss per element (needed for masking)\n",
        "criterion = nn.BCELoss(reduction='none')\n",
        "\n",
        "\n",
        "# Training configuration\n",
        "NUM_EPOCHS = 50\n",
        "#best_val_auc = 0\n",
        "\n",
        "# Track metrics for visualization\n",
        "history = {\n",
        "    'train_loss': [], 'train_auc': [],\n",
        "#    'val_loss': [], 'val_auc': []\n",
        "}\n",
        "\n",
        "print(f\"\\nStarting training for max {NUM_EPOCHS} epochs...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Main training loop\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    print(f\"\\nEpoch {epoch+1}/{NUM_EPOCHS}\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    # Train for one epoch\n",
        "    train_loss, train_auc = train_epoch(\n",
        "        model, train_loader, optimizer, criterion, device\n",
        "    )\n",
        "\n",
        "    # Validate\n",
        "    #val_loss, val_auc = validate_epoch(\n",
        "    #    model, val_loader, criterion, device\n",
        "    #)\n",
        "\n",
        "    # Store history\n",
        "    history['train_loss'].append(train_loss)\n",
        "    history['train_auc'].append(train_auc)\n",
        "    #history['val_loss'].append(val_loss)\n",
        "    #history['val_auc'].append(val_auc)\n",
        "\n",
        "    # Print metrics\n",
        "    print(f\"Train - Loss: {train_loss:.4f}, AUC: {train_auc:.4f}\")\n",
        "    #print(f\"Valid - Loss: {val_loss:.4f}, AUC: {val_auc:.4f}\")\n",
        "\n",
        "    # Save best model\n",
        "    #if val_auc > best_val_auc:\n",
        "    #    best_val_auc = val_auc\n",
        "\n",
        "    if epoch + 1 == NUM_EPOCHS:\n",
        "        # Save checkpoint\n",
        "        checkpoint = {\n",
        "            'epoch': epoch + 1,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            #'best_val_auc': best_val_auc,\n",
        "            'train_auc': train_auc,\n",
        "            'history': history\n",
        "        }\n",
        "        torch.save(checkpoint, 'best_sakt_model.pth')\n",
        "        #print(f\"âœ“ New best model saved! (AUC: {best_val_auc:.4f})\")\n",
        "        print(f\"âœ“ Final model saved! (Train AUC: {train_auc:.4f})\")\n",
        "\n",
        "print(f\"\\nTraining complete!\")\n",
        "#print(f\"Best validation AUC: {best_val_auc:.4f}\")\n",
        "print(f\"Final training AUC: {train_auc:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vp4XSTwZFEzB"
      },
      "source": [
        "# CELL 15: Plot Training History"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 562
        },
        "id": "QR2gh3sLFHmN",
        "outputId": "3a83f01b-e27e-437d-ce51-d2e54d5bf914"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'val_loss'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-30-4252565987.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'b-'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Train Loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmarker\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'o'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmarkersize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r-'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Val Loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmarker\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m's'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmarkersize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Epoch'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'val_loss'"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# CELL 15: Plot Training History\n",
        "\n",
        "# Create figure with two subplots\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "# Subplot 1: Loss curves\n",
        "plt.subplot(1, 2, 1)\n",
        "epochs = range(1, len(history['train_loss']) + 1)\n",
        "plt.plot(epochs, history['train_loss'], 'b-', label='Train Loss', marker='o', markersize=4)\n",
        "plt.plot(epochs, history['val_loss'], 'r-', label='Val Loss', marker='s', markersize=4)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Subplot 2: AUC curves\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs, history['train_auc'], 'b-', label='Train AUC', marker='o', markersize=4)\n",
        "plt.plot(epochs, history['val_auc'], 'r-', label='Val AUC', marker='s', markersize=4)\n",
        "plt.axhline(y=0.82, color='g', linestyle='--', label='Target AUC (0.82)')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('AUC')\n",
        "plt.title('Training and Validation AUC')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print summary statistics\n",
        "print(\"Training Summary:\")\n",
        "print(\"-\" * 40)\n",
        "print(f\"Total epochs trained: {len(history['train_loss'])}\")\n",
        "print(f\"Best validation AUC: {max(history['val_auc']):.4f}\")\n",
        "print(f\"Final train AUC: {history['train_auc'][-1]:.4f}\")\n",
        "print(f\"Final validation AUC: {history['val_auc'][-1]:.4f}\")\n",
        "\n",
        "# Check for overfitting\n",
        "final_gap = history['train_auc'][-1] - history['val_auc'][-1]\n",
        "print(f\"\\nTrain-Val gap: {final_gap:.4f}\")\n",
        "if final_gap > 0.05:\n",
        "    print(\"Model might be overfitting\")\n",
        "else:\n",
        "    print(\"âœ“ No significant overfitting detected\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-qt2FatlFKDk"
      },
      "source": [
        "# CELL 16: Evaluate on Test Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fpUEBkydFMc5",
        "outputId": "9f26581f-650f-46fc-daab-7d34df0f3b53"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluating on test set...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test Set Results:\n",
            "Test Loss: 0.5582\n",
            "Test AUC: 0.7356\n",
            "\n",
            "Generalization check:\n",
            "Test AUC: 0.7356\n",
            "\n",
            "Additional Test Metrics:\n",
            "Accuracy: 0.7168\n",
            "Precision: 0.7333\n",
            "Recall: 0.8929\n"
          ]
        }
      ],
      "source": [
        "# CELL 16: Evaluate on Test Set\n",
        "\n",
        "# Load best model\n",
        "checkpoint = torch.load('best_sakt_model.pth', weights_only=False)\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "#print(f\"Loaded best model from epoch {checkpoint['epoch']} with val AUC {checkpoint['best_val_auc']:.4f}\")\n",
        "\n",
        "# Evaluate on test set\n",
        "print(\"\\nEvaluating on test set...\")\n",
        "test_loss, test_auc = validate_epoch(model, test_loader, criterion, device)\n",
        "\n",
        "print(f\"\\nTest Set Results:\")\n",
        "print(f\"Test Loss: {test_loss:.4f}\")\n",
        "print(f\"Test AUC: {test_auc:.4f}\")\n",
        "\n",
        "# Compare with validation performance\n",
        "print(f\"\\nGeneralization check:\")\n",
        "#print(f\"Validation AUC: {checkpoint['best_val_auc']:.4f}\")\n",
        "print(f\"Test AUC: {test_auc:.4f}\")\n",
        "#print(f\"Val-Test gap: {abs(checkpoint['best_val_auc'] - test_auc):.4f}\")\n",
        "\n",
        "# Additional metrics\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Get predictions for additional metrics\n",
        "model.eval()\n",
        "all_predictions = []\n",
        "all_targets = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        interactions = batch['interactions'].to(device)\n",
        "        skills = batch['skills'].to(device)\n",
        "        targets = batch['targets'].to(device)\n",
        "        mask = batch['mask'].to(device)\n",
        "\n",
        "        predictions = model(interactions, skills)\n",
        "\n",
        "        # Get actual batch size (last batch might be smaller)\n",
        "        current_batch_size = interactions.shape[0]\n",
        "\n",
        "        # Extract valid predictions for each sequence in batch\n",
        "        for i in range(current_batch_size):\n",
        "            mask_i = mask[i].cpu().numpy()\n",
        "            pred_i = predictions[i].cpu().numpy()\n",
        "            target_i = targets[i].cpu().numpy()\n",
        "\n",
        "            # Only add non-padded values\n",
        "            valid_idx = mask_i == 1\n",
        "            all_predictions.extend(pred_i[valid_idx].tolist())\n",
        "            all_targets.extend(target_i[valid_idx].tolist())\n",
        "\n",
        "# Convert to numpy arrays\n",
        "all_predictions = np.array(all_predictions)\n",
        "all_targets = np.array(all_targets)\n",
        "\n",
        "# Calculate additional metrics using 0.5 threshold\n",
        "binary_predictions = (all_predictions > 0.5).astype(int)\n",
        "\n",
        "accuracy = accuracy_score(all_targets, binary_predictions)\n",
        "precision = precision_score(all_targets, binary_predictions)\n",
        "recall = recall_score(all_targets, binary_predictions)\n",
        "#f1 = f1_score(all_targets, binary_predictions)\n",
        "\n",
        "print(f\"\\nAdditional Test Metrics:\")\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "#print(f\"F1-Score: {f1:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CohmA6OQFQKe"
      },
      "source": [
        "# CELL 17: Save Final Model and Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "K2aDuaUnFSN6",
        "outputId": "aa267ae0-722c-4739-ce3e-c18bc5b5b01b"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'best_val_auc'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-32-69975784.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m     },\n\u001b[1;32m     20\u001b[0m     'results': {\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0;34m'best_val_auc'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'best_val_auc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0;34m'test_auc'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_auc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;34m'test_accuracy'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'best_val_auc'"
          ]
        }
      ],
      "source": [
        "# CELL 17: Save Final Model and Results\n",
        "\n",
        "import json\n",
        "from datetime import datetime\n",
        "\n",
        "# Create results summary\n",
        "results = {\n",
        "    'model_name': 'SAKT',\n",
        "    'dataset': 'ASSISTments2009',\n",
        "    'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
        "    'hyperparameters': {\n",
        "        'num_skills': 145,\n",
        "        'embed_dim': 128,\n",
        "        'num_heads': 5,\n",
        "        'dropout': 0.2,\n",
        "        'learning_rate': 0.001,\n",
        "        'batch_size': 64,\n",
        "        'max_seq_len': 100\n",
        "    },\n",
        "    'results': {\n",
        "        'best_val_auc': float(checkpoint['best_val_auc']),\n",
        "        'test_auc': float(test_auc),\n",
        "        'test_accuracy': float(accuracy),\n",
        "        'test_precision': float(precision),\n",
        "        'test_recall': float(recall),\n",
        "        'test_f1': float(f1),\n",
        "        'total_epochs': len(history['train_loss']),\n",
        "        'best_epoch': checkpoint['epoch']\n",
        "    },\n",
        "    'data_stats': {\n",
        "        'train_students': len(data['train']),\n",
        "        'val_students': len(data['val']),\n",
        "        'test_students': len(data['test']),\n",
        "        'unique_skills': data['num_skills']\n",
        "    }\n",
        "}\n",
        "\n",
        "# Save results to JSON\n",
        "with open('sakt_results.json', 'w') as f:\n",
        "    json.dump(results, f, indent=4)\n",
        "\n",
        "# Save final model with all information\n",
        "final_save = {\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'optimizer_state_dict': optimizer.state_dict(),\n",
        "    'results': results,\n",
        "    'history': history,\n",
        "    'skill_to_idx': data['skill_to_idx']\n",
        "}\n",
        "\n",
        "torch.save(final_save, 'sakt_final_model.pth')\n",
        "\n",
        "print(\"Model and results saved!\")\n",
        "print(f\"\\nSummary for thesis:\")\n",
        "print(f\"- SAKT achieved {test_auc:.4f} AUC on ASSISTments2009\")\n",
        "print(f\"- Model has {sum(p.numel() for p in model.parameters()):,} parameters\")\n",
        "print(f\"- Training took {len(history['train_loss'])} epochs\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IsIdj8XHdL5B"
      },
      "source": [
        "# Created this cell to perform a Hyperparameter search for the improving the AUC of the SAKT paper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YckB_GGPW2L7"
      },
      "outputs": [],
      "source": [
        "# CELL: Hyperparameter Search for SAKT\n",
        "\n",
        "import itertools\n",
        "from datetime import datetime\n",
        "import json\n",
        "\n",
        "# Define hyperparameter search space\n",
        "param_grid = {\n",
        "    'embed_dim': [50, 100, 150, 200],  # Paper tested these\n",
        "    'num_heads': [5],  # Keep at 5 as per paper\n",
        "    'dropout': [0.1, 0.2, 0.3],\n",
        "    'lr': [5e-4, 1e-3, 2e-3],\n",
        "    'weight_decay': [0, 1e-5, 1e-4],\n",
        "    'batch_size': [64, 128],  # Paper mentions 128 for ASSIST2009\n",
        "}\n",
        "\n",
        "# Generate all combinations\n",
        "param_combinations = list(itertools.product(*param_grid.values()))\n",
        "param_names = list(param_grid.keys())\n",
        "\n",
        "print(f\"Total hyperparameter combinations to test: {len(param_combinations)}\")\n",
        "\n",
        "# Function to train with specific hyperparameters\n",
        "def train_with_params(params_dict, max_epochs=20, patience=5):\n",
        "    \"\"\"\n",
        "    Train SAKT with given hyperparameters and return best validation AUC\n",
        "    \"\"\"\n",
        "    # Create model with specified parameters\n",
        "    model = SAKT(\n",
        "        num_skills=145,\n",
        "        embed_dim=params_dict['embed_dim'],\n",
        "        num_heads=params_dict['num_heads'],\n",
        "        dropout=params_dict['dropout']\n",
        "    ).to(device)\n",
        "\n",
        "    # Create optimizer with specified learning rate\n",
        "    optimizer = optim.Adam(\n",
        "        model.parameters(),\n",
        "        lr=params_dict['lr'],\n",
        "        weight_decay=params_dict['weight_decay']\n",
        "    )\n",
        "\n",
        "    # Recreate data loaders with specified batch size\n",
        "    train_loader_hp = DataLoader(train_dataset, batch_size=params_dict['batch_size'], shuffle=True)\n",
        "    val_loader_hp = DataLoader(val_dataset, batch_size=params_dict['batch_size'], shuffle=False)\n",
        "\n",
        "    # Loss function\n",
        "    criterion = nn.BCELoss(reduction='none')\n",
        "\n",
        "    # Training loop with early stopping\n",
        "    best_val_auc = 0\n",
        "    patience_counter = 0\n",
        "\n",
        "    for epoch in range(max_epochs):\n",
        "        # Train epoch\n",
        "        train_loss, train_auc = train_epoch(model, train_loader_hp, optimizer, criterion, device)\n",
        "\n",
        "        # Validate\n",
        "        val_loss, val_auc = validate_epoch(model, val_loader_hp, criterion, device)\n",
        "\n",
        "        # Check if improved\n",
        "        if val_auc > best_val_auc:\n",
        "            best_val_auc = val_auc\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "\n",
        "        # Early stopping\n",
        "        if patience_counter >= patience:\n",
        "            break\n",
        "\n",
        "    return best_val_auc, epoch + 1\n",
        "\n",
        "# Run hyperparameter search\n",
        "results = []\n",
        "best_auc = 0\n",
        "best_params = None\n",
        "\n",
        "print(\"\\nStarting hyperparameter search...\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Test a subset first (remove this limit for full search)\n",
        "# For full search, remove the [:10] to test all combinations\n",
        "for i, param_values in enumerate(param_combinations):  # Testing all combinations\n",
        "    params_dict = dict(zip(param_names, param_values))\n",
        "\n",
        "    print(f\"\\nTesting combination {i+1}/{len(param_combinations)}\")\n",
        "    print(f\"Parameters: {params_dict}\")\n",
        "\n",
        "    try:\n",
        "        # Train with these parameters\n",
        "        val_auc, epochs_trained = train_with_params(params_dict, max_epochs=15, patience=3)\n",
        "\n",
        "        # Store results\n",
        "        result = {\n",
        "            'params': params_dict,\n",
        "            'val_auc': val_auc,\n",
        "            'epochs': epochs_trained\n",
        "        }\n",
        "        results.append(result)\n",
        "\n",
        "        print(f\"Validation AUC: {val_auc:.4f} (trained for {epochs_trained} epochs)\")\n",
        "\n",
        "        # Track best\n",
        "        if val_auc > best_auc:\n",
        "            best_auc = val_auc\n",
        "            best_params = params_dict\n",
        "            print(f\"âœ“ New best AUC!\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error with parameters {params_dict}: {str(e)}\")\n",
        "        continue\n",
        "\n",
        "# Sort results by AUC\n",
        "results.sort(key=lambda x: x['val_auc'], reverse=True)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"HYPERPARAMETER SEARCH RESULTS\")\n",
        "print(\"=\"*80)\n",
        "print(f\"\\nBest parameters found:\")\n",
        "print(f\"AUC: {best_auc:.4f}\")\n",
        "for param, value in best_params.items():\n",
        "    print(f\"{param}: {value}\")\n",
        "\n",
        "print(\"\\nTop 5 configurations:\")\n",
        "for i, result in enumerate(results[:5]):\n",
        "    print(f\"\\n{i+1}. AUC: {result['val_auc']:.4f}\")\n",
        "    for param, value in result['params'].items():\n",
        "        print(f\"   {param}: {value}\")\n",
        "\n",
        "# Save results\n",
        "with open('hyperparameter_search_results.json', 'w') as f:\n",
        "    json.dump(results, f, indent=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tG8IQBhjCUfS",
        "outputId": "96bd1e59-3b2c-459c-8da8-32f13e788368"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Available columns in ASSIST2009:\n",
            "['Unnamed: 0', 'order_id', 'assignment_id', 'user_id', 'assistment_id', 'problem_id', 'original', 'correct', 'attempt_count', 'ms_first_response', 'tutor_mode', 'answer_type', 'sequence_id', 'student_class_id', 'position', 'type', 'base_sequence_id', 'skill_id', 'skill_name', 'teacher_id', 'school_id', 'hint_count', 'hint_total', 'overlap_time', 'template_id', 'answer_id', 'answer_text', 'first_action', 'bottom_hint', 'opportunity', 'opportunity_original']\n",
            "\n",
            "Available enhancement features: ['ms_first_response', 'hint_count', 'attempt_count', 'overlap_time']\n",
            "ms_first_response: 0.0% missing\n",
            "hint_count: 0.0% missing\n",
            "attempt_count: 0.0% missing\n",
            "overlap_time: 0.0% missing\n"
          ]
        }
      ],
      "source": [
        "# CELL 17: Examine Available Features for Enhancement\n",
        "\n",
        "# Load your data and check columns\n",
        "print(\"Available columns in ASSIST2009:\")\n",
        "print(df.columns.tolist())\n",
        "\n",
        "# Check for key features we want to use\n",
        "key_features = ['ms_first_response', 'hint_count', 'attempt_count', 'overlap_time']\n",
        "available_features = [f for f in key_features if f in df.columns]\n",
        "print(f\"\\nAvailable enhancement features: {available_features}\")\n",
        "\n",
        "# Check data completeness for these features\n",
        "for feature in available_features:\n",
        "    if feature in df.columns:\n",
        "        missing_pct = df[feature].isna().sum() / len(df) * 100\n",
        "        print(f\"{feature}: {missing_pct:.1f}% missing\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NdnNGQqUCbWj",
        "outputId": "8211cc91-aa67-4831-f8f9-28147346f8d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating enhanced sequences...\n",
            "Enhanced sequences created:\n",
            "- Train: 3330 students\n",
            "- Test: 833 students\n"
          ]
        }
      ],
      "source": [
        "# CELL 18: Enhanced Data Preprocessing for DA-SAKT\n",
        "\n",
        "def create_enhanced_sequences(df_clean, skill_to_idx, num_skills):\n",
        "    \"\"\"Create sequences with additional features for enhanced SAKT\"\"\"\n",
        "    sequences = []\n",
        "\n",
        "    for user_id, user_data in df_clean.groupby('user_id'):\n",
        "        # Basic features\n",
        "        skills = user_data['skill_id'].values\n",
        "        corrects = user_data['correct'].values\n",
        "        skill_indices = [skill_to_idx[skill] for skill in skills]\n",
        "\n",
        "        # Enhanced features\n",
        "        # Response time (normalize to 0-1 range)\n",
        "        if 'ms_first_response' in user_data.columns:\n",
        "            response_times = user_data['ms_first_response'].fillna(user_data['ms_first_response'].median()).values\n",
        "            # Clip extreme values and normalize\n",
        "            response_times = np.clip(response_times, 0, 300000)  # Max 5 minutes\n",
        "            response_times = response_times / 300000.0\n",
        "        else:\n",
        "            response_times = np.ones(len(skills)) * 0.5  # Default middle difficulty\n",
        "\n",
        "        # Hint usage (normalize by max hints)\n",
        "        if 'hint_count' in user_data.columns:\n",
        "            hint_counts = user_data['hint_count'].fillna(0).values\n",
        "            hint_counts = np.clip(hint_counts, 0, 5) / 5.0\n",
        "        else:\n",
        "            hint_counts = np.zeros(len(skills))\n",
        "\n",
        "        # Attempt count\n",
        "        if 'attempt_count' in user_data.columns:\n",
        "            attempt_counts = user_data['attempt_count'].fillna(1).values\n",
        "            attempt_counts = np.clip(attempt_counts, 1, 5) / 5.0\n",
        "        else:\n",
        "            attempt_counts = np.ones(len(skills)) * 0.2\n",
        "\n",
        "        # Create interactions\n",
        "        interactions = []\n",
        "        for skill_idx, correct in zip(skill_indices, corrects):\n",
        "            interaction = skill_idx + (correct * num_skills)\n",
        "            interactions.append(interaction)\n",
        "\n",
        "        sequences.append({\n",
        "            'user_id': user_id,\n",
        "            'skill_indices': skill_indices,\n",
        "            'corrects': corrects,\n",
        "            'interactions': interactions,\n",
        "            'response_times': response_times,\n",
        "            'hint_counts': hint_counts,\n",
        "            'attempt_counts': attempt_counts,\n",
        "            'length': len(interactions)\n",
        "        })\n",
        "\n",
        "    return sequences\n",
        "\n",
        "# Create enhanced sequences\n",
        "print(\"Creating enhanced sequences...\")\n",
        "enhanced_sequences = create_enhanced_sequences(df_clean, skill_to_idx, num_skills)\n",
        "\n",
        "# Split data (80/20)\n",
        "from sklearn.model_selection import train_test_split\n",
        "train_sequences_enh, test_sequences_enh = train_test_split(\n",
        "    enhanced_sequences,\n",
        "    test_size=0.2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "print(f\"Enhanced sequences created:\")\n",
        "print(f\"- Train: {len(train_sequences_enh)} students\")\n",
        "print(f\"- Test: {len(test_sequences_enh)} students\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "14ir5gkkCjyb",
        "outputId": "cd6dba49-4800-4d18-efa8-097362f53ca0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing Difficulty-Aware SAKT...\n",
            "Enhanced model parameters: 298,002\n"
          ]
        }
      ],
      "source": [
        "# CELL 19: Difficulty-Aware SAKT Model\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class DifficultyAwareSAKT(nn.Module):\n",
        "    def __init__(self, num_skills, embed_dim=100, num_heads=5, dropout=0.2, num_features=3):\n",
        "        \"\"\"\n",
        "        Enhanced SAKT with difficulty awareness and multi-feature integration\n",
        "\n",
        "        Args:\n",
        "            num_features: Number of additional features (response_time, hints, attempts)\n",
        "        \"\"\"\n",
        "        super(DifficultyAwareSAKT, self).__init__()\n",
        "\n",
        "        self.num_skills = num_skills\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "        # Original SAKT embeddings\n",
        "        self.interaction_embed = nn.Embedding(\n",
        "            num_skills * 2 + 1,\n",
        "            embed_dim,\n",
        "            padding_idx=num_skills * 2\n",
        "        )\n",
        "        self.skill_embed = nn.Embedding(\n",
        "            num_skills + 1,\n",
        "            embed_dim,\n",
        "            padding_idx=0\n",
        "        )\n",
        "        self.pos_embed = nn.Embedding(1000, embed_dim)\n",
        "\n",
        "        # NEW: Feature embeddings\n",
        "        self.feature_proj = nn.Linear(num_features, embed_dim // 4)\n",
        "\n",
        "        # NEW: Difficulty-aware projection\n",
        "        self.difficulty_proj = nn.Linear(embed_dim + embed_dim // 4, embed_dim)\n",
        "\n",
        "        # Enhanced attention with difficulty awareness\n",
        "        self.attention = nn.MultiheadAttention(\n",
        "            embed_dim,\n",
        "            num_heads,\n",
        "            dropout=dropout,\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        # NEW: Sparse attention gate (learns which interactions to focus on)\n",
        "        self.attention_gate = nn.Sequential(\n",
        "            nn.Linear(embed_dim * 2, embed_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(embed_dim, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "        # Layer normalization\n",
        "        self.layer_norm1 = nn.LayerNorm(embed_dim)\n",
        "        self.layer_norm2 = nn.LayerNorm(embed_dim)\n",
        "\n",
        "        # Feed-forward network\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(embed_dim, embed_dim * 4),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(embed_dim * 4, embed_dim)\n",
        "        )\n",
        "\n",
        "        # Output prediction with difficulty awareness\n",
        "        self.pred = nn.Linear(embed_dim, 1)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, interactions, skills, features):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            interactions: [batch_size, seq_len]\n",
        "            skills: [batch_size, seq_len]\n",
        "            features: [batch_size, seq_len, num_features] - response_time, hints, attempts\n",
        "        \"\"\"\n",
        "        batch_size, seq_len = interactions.shape\n",
        "\n",
        "        # Position indices\n",
        "        positions = torch.arange(seq_len, device=interactions.device).unsqueeze(0).expand(batch_size, -1)\n",
        "\n",
        "        # Embed interactions and skills\n",
        "        interaction_embeds = self.interaction_embed(interactions)\n",
        "        skill_embeds = self.skill_embed(skills)\n",
        "\n",
        "        # NEW: Project features and combine with embeddings\n",
        "        feature_embeds = self.feature_proj(features)  # [batch, seq_len, embed_dim//4]\n",
        "\n",
        "        # Enhance interaction embeddings with features (difficulty-aware)\n",
        "        enhanced_interactions = torch.cat([interaction_embeds, feature_embeds], dim=-1)\n",
        "        enhanced_interactions = self.difficulty_proj(enhanced_interactions)\n",
        "\n",
        "        # Add positional embeddings\n",
        "        enhanced_interactions = enhanced_interactions + self.pos_embed(positions)\n",
        "        skill_embeds = skill_embeds + self.pos_embed(positions)\n",
        "\n",
        "        # Causal mask\n",
        "        attn_mask = torch.triu(\n",
        "            torch.ones(seq_len, seq_len, device=interactions.device) * float('-inf'),\n",
        "            diagonal=1\n",
        "        )\n",
        "\n",
        "        # Apply attention\n",
        "        attended, attn_weights = self.attention(\n",
        "            query=skill_embeds,\n",
        "            key=enhanced_interactions,\n",
        "            value=enhanced_interactions,\n",
        "            attn_mask=attn_mask,\n",
        "            need_weights=True\n",
        "        )\n",
        "\n",
        "        # NEW: Apply sparse attention gate\n",
        "        gate_input = torch.cat([skill_embeds, attended], dim=-1)\n",
        "        gate_weights = self.attention_gate(gate_input)\n",
        "        attended = attended * gate_weights\n",
        "\n",
        "        # Residual connection and layer norm\n",
        "        attended = self.layer_norm1(skill_embeds + self.dropout(attended))\n",
        "\n",
        "        # Feed-forward network\n",
        "        ffn_out = self.ffn(attended)\n",
        "        ffn_out = self.layer_norm2(attended + self.dropout(ffn_out))\n",
        "\n",
        "        # Predict\n",
        "        pred = self.pred(ffn_out).squeeze(-1)\n",
        "        return torch.sigmoid(pred)\n",
        "\n",
        "# Test the enhanced model\n",
        "print(\"Testing Difficulty-Aware SAKT...\")\n",
        "model_enhanced = DifficultyAwareSAKT(num_skills=145)\n",
        "print(f\"Enhanced model parameters: {sum(p.numel() for p in model_enhanced.parameters()):,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t96ORb09Cq2f",
        "outputId": "b92c426c-614a-4a83-cd80-17a8c29c5f81"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enhanced datasets created:\n",
            "Train batches: 71\n",
            "Test batches: 18\n"
          ]
        }
      ],
      "source": [
        "# CELL 20: Enhanced Dataset for DA-SAKT\n",
        "\n",
        "class EnhancedSAKTDataset(Dataset):\n",
        "    def __init__(self, sequences, max_seq_len=100, num_skills=145):\n",
        "        self.max_seq_len = max_seq_len\n",
        "        self.num_skills = num_skills\n",
        "        self.padding_interaction = num_skills * 2\n",
        "\n",
        "        # Process sequences\n",
        "        self.data = []\n",
        "        for seq in sequences:\n",
        "            if len(seq['interactions']) > max_seq_len:\n",
        "                # Split long sequences\n",
        "                for i in range(0, len(seq['interactions']), max_seq_len):\n",
        "                    end_idx = min(i + max_seq_len, len(seq['interactions']))\n",
        "                    self.data.append({\n",
        "                        'interactions': seq['interactions'][i:end_idx],\n",
        "                        'skills': seq['skill_indices'][i:end_idx],\n",
        "                        'corrects': seq['corrects'][i:end_idx],\n",
        "                        'response_times': seq['response_times'][i:end_idx],\n",
        "                        'hint_counts': seq['hint_counts'][i:end_idx],\n",
        "                        'attempt_counts': seq['attempt_counts'][i:end_idx]\n",
        "                    })\n",
        "            else:\n",
        "                self.data.append({\n",
        "                    'interactions': seq['interactions'],\n",
        "                    'skills': seq['skill_indices'],\n",
        "                    'corrects': seq['corrects'],\n",
        "                    'response_times': seq['response_times'],\n",
        "                    'hint_counts': seq['hint_counts'],\n",
        "                    'attempt_counts': seq['attempt_counts']\n",
        "                })\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        seq = self.data[idx]\n",
        "        seq_len = len(seq['interactions'])\n",
        "\n",
        "        # Convert to lists and apply shifting\n",
        "        interactions = list(seq['interactions'])\n",
        "        skills = list(seq['skills'])\n",
        "        corrects = list(seq['corrects'])\n",
        "        response_times = list(seq['response_times'])\n",
        "        hint_counts = list(seq['hint_counts'])\n",
        "        attempt_counts = list(seq['attempt_counts'])\n",
        "\n",
        "        # Shift interactions\n",
        "        shifted_interactions = []\n",
        "        shifted_features = []\n",
        "        for i in range(seq_len):\n",
        "            if i == 0:\n",
        "                shifted_interactions.append(self.padding_interaction)\n",
        "                shifted_features.append([0.5, 0.0, 0.2])  # Default features\n",
        "            else:\n",
        "                shifted_interactions.append(interactions[i-1])\n",
        "                shifted_features.append([\n",
        "                    response_times[i-1],\n",
        "                    hint_counts[i-1],\n",
        "                    attempt_counts[i-1]\n",
        "                ])\n",
        "\n",
        "        interactions = shifted_interactions\n",
        "        features = shifted_features\n",
        "\n",
        "        # Pad to the LEFT\n",
        "        if seq_len < self.max_seq_len:\n",
        "            pad_len = self.max_seq_len - seq_len\n",
        "\n",
        "            interactions = [self.padding_interaction] * pad_len + interactions\n",
        "            skills = [0] * pad_len + skills\n",
        "            corrects = [0] * pad_len + corrects\n",
        "            features = [[0.5, 0.0, 0.2]] * pad_len + features\n",
        "            mask = [0] * pad_len + [1] * seq_len\n",
        "        else:\n",
        "            mask = [1] * self.max_seq_len\n",
        "\n",
        "        return {\n",
        "            'interactions': torch.tensor(interactions, dtype=torch.long),\n",
        "            'skills': torch.tensor(skills, dtype=torch.long),\n",
        "            'targets': torch.tensor(corrects, dtype=torch.float),\n",
        "            'features': torch.tensor(features, dtype=torch.float),\n",
        "            'mask': torch.tensor(mask, dtype=torch.float)\n",
        "        }\n",
        "\n",
        "# Create enhanced datasets\n",
        "train_dataset_enh = EnhancedSAKTDataset(train_sequences_enh, max_seq_len=100, num_skills=num_skills)\n",
        "test_dataset_enh = EnhancedSAKTDataset(test_sequences_enh, max_seq_len=100, num_skills=num_skills)\n",
        "\n",
        "# Create dataloaders\n",
        "train_loader_enh = DataLoader(train_dataset_enh, batch_size=64, shuffle=True)\n",
        "test_loader_enh = DataLoader(test_dataset_enh, batch_size=64, shuffle=False)\n",
        "\n",
        "print(f\"Enhanced datasets created:\")\n",
        "print(f\"Train batches: {len(train_loader_enh)}\")\n",
        "print(f\"Test batches: {len(test_loader_enh)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v1UKWO09CvuQ",
        "outputId": "433a003d-fe71-470a-e49a-4cb6f7bfc7de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Difficulty-Aware SAKT...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10: Train Loss=0.5408, Train AUC=0.7558\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 20: Train Loss=0.5301, Train AUC=0.7681\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 30: Train Loss=0.5248, Train AUC=0.7734\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 40: Train Loss=0.5210, Train AUC=0.7773\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 50: Train Loss=0.5172, Train AUC=0.7811\n",
            "\n",
            "============================================================\n",
            "MODEL COMPARISON\n",
            "============================================================\n",
            "\n",
            "Baseline SAKT:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test AUC: 0.7356\n",
            "\n",
            "Difficulty-Aware SAKT:\n",
            "Test AUC: 0.7373\n",
            "\n",
            "Improvement: +0.24%\n",
            "Absolute gain: +0.0018\n"
          ]
        }
      ],
      "source": [
        "# CELL 21: Train and Compare Models\n",
        "\n",
        "# Modified training functions for enhanced model\n",
        "def train_epoch_enhanced(model, train_loader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    all_predictions = []\n",
        "    all_targets = []\n",
        "\n",
        "    for batch in tqdm(train_loader, desc='Training', leave=False):\n",
        "        interactions = batch['interactions'].to(device)\n",
        "        skills = batch['skills'].to(device)\n",
        "        targets = batch['targets'].to(device)\n",
        "        features = batch['features'].to(device)\n",
        "        mask = batch['mask'].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass with features\n",
        "        predictions = model(interactions, skills, features)\n",
        "\n",
        "        # Masked loss\n",
        "        loss = criterion(predictions, targets)\n",
        "        masked_loss = (loss * mask).sum() / mask.sum()\n",
        "\n",
        "        masked_loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += masked_loss.item()\n",
        "\n",
        "        # Collect predictions\n",
        "        valid_idx = mask == 1\n",
        "        valid_predictions = predictions[valid_idx].detach().cpu().numpy()\n",
        "        valid_targets = targets[valid_idx].detach().cpu().numpy()\n",
        "\n",
        "        all_predictions.extend(valid_predictions)\n",
        "        all_targets.extend(valid_targets)\n",
        "\n",
        "    epoch_loss = total_loss / len(train_loader)\n",
        "    epoch_auc = roc_auc_score(all_targets, all_predictions)\n",
        "\n",
        "    return epoch_loss, epoch_auc\n",
        "\n",
        "# Train enhanced model\n",
        "print(\"Training Difficulty-Aware SAKT...\")\n",
        "model_enhanced = DifficultyAwareSAKT(\n",
        "    num_skills=145,\n",
        "    embed_dim=100,\n",
        "    num_heads=5,\n",
        "    dropout=0.2,\n",
        "    num_features=3\n",
        ").to(device)\n",
        "\n",
        "optimizer_enh = optim.Adam(model_enhanced.parameters(), lr=1e-3, weight_decay=1e-4)\n",
        "criterion = nn.BCELoss(reduction='none')\n",
        "\n",
        "# Training loop\n",
        "NUM_EPOCHS = 50\n",
        "best_test_auc = 0\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    train_loss, train_auc = train_epoch_enhanced(\n",
        "        model_enhanced, train_loader_enh, optimizer_enh, criterion, device\n",
        "    )\n",
        "\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        print(f\"Epoch {epoch+1}: Train Loss={train_loss:.4f}, Train AUC={train_auc:.4f}\")\n",
        "\n",
        "    # Save final model\n",
        "    if epoch + 1 == NUM_EPOCHS:\n",
        "        torch.save({\n",
        "            'model_state_dict': model_enhanced.state_dict(),\n",
        "            'train_auc': train_auc\n",
        "        }, 'enhanced_sakt_model.pth')\n",
        "\n",
        "# Evaluate both models\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"MODEL COMPARISON\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Load baseline model\n",
        "baseline_checkpoint = torch.load('best_sakt_model.pth', weights_only=False)\n",
        "model_baseline = SAKT(num_skills=145, embed_dim=100, num_heads=5, dropout=0.2).to(device)\n",
        "model_baseline.load_state_dict(baseline_checkpoint['model_state_dict'])\n",
        "\n",
        "# Evaluate baseline\n",
        "print(\"\\nBaseline SAKT:\")\n",
        "baseline_test_loss, baseline_test_auc = validate_epoch(\n",
        "    model_baseline, test_loader, criterion, device\n",
        ")\n",
        "print(f\"Test AUC: {baseline_test_auc:.4f}\")\n",
        "\n",
        "# Evaluate enhanced model\n",
        "print(\"\\nDifficulty-Aware SAKT:\")\n",
        "# Create a validation function for enhanced model\n",
        "def validate_enhanced(model, test_loader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    all_predictions = []\n",
        "    all_targets = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            interactions = batch['interactions'].to(device)\n",
        "            skills = batch['skills'].to(device)\n",
        "            targets = batch['targets'].to(device)\n",
        "            features = batch['features'].to(device)\n",
        "            mask = batch['mask'].to(device)\n",
        "\n",
        "            predictions = model(interactions, skills, features)\n",
        "\n",
        "            loss = criterion(predictions, targets)\n",
        "            masked_loss = (loss * mask).sum() / mask.sum()\n",
        "            total_loss += masked_loss.item()\n",
        "\n",
        "            valid_idx = mask == 1\n",
        "            valid_predictions = predictions[valid_idx].cpu().numpy()\n",
        "            valid_targets = targets[valid_idx].cpu().numpy()\n",
        "\n",
        "            all_predictions.extend(valid_predictions)\n",
        "            all_targets.extend(valid_targets)\n",
        "\n",
        "    test_loss = total_loss / len(test_loader)\n",
        "    test_auc = roc_auc_score(all_targets, all_predictions)\n",
        "\n",
        "    return test_loss, test_auc\n",
        "\n",
        "enhanced_test_loss, enhanced_test_auc = validate_enhanced(\n",
        "    model_enhanced, test_loader_enh, criterion, device\n",
        ")\n",
        "print(f\"Test AUC: {enhanced_test_auc:.4f}\")\n",
        "\n",
        "# Calculate improvement\n",
        "improvement = (enhanced_test_auc - baseline_test_auc) / baseline_test_auc * 100\n",
        "print(f\"\\nImprovement: {improvement:+.2f}%\")\n",
        "print(f\"Absolute gain: {enhanced_test_auc - baseline_test_auc:+.4f}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOB8KVWjsLQ+xS3xaR0xqPW",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}