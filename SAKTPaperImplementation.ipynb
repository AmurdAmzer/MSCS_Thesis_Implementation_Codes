{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMG4yw7nh9dzo8HPTNIjeht",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AmurdAmzer/SAKT-Paper_Implementation/blob/main/SAKTPaperImplementation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cell 1: Uploading data via Colab's File Browser"
      ],
      "metadata": {
        "id": "r3Mjf06XlwkI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 1: Uploading data via Colab's File Browser\n",
        "\n",
        "# os stands for \"Operating System\" - it's like a special toolkit that lets your Python code talk to your computer's file system.\n",
        "import os\n",
        "print(\"Files in current directory:\")\n",
        "for file in os.listdir():\n",
        "  if file.endswith(\".csv\"):\n",
        "    print(f\" - {file}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LacccNggl2gy",
        "outputId": "756c9924-d7c7-44a8-bd8a-6058e35baa9d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files in current directory:\n",
            " - skill_builder_data_corrected_collapsed.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Cell 2: Imports and File upload"
      ],
      "metadata": {
        "id": "soB_jKNd5A3M"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "l7PycBC01iR8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1579c734-d912-462f-c0fa-48d1134ca7ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (196780, 31)\n",
            "\n",
            "Column names: ['Unnamed: 0', 'order_id', 'assignment_id', 'user_id', 'assistment_id', 'problem_id', 'original', 'correct', 'attempt_count', 'ms_first_response', 'tutor_mode', 'answer_type', 'sequence_id', 'student_class_id', 'position', 'type', 'base_sequence_id', 'skill_id', 'skill_name', 'teacher_id', 'school_id', 'hint_count', 'hint_total', 'overlap_time', 'template_id', 'answer_id', 'answer_text', 'first_action', 'bottom_hint', 'opportunity', 'opportunity_original']\n",
            "\n",
            "First 5 rows:    Unnamed: 0  order_id  assignment_id  user_id  assistment_id  problem_id  \\\n",
            "0           1  33022537         277618    64525          33139       51424   \n",
            "1           2  33022709         277618    64525          33150       51435   \n",
            "2           3  35450204         220674    70363          33159       51444   \n",
            "3           4  35450295         220674    70363          33110       51395   \n",
            "4           5  35450311         220674    70363          33196       51481   \n",
            "\n",
            "   original  correct  attempt_count  ms_first_response  ... hint_count  \\\n",
            "0         1        1            1.0            32454.0  ...        0.0   \n",
            "1         1        1            1.0             4922.0  ...        0.0   \n",
            "2         1        0            2.0            25390.0  ...        0.0   \n",
            "3         1        1            1.0             4859.0  ...        0.0   \n",
            "4         1        0           14.0            19813.0  ...        3.0   \n",
            "\n",
            "  hint_total  overlap_time  template_id  answer_id answer_text  first_action  \\\n",
            "0        3.0       32454.0      30799.0        NaN          26           0.0   \n",
            "1        3.0        4922.0      30799.0        NaN          55           0.0   \n",
            "2        3.0       42000.0      30799.0        NaN          88           0.0   \n",
            "3        3.0        4859.0      30059.0        NaN          41           0.0   \n",
            "4        4.0      124564.0      30060.0        NaN          65           0.0   \n",
            "\n",
            "  bottom_hint opportunity  opportunity_original  \n",
            "0         NaN         1.0                   1.0  \n",
            "1         NaN         2.0                   2.0  \n",
            "2         NaN         1.0                   1.0  \n",
            "3         NaN         2.0                   2.0  \n",
            "4         0.0         3.0                   3.0  \n",
            "\n",
            "[5 rows x 31 columns]\n"
          ]
        }
      ],
      "source": [
        "# Cell 2: Imports and File upload (df = pd.read_csv(....)) opens the spreadsheet(data) and puts it into a \"DataFrame\"\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Load the CSV file (Remember to use the actual file name)\n",
        "# encoding='ISO-8859-1' handles special characters in the data\n",
        "# low_memory=False prevents dtype warnings for mixed types\n",
        "\n",
        "# I choose to call my loaded data df, df can mean DataFrame, df is just a variable name, I can choose to name my uploaded data anything.\n",
        "# low_memory=False tells the computer \"take your time reading this properly, don't rush\"\n",
        "# encoding='ISO-8859-1' = Like telling your computer \"hey, this file might have special characters like currency symbols ($ etc.)\".\n",
        "#  Real-Life Analogy\n",
        "\"\"\"\n",
        "It's like telling your computer:\n",
        "When you read this file, treat these bytes or symbols as Latin-style letters â€” not random gibberish.\"\n",
        "\n",
        "If you don't specify the correct encoding, Python might fail to read the file\n",
        "\"\"\"\n",
        "\n",
        "df = pd.read_csv('skill_builder_data_corrected_collapsed.csv', encoding='ISO-8859-1', low_memory=False)\n",
        "\n",
        "# Show basic information about the dataset\n",
        "print(f\"Dataset shape: {df.shape}\") # (rows, columns)\n",
        "print(f\"\\nColumn names: {list(df.columns)}\") # all column names\n",
        "print(f\"\\nFirst 5 rows: {df.head()}\") # preview first 5 rows. head() has a default parameter built in df.head(n=5). you could specify by df.head(5) etc.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cell3: Key Statistics - Understanding the dataset size and scope"
      ],
      "metadata": {
        "id": "Ir9cX3Pr5Ncc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: Key Statistics - Understanding the dataset size and scope\n",
        "\n",
        "print(\"===DATASET OVERVIEW===\")\n",
        "\n",
        "# Count total number of student interactions = number of rows\n",
        "print(f\"Toatal interactions: {len(df)}\")\n",
        "\n",
        "# Count unique students - each student has a unique user_id\n",
        "print(f\"Unique students: {df['user_id'].nunique()}\") # df['user_id] = access the user id column. nunique = count how many unique values are in that column\n",
        "\n",
        "# Count unique problems - individual questions students attempted\n",
        "print(f\"Unique problems: {df['problem_id'].nunique()}\")\n",
        "\n",
        "# Count unique skills - knowledge concepts being tested. this is crucial because I will create embeddings for each skill\n",
        "print(f\"Unique skills: {df['skill_id'].nunique()}\")\n",
        "\n",
        "# Calculate overall performance - percentage of correct answers\n",
        "print(f\"\\nCorrect rate: {df['correct'].mean():.2%}\")  # .2% means format the number as percentage with two decimal places, so for eg. 0.825641 becomes 82.56%\n",
        "\n",
        "# Check data completeness for skill_id (Critical for SAKT)\n",
        "# NB. SAKT needs skill_id to work - rows without the skill_id must be removed\n",
        "print(f\"Rows with skill_id: {df['skill_id'].notna().sum()}\")\n",
        "print(f\"Rows missing skill_id: {df['skill_id'].isna().sum()} ({df['skill_id'].isna().mean():.1%})\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cqUdL-mz24hc",
        "outputId": "ea07fa6c-f40d-42f6-d710-a7a9b5ebec10"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===DATASET OVERVIEW===\n",
            "Toatal interactions: 196780\n",
            "Unique students: 3716\n",
            "Unique problems: 12344\n",
            "Unique skills: 108\n",
            "\n",
            "Correct rate: 65.94%\n",
            "Rows with skill_id: 196779\n",
            "Rows missing skill_id: 1 (0.0%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CELL 4: Examine One Student's Learning Journey (Creating a case study).\n",
        "# Before SAKT learns from all students, you want to see what one student's learning path looks like\n",
        "# This helps to understand the sequential nature of the data"
      ],
      "metadata": {
        "id": "QhKU22wsrOMe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4: Examine One Student's (random) Learning Journey\n",
        "# This helps us understand the sequential nature of the data\n",
        "\n",
        "# Find students sorted by number of attempts (most active students)\n",
        "student_activity = df['user_id'].value_counts()\n",
        "\n",
        "# Pick the 11th most active student (avoid outliers)\n",
        "student_id = student_activity.index[20]\n",
        "\n",
        "# Get all data for this student, sorted by time\n",
        "# Order_id represents the sequence of attempsts\n",
        "\n",
        "student_data = df[df['user_id'] == student_id].sort_values('order_id')\n",
        "\n",
        "# Display student summary\n",
        "print(f\"Student {student_id} attempted {len(student_data)} problems\")\n",
        "print(f\"Skills attempted: {student_data['skill_id'].nunique()}\")\n",
        "print(f\"Correct rate: {student_data['correct'].mean():.2%}\")\n",
        "\n",
        "# Show their first 30 attempts to see the sequential Pattern\n",
        "print(\"\\nFirst 30 attempts:\")\n",
        "print(student_data[['order_id', 'skill_id', 'correct', 'ms_first_response']].head(30))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8nUiiGtprW7R",
        "outputId": "6c407386-37e9-411a-bde2-ec2d61010d7c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Student 71740 attempted 513 problems\n",
            "Skills attempted: 61\n",
            "Correct rate: 71.93%\n",
            "\n",
            "First 30 attempts:\n",
            "        order_id skill_id  correct  ms_first_response\n",
            "196319  21463788      278        1             7204.0\n",
            "121208  21464927       67        0            39127.0\n",
            "121209  21465870       67        0            15611.0\n",
            "121210  21466218       67        1            20876.0\n",
            "121211  21466374       67        1            15829.0\n",
            "121212  21466461       67        1            15672.0\n",
            "196320  21466659      278        0            33294.0\n",
            "196321  21467209      278        1            11968.0\n",
            "196322  21467328      278        1             7718.0\n",
            "196323  21467379      278        0             6390.0\n",
            "196324  21467762      278        0            21623.0\n",
            "108904  21468008       58        1            36952.0\n",
            "183828  21468182      277        0            14547.0\n",
            "183829  21468276      277        0             5844.0\n",
            "183830  21468333      277        0             5641.0\n",
            "183831  21468416      277        0            12313.0\n",
            "183832  21468541      277        1             6062.0\n",
            "196325  26607569      278        0            16895.0\n",
            "196326  26607586      278        1            56513.0\n",
            "196327  26607593      278        1            45375.0\n",
            "196328  26607599      278        1            12974.0\n",
            "107374  26607610       54        1             4664.0\n",
            "183833  26607632      277        1            42229.0\n",
            "51666   26607651       24        1            28361.0\n",
            "173558  26677120       92        0            23549.0\n",
            "183834  26677180      277        1             4467.0\n",
            "183835  26677188      277        1             3421.0\n",
            "183836  26677207      277        1             3439.0\n",
            "167121  29522603       85        1             4294.0\n",
            "131050  29522622    74_85        0             4145.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CELL 5: Visualize Key Patterns to Understand The Data Better"
      ],
      "metadata": {
        "id": "j4AOb7nX5wK_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 5: Visualing Key Patterns to Understand the Data Better\n",
        "\n",
        "# Calculate seuence length for each student (how many problems/quenstions/skills did each student attempt)\n",
        "\n",
        "seq_lengths = df.groupby('user_id').size()\n",
        "\n",
        "# seq_len for the Top 20 students\n",
        "print(\"top 20 Most Active Students:\")\n",
        "print(\"user_id: sequence length\")\n",
        "sorted_seq = seq_lengths.sort_values(ascending=False)\n",
        "for user_id, length in sorted_seq.head(20).items():\n",
        "  print(f\"{user_id}:     {length}\")\n",
        "\n",
        "print(\"\\n\")\n",
        "\n",
        "\n",
        "# seq_len for the Buttom 20 students\n",
        "print(\"Buttom 20 Least Active Students:\")\n",
        "print(\"user_id: sequence length\")\n",
        "sorted_seq = seq_lengths.sort_values(ascending=False)\n",
        "for user_id, length in sorted_seq.tail(20).items():\n",
        "  print(f\"{user_id}:     {length}\")\n",
        "\n",
        "print(\"\\n\")\n",
        "\n",
        "# Random 20 students sorted by length\n",
        "print(\"20 Random students sorted by length\")\n",
        "print(\"user_id: sequence length\")\n",
        "random_sample = seq_lengths.sample(20).sort_values() # If you remove the ascending argument, pandas uses the default, which is: ascending=True\n",
        "for user_id, length in random_sample.items():\n",
        "  print(f\"{user_id}:     {length}\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WPbKjUOS5vWK",
        "outputId": "8d404117-18db-4a69-a98c-591100a300b7"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "top 20 Most Active Students:\n",
            "user_id: sequence length\n",
            "75169:     758\n",
            "78970:     714\n",
            "71881:     683\n",
            "79021:     635\n",
            "78978:     627\n",
            "79019:     624\n",
            "79029:     602\n",
            "70746:     589\n",
            "80807:     578\n",
            "73684:     562\n",
            "78989:     555\n",
            "79032:     549\n",
            "78673:     547\n",
            "70729:     544\n",
            "78980:     542\n",
            "79034:     534\n",
            "96274:     526\n",
            "96235:     525\n",
            "78977:     518\n",
            "79013:     517\n",
            "\n",
            "\n",
            "Buttom 20 Least Active Students:\n",
            "user_id: sequence length\n",
            "84116:     1\n",
            "84117:     1\n",
            "84118:     1\n",
            "83853:     1\n",
            "91165:     1\n",
            "91220:     1\n",
            "85307:     1\n",
            "82395:     1\n",
            "74701:     1\n",
            "74864:     1\n",
            "92256:     1\n",
            "92303:     1\n",
            "84960:     1\n",
            "84997:     1\n",
            "85012:     1\n",
            "81018:     1\n",
            "71560:     1\n",
            "79862:     1\n",
            "81036:     1\n",
            "71573:     1\n",
            "\n",
            "\n",
            "20 Random students sorted by length\n",
            "user_id: sequence length\n",
            "81486:     2\n",
            "91622:     2\n",
            "88480:     3\n",
            "91258:     6\n",
            "89201:     7\n",
            "90192:     8\n",
            "87519:     9\n",
            "82103:     13\n",
            "83299:     16\n",
            "78049:     17\n",
            "88820:     17\n",
            "89940:     18\n",
            "88127:     19\n",
            "80136:     21\n",
            "84072:     24\n",
            "86562:     24\n",
            "83300:     54\n",
            "79673:     116\n",
            "79830:     128\n",
            "78915:     406\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CELL 6: Data Quality Check - Critical for Reliable Model Training"
      ],
      "metadata": {
        "id": "ATuGUM4053dg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 6: Data Quality Check - Critical for Reliable Model Training\n",
        "\n",
        "print(\"Data Quality Report:\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# Check 1: Duplicate rows (same data appearing multiple times)\n",
        "duplicate_count = df.duplicated().sum()\n",
        "print(f\"Duplicate rows: {duplicate_count}\")\n",
        "if duplicate_count > 0:\n",
        "    print(\"Need to remove duplicates!\")\n",
        "\n",
        "# Check 2: Students with very few attempts\n",
        "# SAKT needs sufficient history to learn patterns\n",
        "too_few = (seq_lengths < 5).sum()\n",
        "print(f\"Students with <5 attempts: {too_few} ({too_few/len(seq_lengths)*100:.1f}%)\")\n",
        "\n",
        "# Check 3: Students with too many attempts (potential outliers)\n",
        "too_many = (seq_lengths > 500).sum()\n",
        "print(f\"Students with >500 attempts: {too_many}\")\n",
        "\n",
        "# Check 4: Temporal ordering validation\n",
        "# Sort by user and order_id to check sequence integrity\n",
        "df_sorted = df.sort_values(['user_id', 'order_id'])\n",
        "# For each user, check if order_id always increases\n",
        "is_ordered = df_sorted.groupby('user_id')['order_id'].apply(\n",
        "    lambda x: (x.diff().dropna() > 0).all()  # diff() calculates difference between consecutive values\n",
        ").all()\n",
        "print(f\"All sequences properly ordered: {is_ordered}\")\n",
        "\n",
        "# Check 5: Original vs scaffolding problems\n",
        "# Original = main problem, scaffolding = hints/sub-problems\n",
        "original_count = (df['original'] == 1).sum()\n",
        "scaffold_count = (df['original'] == 0).sum()\n",
        "print(f\"\\nOriginal problems: {original_count} ({original_count/len(df)*100:.1f}%)\")\n",
        "print(f\"Scaffolding problems: {scaffold_count} ({scaffold_count/len(df)*100:.1f}%)\")\n",
        "print(\"SAKT paper uses only original problems\")\n",
        "\n",
        "# Check 6: Answer distribution\n",
        "print(f\"\\nAnswer distribution:\")\n",
        "print(f\"Correct: {(df['correct'] == 1).sum()} ({df['correct'].mean()*100:.1f}%)\")\n",
        "print(f\"Incorrect: {(df['correct'] == 0).sum()} ({(1-df['correct'].mean())*100:.1f}%)\")\n",
        "\n",
        "# Check 7: Critical missing data for SAKT\n",
        "print(f\"\\nMissing data analysis:\")\n",
        "print(f\"Missing skill_id: {df['skill_id'].isna().sum()} rows ({df['skill_id'].isna().mean()*100:.1f}%)\")\n",
        "print(f\"Missing user_id: {df['user_id'].isna().sum()} rows\")\n",
        "print(f\"Missing correct: {df['correct'].isna().sum()} rows\")\n",
        "print(\"SAKT requires skill_id, user_id, and correct to be present\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nh6WRXAH56qi",
        "outputId": "cff73206-ab03-4988-f3ed-d2f2fb623322"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data Quality Report:\n",
            "----------------------------------------\n",
            "Duplicate rows: 0\n",
            "Students with <5 attempts: 566 (15.2%)\n",
            "Students with >500 attempts: 24\n",
            "All sequences properly ordered: True\n",
            "\n",
            "Original problems: 193308 (98.2%)\n",
            "Scaffolding problems: 3472 (1.8%)\n",
            "SAKT paper uses only original problems\n",
            "\n",
            "Answer distribution:\n",
            "Correct: 129756 (65.9%)\n",
            "Incorrect: 67024 (34.1%)\n",
            "\n",
            "Missing data analysis:\n",
            "Missing skill_id: 1 rows (0.0%)\n",
            "Missing user_id: 0 rows\n",
            "Missing correct: 0 rows\n",
            "SAKT requires skill_id, user_id, and correct to be present\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CELL 7: Data Preprocessing for SAKT"
      ],
      "metadata": {
        "id": "OKW2EPOJ7Y3Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 7: Data Preprocessing for SAKT\n",
        "\n",
        "print(\"Starting data preprocessing...\")\n",
        "print(f\"Original data shape: {df.shape}\")\n",
        "\n",
        "# Step 1: Keep only original problems (main problems, not hints)\n",
        "# SAKT paper specifies using only original problems\n",
        "df_clean = df[df['original'] == 1].copy()\n",
        "print(f\"\\nAfter keeping only original problems: {df_clean.shape}\")\n",
        "\n",
        "# Step 2: Remove rows with missing skill_id\n",
        "# SAKT requires skill_id to create embeddings\n",
        "df_clean = df_clean.dropna(subset=['skill_id'])\n",
        "print(f\"After removing missing skill_id: {df_clean.shape}\")\n",
        "\n",
        "# Step 3: Convert skill_id to integer (it might be float due to NaN values)\n",
        "df_clean['skill_id'] = df_clean['skill_id'].astype(int)\n",
        "\n",
        "# Step 4: Calculate sequence lengths per student\n",
        "student_seq_lengths = df_clean.groupby('user_id').size()\n",
        "\n",
        "# Step 5: Keep only students with >= 5 attempts\n",
        "# Too few attempts don't provide enough learning history\n",
        "valid_students = student_seq_lengths[student_seq_lengths >= 5].index\n",
        "df_clean = df_clean[df_clean['user_id'].isin(valid_students)]\n",
        "print(f\"After removing students with <5 attempts: {df_clean.shape}\")\n",
        "\n",
        "# Step 6: Sort by user_id and order_id (temporal order)\n",
        "df_clean = df_clean.sort_values(['user_id', 'order_id'])\n",
        "\n",
        "print(f\"\\nFinal clean dataset:\")\n",
        "print(f\"- Total interactions: {len(df_clean)}\")\n",
        "print(f\"- Unique students: {df_clean['user_id'].nunique()}\")\n",
        "print(f\"- Unique skills: {df_clean['skill_id'].nunique()}\")\n",
        "print(f\"- Average correct rate: {df_clean['correct'].mean():.2%}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gLCecolb7fwX",
        "outputId": "bb8631c1-b7ec-411b-c991-cdf9dd5c5c43"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting data preprocessing...\n",
            "Original data shape: (196780, 31)\n",
            "\n",
            "After keeping only original problems: (193308, 31)\n",
            "After removing missing skill_id: (193307, 31)\n",
            "After removing students with <5 attempts: (192007, 31)\n",
            "\n",
            "Final clean dataset:\n",
            "- Total interactions: 192007\n",
            "- Unique students: 3145\n",
            "- Unique skills: 104\n",
            "- Average correct rate: 66.36%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CELL 8: Transform Data into SAKT Input Format"
      ],
      "metadata": {
        "id": "D9i4en2K7iJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 8: Transform Data into SAKT Input Format\n",
        "\n",
        "# Get unique skills and create mapping\n",
        "unique_skills = sorted(df_clean['skill_id'].unique())\n",
        "num_skills = len(unique_skills)\n",
        "\n",
        "# Create skill_id to index mapping (0 to num_skills-1)\n",
        "skill_to_idx = {skill: idx for idx, skill in enumerate(unique_skills)}\n",
        "\n",
        "print(f\"Number of unique skills: {num_skills}\")\n",
        "print(f\"Skill IDs range: {min(unique_skills)} to {max(unique_skills)}\")\n",
        "\n",
        "# Function to create sequences for each student\n",
        "def create_student_sequences(df_clean, skill_to_idx, num_skills):\n",
        "    \"\"\"\n",
        "    Convert student interactions into SAKT format:\n",
        "    - interaction = skill_idx + (correct * num_skills)\n",
        "    \"\"\"\n",
        "    sequences = []\n",
        "\n",
        "    # Process each student\n",
        "    for user_id, user_data in df_clean.groupby('user_id'):\n",
        "        # Get student's attempt history\n",
        "        skills = user_data['skill_id'].values\n",
        "        corrects = user_data['correct'].values\n",
        "\n",
        "        # Convert skill_id to indices\n",
        "        skill_indices = [skill_to_idx[skill] for skill in skills]\n",
        "\n",
        "        # Create interaction sequence (SAKT encoding)\n",
        "        # interaction = skill_index + (correct * num_skills)\n",
        "        interactions = []\n",
        "        for skill_idx, correct in zip(skill_indices, corrects):\n",
        "            interaction = skill_idx + (correct * num_skills)\n",
        "            interactions.append(interaction)\n",
        "\n",
        "        sequences.append({\n",
        "            'user_id': user_id,\n",
        "            'skill_indices': skill_indices,\n",
        "            'corrects': corrects,\n",
        "            'interactions': interactions,\n",
        "            'length': len(interactions)\n",
        "        })\n",
        "\n",
        "    return sequences\n",
        "\n",
        "# Create sequences\n",
        "print(\"\\nCreating student sequences...\")\n",
        "sequences = create_student_sequences(df_clean, skill_to_idx, num_skills)\n",
        "\n",
        "if len(interactions) > 100:\n",
        "    # Split into chunks of 100\n",
        "    for i in range(0, len(interactions), 100):\n",
        "        chunk = interactions[i:i+100]\n",
        "\n",
        "# Show example sequence\n",
        "print(f\"\\nExample sequence (first student):\")\n",
        "example = sequences[0]\n",
        "print(f\"User ID: {example['user_id']}\")\n",
        "print(f\"Sequence length: {example['length']}\")\n",
        "print(f\"First 5 skills attempted: {example['skill_indices'][:5]}\")\n",
        "print(f\"First 5 responses (0=wrong, 1=correct): {example['corrects'][:5]}\")\n",
        "print(f\"First 5 interaction encodings: {example['interactions'][:5]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        },
        "id": "osY3ties7k2s",
        "outputId": "13f75862-c96d-4dca-a78c-c55a504bda3f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of unique skills: 104\n",
            "Skill IDs range: 1 to 173190193221\n",
            "\n",
            "Creating student sequences...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'interactions' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-13-1807121706.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0msequences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_student_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_clean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskill_to_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_skills\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minteractions\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m     \u001b[0;31m# Split into chunks of 100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minteractions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'interactions' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "HPht-lxO7mZO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CELL 9: Split Data for Training (Student-Level Split)"
      ],
      "metadata": {
        "id": "ZW62OMAt7oKR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 9: Split Data for Training (Student-Level Split)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split at student level (not interaction level) to prevent data leakage\n",
        "# Each student's full sequence goes into either train, val, or test\n",
        "\n",
        "# First split: 80% train+val, 20% test\n",
        "train_val_sequences, test_sequences = train_test_split(\n",
        "    sequences,\n",
        "    test_size=0.2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Second split: From train+val, take 80% train, 20% val\n",
        "train_sequences, val_sequences = train_test_split(\n",
        "    train_val_sequences,\n",
        "    test_size=0.2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "print(f\"Dataset splits:\")\n",
        "print(f\"- Train: {len(train_sequences)} students\")\n",
        "print(f\"- Val: {len(val_sequences)} students\")\n",
        "print(f\"- Test: {len(test_sequences)} students\")\n",
        "\n",
        "# Calculate total interactions per split\n",
        "train_interactions = sum(seq['length'] for seq in train_sequences)\n",
        "val_interactions = sum(seq['length'] for seq in val_sequences)\n",
        "test_interactions = sum(seq['length'] for seq in test_sequences)\n",
        "\n",
        "print(f\"\\nTotal interactions per split:\")\n",
        "print(f\"- Train: {train_interactions:,}\")\n",
        "print(f\"- Val: {val_interactions:,}\")\n",
        "print(f\"- Test: {test_interactions:,}\")\n",
        "\n",
        "# Save the processed data\n",
        "import pickle\n",
        "\n",
        "save_data = {\n",
        "    'train': train_sequences,\n",
        "    'val': val_sequences,\n",
        "    'test': test_sequences,\n",
        "    'num_skills': num_skills,\n",
        "    'skill_to_idx': skill_to_idx\n",
        "}\n",
        "\n",
        "with open('sakt_preprocessed_data.pkl', 'wb') as f:\n",
        "    pickle.dump(save_data, f)\n",
        "\n",
        "print(\"\\nData preprocessing complete! Saved to 'sakt_preprocessed_data.pkl'\")"
      ],
      "metadata": {
        "id": "eV3IUXSR7qwt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CELL 10: SAKT Model Implementation"
      ],
      "metadata": {
        "id": "JzMvcACxEkYx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 10: SAKT Model Implementation\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F #importing the functional module from PyTorch's torch.nn package, and giving it a shorter alias: F.\n",
        "import math\n",
        "\n",
        "class SAKT(nn.Module): # Class SAKT model that inherits nn.module\n",
        "    def __init__(self, num_skills, embed_dim=128, num_heads=4, dropout=0.2):\n",
        "        \"\"\"\n",
        "        SAKT Model matching the paper implementation\n",
        "\n",
        "        Args:\n",
        "            num_skills: Number of unique skills (145 in this case)\n",
        "            embed_dim: Embedding dimension (paper uses 128)\n",
        "            num_heads: Number of attention heads (paper uses 5)\n",
        "            dropout: Dropout rate (paper uses 0.2)\n",
        "        \"\"\"\n",
        "        super(SAKT, self).__init__()\n",
        "\n",
        "        self.num_skills = num_skills\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "        # Embedding layers\n",
        "        # Interaction embedding: Maps (skill + correct*num_skills) to vectors\n",
        "        self.interaction_embed = nn.Embedding(\n",
        "            num_skills * 2,  # *2 because skill + correct*num_skills\n",
        "            embed_dim\n",
        "        )\n",
        "\n",
        "        # Exercise/skill embedding: Maps skills to vectors for queries\n",
        "        self.skill_embed = nn.Embedding(num_skills, embed_dim)\n",
        "\n",
        "        # Positional embedding: Adds temporal information\n",
        "        self.pos_embed = nn.Embedding(1000, embed_dim)  # max sequence length 1000\n",
        "\n",
        "        # Multi-head attention layer\n",
        "        self.attention = nn.MultiheadAttention(\n",
        "            embed_dim,\n",
        "            num_heads,\n",
        "            dropout=dropout,\n",
        "            batch_first=True  # Important: batch dimension first\n",
        "        )\n",
        "\n",
        "        # Layer normalization\n",
        "        self.layer_norm1 = nn.LayerNorm(embed_dim)\n",
        "        self.layer_norm2 = nn.LayerNorm(embed_dim)\n",
        "\n",
        "        # Feed-forward network\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(embed_dim, embed_dim * 4),  # Paper uses 4x hidden size\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(embed_dim * 4, embed_dim)\n",
        "        )\n",
        "\n",
        "        # Output prediction layer\n",
        "        self.pred = nn.Linear(embed_dim, 1)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, interactions, skills):\n",
        "        \"\"\"\n",
        "        Forward pass of SAKT\n",
        "\n",
        "        Args:\n",
        "            interactions: [batch_size, seq_len] - past interactions (skill + correct*num_skills)\n",
        "            skills: [batch_size, seq_len] - skills to predict performance on\n",
        "\n",
        "        Returns:\n",
        "            predictions: [batch_size, seq_len] - probability of correct answer\n",
        "        \"\"\"\n",
        "        batch_size, seq_len = interactions.shape\n",
        "\n",
        "        # Create position indices\n",
        "        positions = torch.arange(seq_len, device=interactions.device).unsqueeze(0).expand(batch_size, -1)\n",
        "\n",
        "        # Embed interactions (for Keys and Values in attention)\n",
        "        interaction_embeds = self.interaction_embed(interactions)  # [batch, seq_len, embed_dim]\n",
        "\n",
        "        # Embed skills (for Queries in attention)\n",
        "        skill_embeds = self.skill_embed(skills)  # [batch, seq_len, embed_dim]\n",
        "\n",
        "        # Add positional embeddings\n",
        "        interaction_embeds = interaction_embeds + self.pos_embed(positions)\n",
        "        skill_embeds = skill_embeds + self.pos_embed(positions)\n",
        "\n",
        "        # Create attention mask (causal mask - can't see future)\n",
        "        attn_mask = torch.triu(\n",
        "            torch.ones(seq_len, seq_len, device=interactions.device) * float('-inf'),\n",
        "            diagonal=1\n",
        "        )\n",
        "\n",
        "        # Apply self-attention\n",
        "        # Query: what skill we're predicting\n",
        "        # Key & Value: past interaction history\n",
        "        attended, _ = self.attention(\n",
        "            query=skill_embeds,\n",
        "            key=interaction_embeds,\n",
        "            value=interaction_embeds,\n",
        "            attn_mask=attn_mask,\n",
        "            need_weights=False\n",
        "        )\n",
        "\n",
        "        # Residual connection and layer norm\n",
        "        attended = self.layer_norm1(skill_embeds + self.dropout(attended))\n",
        "\n",
        "        # Feed-forward network with residual\n",
        "        ffn_out = self.ffn(attended)\n",
        "        ffn_out = self.layer_norm2(attended + self.dropout(ffn_out))\n",
        "\n",
        "        # Predict probability of correct answer\n",
        "        pred = self.pred(ffn_out).squeeze(-1)  # [batch, seq_len]\n",
        "        return torch.sigmoid(pred)\n",
        "\n",
        "# Test the model\n",
        "print(\"Testing SAKT model...\")\n",
        "model = SAKT(num_skills=145)\n",
        "\n",
        "# Create dummy batch\n",
        "batch_interactions = torch.randint(0, 290, (2, 50))  # 2 sequences, length 50\n",
        "batch_skills = torch.randint(0, 145, (2, 50))\n",
        "\n",
        "# Forward pass\n",
        "output = model(batch_interactions, batch_skills)\n",
        "print(f\"Model output shape: {output.shape}\")\n",
        "print(f\"Output range: [{output.min():.3f}, {output.max():.3f}]\")\n",
        "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")"
      ],
      "metadata": {
        "id": "YYZUAsjHEmZQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CELL 11: Create PyTorch Dataset and DataLoaders"
      ],
      "metadata": {
        "id": "0aIb8Y30EtBq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 11: Create PyTorch Dataset and DataLoaders\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "\n",
        "class SAKTDataset(Dataset):\n",
        "    \"\"\"Dataset class for SAKT\"\"\"\n",
        "\n",
        "    def __init__(self, sequences, max_seq_len=100):\n",
        "        self.sequences = sequences\n",
        "        self.max_seq_len = max_seq_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sequences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        seq = self.sequences[idx]\n",
        "        seq_len = seq['length']\n",
        "\n",
        "        # Prepare input sequences\n",
        "        interactions = list(seq['interactions'])\n",
        "        skills = list(seq['skill_indices'])\n",
        "        corrects = list(seq['corrects'])\n",
        "\n",
        "        # Truncate if too long\n",
        "        if seq_len > self.max_seq_len:\n",
        "            interactions = interactions[:self.max_seq_len]\n",
        "            skills = skills[:self.max_seq_len]\n",
        "            corrects = corrects[:self.max_seq_len]\n",
        "            seq_len = self.max_seq_len\n",
        "\n",
        "        # Create input (all except last) and target (all except first)\n",
        "        input_interactions = interactions[:-1]\n",
        "        input_skills = skills[:-1]\n",
        "\n",
        "        target_skills = skills[1:]\n",
        "        target_corrects = corrects[1:]\n",
        "\n",
        "        # Pad sequences to max_seq_len - 1\n",
        "        pad_len = self.max_seq_len - 1 - len(input_interactions)\n",
        "\n",
        "        if pad_len > 0:\n",
        "            input_interactions = input_interactions + [0] * pad_len\n",
        "            input_skills = input_skills + [0] * pad_len\n",
        "            target_skills = target_skills + [0] * pad_len\n",
        "            target_corrects = target_corrects + [0] * pad_len\n",
        "\n",
        "        return {\n",
        "            'interactions': torch.tensor(input_interactions, dtype=torch.long),\n",
        "            'skills': torch.tensor(target_skills, dtype=torch.long),\n",
        "            'targets': torch.tensor(target_corrects, dtype=torch.float),\n",
        "            'mask': torch.tensor([1] * (seq_len-1) + [0] * pad_len, dtype=torch.float)\n",
        "        }\n",
        "\n",
        "# Load preprocessed data\n",
        "import pickle\n",
        "with open('sakt_preprocessed_data.pkl', 'rb') as f:\n",
        "    data = pickle.load(f)\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = SAKTDataset(data['train'], max_seq_len=100)\n",
        "val_dataset = SAKTDataset(data['val'], max_seq_len=100)\n",
        "test_dataset = SAKTDataset(data['test'], max_seq_len=100)\n",
        "\n",
        "# Create dataloaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=128, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
        "\n",
        "# Test dataloader\n",
        "batch = next(iter(train_loader))\n",
        "print(f\"Batch keys: {batch.keys()}\")\n",
        "print(f\"Interactions shape: {batch['interactions'].shape}\")\n",
        "print(f\"Skills shape: {batch['skills'].shape}\")\n",
        "print(f\"Targets shape: {batch['targets'].shape}\")\n",
        "print(f\"Mask shape: {batch['mask'].shape}\")"
      ],
      "metadata": {
        "id": "RnIpiDMGEvyu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "G3iDyuwvE2GR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CELL 12: Test Data Loaders and Verify Data Format"
      ],
      "metadata": {
        "id": "ZBUnkJ5uE4Ox"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 12: Test Data Loaders and Verify Data Format\n",
        "\n",
        "import pickle\n",
        "\n",
        "# Load the preprocessed data we saved in Cell 9\n",
        "# 'rb' means read in binary mode (pickle files are binary)\n",
        "with open('sakt_preprocessed_data.pkl', 'rb') as f:\n",
        "    data = pickle.load(f)  # Converts file back to Python dictionary\n",
        "\n",
        "print(f\"Loaded data with {data['num_skills']} unique skills\")\n",
        "\n",
        "# Create PyTorch datasets from the sequences\n",
        "# SAKTDataset handles padding and creating input/target pairs\n",
        "train_dataset = SAKTDataset(data['train'], max_seq_len=100)\n",
        "val_dataset = SAKTDataset(data['val'], max_seq_len=100)\n",
        "test_dataset = SAKTDataset(data['test'], max_seq_len=100)\n",
        "\n",
        "print(f\"\\nDataset sizes:\")\n",
        "print(f\"Train: {len(train_dataset)} students\")\n",
        "print(f\"Val: {len(val_dataset)} students\")\n",
        "print(f\"Test: {len(test_dataset)} students\")\n",
        "\n",
        "# Create data loaders that will feed batches to my model\n",
        "# batch_size=64 means process 64 students at once\n",
        "# shuffle=True randomizes order each epoch (important for training)\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "# Test by loading one batch to verify everything works\n",
        "print(f\"\\nTesting data loader with one batch:\")\n",
        "batch = next(iter(train_loader))  # Get first batch\n",
        "\n",
        "# Check what's in each batch\n",
        "print(f\"Batch contains: {list(batch.keys())}\")\n",
        "print(f\"Interactions shape: {batch['interactions'].shape}\")  # [64, 99]\n",
        "print(f\"Skills shape: {batch['skills'].shape}\")              # [64, 99]\n",
        "print(f\"Targets shape: {batch['targets'].shape}\")            # [64, 99]\n",
        "print(f\"Mask shape: {batch['mask'].shape}\")                  # [64, 99]\n",
        "\n",
        "# Look at one student's data to understand format\n",
        "print(f\"\\nExample from first student in batch:\")\n",
        "first_seq_len = batch['mask'][0].sum().int()  # Count non-padded positions\n",
        "print(f\"Actual sequence length: {first_seq_len}\")\n",
        "print(f\"First 5 interactions: {batch['interactions'][0][:5].tolist()}\")\n",
        "print(f\"First 5 skills to predict: {batch['skills'][0][:5].tolist()}\")\n",
        "print(f\"First 5 correct/incorrect: {batch['targets'][0][:5].tolist()}\")\n",
        "print(f\"First 5 mask values: {batch['mask'][0][:5].tolist()}\")  # 1=real, 0=padding"
      ],
      "metadata": {
        "id": "Oho9RKT4E5QD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CELL 13: Training Functions for SAKT"
      ],
      "metadata": {
        "id": "KSAwA5DuE7nl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 13: Training Functions for SAKT\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def train_epoch(model, train_loader, optimizer, criterion, device):\n",
        "    \"\"\"\n",
        "    Train the model for one epoch\n",
        "\n",
        "    Args:\n",
        "        model: SAKT model\n",
        "        train_loader: DataLoader with training data\n",
        "        optimizer: Adam optimizer\n",
        "        criterion: BCELoss function\n",
        "        device: cuda or cpu\n",
        "\n",
        "    Returns:\n",
        "        epoch_loss: Average loss for this epoch\n",
        "        epoch_auc: AUC score for this epoch\n",
        "    \"\"\"\n",
        "    model.train()  # Enable dropout and batch norm training behavior\n",
        "    total_loss = 0\n",
        "    all_predictions = []\n",
        "    all_targets = []\n",
        "\n",
        "    # Progress bar to track training\n",
        "    pbar = tqdm(train_loader, desc='Training', leave=False)\n",
        "\n",
        "    for batch in pbar:\n",
        "        # Move all tensors to GPU if available\n",
        "        interactions = batch['interactions'].to(device)\n",
        "        skills = batch['skills'].to(device)\n",
        "        targets = batch['targets'].to(device)\n",
        "        mask = batch['mask'].to(device)\n",
        "\n",
        "        # Clear gradients from previous batch\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass: get predictions from model\n",
        "        predictions = model(interactions, skills)\n",
        "\n",
        "        # Calculate loss only on non-padded positions\n",
        "        # criterion returns loss for each position\n",
        "        loss = criterion(predictions, targets)\n",
        "        # Apply mask and average\n",
        "        masked_loss = (loss * mask).sum() / mask.sum()\n",
        "\n",
        "        # Backward pass: compute gradients\n",
        "        masked_loss.backward()\n",
        "\n",
        "        # Clip gradients to prevent explosion\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "        # Update model parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        # Collect predictions for metrics\n",
        "        total_loss += masked_loss.item()\n",
        "\n",
        "        # Extract only valid (non-padded) predictions\n",
        "        valid_idx = mask == 1\n",
        "        valid_predictions = predictions[valid_idx].detach().cpu().numpy()\n",
        "        valid_targets = targets[valid_idx].detach().cpu().numpy()\n",
        "\n",
        "        all_predictions.extend(valid_predictions)\n",
        "        all_targets.extend(valid_targets)\n",
        "\n",
        "        # Update progress bar with current loss\n",
        "        pbar.set_postfix({'loss': f'{masked_loss.item():.4f}'})\n",
        "\n",
        "    # Calculate epoch metrics\n",
        "    epoch_loss = total_loss / len(train_loader)\n",
        "    epoch_auc = roc_auc_score(all_targets, all_predictions)\n",
        "\n",
        "    return epoch_loss, epoch_auc\n",
        "\n",
        "def validate_epoch(model, val_loader, criterion, device):\n",
        "    \"\"\"\n",
        "    Validate the model (no gradient updates)\n",
        "\n",
        "    Args:\n",
        "        model: SAKT model\n",
        "        val_loader: DataLoader with validation data\n",
        "        criterion: BCELoss function\n",
        "        device: cuda or cpu\n",
        "\n",
        "    Returns:\n",
        "        val_loss: Average validation loss\n",
        "        val_auc: Validation AUC score\n",
        "    \"\"\"\n",
        "    model.eval()  # Disable dropout and use batch norm statistics\n",
        "    total_loss = 0\n",
        "    all_predictions = []\n",
        "    all_targets = []\n",
        "\n",
        "    # No gradient computation needed for validation\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(val_loader, desc='Validating', leave=False):\n",
        "            # Move to device\n",
        "            interactions = batch['interactions'].to(device)\n",
        "            skills = batch['skills'].to(device)\n",
        "            targets = batch['targets'].to(device)\n",
        "            mask = batch['mask'].to(device)\n",
        "\n",
        "            # Get predictions\n",
        "            predictions = model(interactions, skills)\n",
        "\n",
        "            # Calculate loss\n",
        "            loss = criterion(predictions, targets)\n",
        "            masked_loss = (loss * mask).sum() / mask.sum()\n",
        "\n",
        "            # Collect for metrics\n",
        "            total_loss += masked_loss.item()\n",
        "\n",
        "            # Extract valid predictions\n",
        "            valid_idx = mask == 1\n",
        "            valid_predictions = predictions[valid_idx].cpu().numpy()\n",
        "            valid_targets = targets[valid_idx].cpu().numpy()\n",
        "\n",
        "            all_predictions.extend(valid_predictions)\n",
        "            all_targets.extend(valid_targets)\n",
        "\n",
        "    # Calculate validation metrics\n",
        "    val_loss = total_loss / len(val_loader)\n",
        "    val_auc = roc_auc_score(all_targets, all_predictions)\n",
        "\n",
        "    return val_loss, val_auc"
      ],
      "metadata": {
        "id": "y6BM04A2E-iF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CELL 14: Train SAKT Model"
      ],
      "metadata": {
        "id": "uvoiybrsFCFk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 14: Train SAKT Model\n",
        "\n",
        "# Check if GPU is available and use it\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "if device.type == 'cuda':\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "\n",
        "# Initialize model with paper's hyperparameters\n",
        "model = SAKT(\n",
        "    num_skills=145,  # From my dataset analysis\n",
        "    embed_dim=128,   # Paper: d=128\n",
        "    num_heads=4,     # Paper: h=5\n",
        "    dropout=0.2      # Paper: dropout=0.2\n",
        ").to(device)\n",
        "\n",
        "# Count model parameters\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"Total parameters: {total_params:,}\")\n",
        "print(f\"Trainable parameters: {trainable_params:,}\")\n",
        "\n",
        "# Initialize optimizer (Adam with paper's learning rate)\n",
        "optimizer = optim.Adam(\n",
        "    model.parameters(),\n",
        "    lr=1e-3,           # Paper uses 0.001\n",
        "    weight_decay=1e-4  # L2 regularization\n",
        ")\n",
        "\n",
        "# Loss function for binary classification\n",
        "# reduction='none' returns loss per element (needed for masking)\n",
        "criterion = nn.BCELoss(reduction='none')\n",
        "\n",
        "# Learning rate scheduler - reduces LR when validation metric plateaus\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer,\n",
        "    mode='max',        # We want to maximize AUC\n",
        "    factor=0.5,        # Reduce LR by half\n",
        "    patience=3,        # Wait 3 epochs before reducing\n",
        "    verbose=True       # Print when LR is reduced\n",
        ")\n",
        "\n",
        "# Training configuration\n",
        "NUM_EPOCHS = 30\n",
        "EARLY_STOP_PATIENCE = 5\n",
        "best_val_auc = 0\n",
        "patience_counter = 0\n",
        "\n",
        "# Track metrics for visualization\n",
        "history = {\n",
        "    'train_loss': [], 'train_auc': [],\n",
        "    'val_loss': [], 'val_auc': []\n",
        "}\n",
        "\n",
        "print(f\"\\nStarting training for max {NUM_EPOCHS} epochs...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Main training loop\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    print(f\"\\nEpoch {epoch+1}/{NUM_EPOCHS}\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    # Train for one epoch\n",
        "    train_loss, train_auc = train_epoch(\n",
        "        model, train_loader, optimizer, criterion, device\n",
        "    )\n",
        "\n",
        "    # Validate\n",
        "    val_loss, val_auc = validate_epoch(\n",
        "        model, val_loader, criterion, device\n",
        "    )\n",
        "\n",
        "    # Store history\n",
        "    history['train_loss'].append(train_loss)\n",
        "    history['train_auc'].append(train_auc)\n",
        "    history['val_loss'].append(val_loss)\n",
        "    history['val_auc'].append(val_auc)\n",
        "\n",
        "    # Print metrics\n",
        "    print(f\"Train - Loss: {train_loss:.4f}, AUC: {train_auc:.4f}\")\n",
        "    print(f\"Valid - Loss: {val_loss:.4f}, AUC: {val_auc:.4f}\")\n",
        "\n",
        "    # Adjust learning rate based on validation AUC\n",
        "    scheduler.step(val_auc)\n",
        "    current_lr = optimizer.param_groups[0]['lr']\n",
        "    print(f\"Learning rate: {current_lr:.6f}\")\n",
        "\n",
        "    # Save best model\n",
        "    if val_auc > best_val_auc:\n",
        "        best_val_auc = val_auc\n",
        "        patience_counter = 0\n",
        "\n",
        "        # Save checkpoint\n",
        "        checkpoint = {\n",
        "            'epoch': epoch + 1,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'best_val_auc': best_val_auc,\n",
        "            'history': history\n",
        "        }\n",
        "        torch.save(checkpoint, 'best_sakt_model.pth')\n",
        "        print(f\"âœ“ New best model saved! (AUC: {best_val_auc:.4f})\")\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        print(f\"No improvement. Patience: {patience_counter}/{EARLY_STOP_PATIENCE}\")\n",
        "\n",
        "        # Early stopping\n",
        "        if patience_counter >= EARLY_STOP_PATIENCE:\n",
        "            print(f\"\\nEarly stopping triggered after {epoch+1} epochs\")\n",
        "            break\n",
        "\n",
        "print(f\"\\nTraining complete!\")\n",
        "print(f\"Best validation AUC: {best_val_auc:.4f}\")"
      ],
      "metadata": {
        "id": "jqPTIgbjFC37"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CELL 15: Plot Training History"
      ],
      "metadata": {
        "id": "Vp4XSTwZFEzB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 15: Plot Training History\n",
        "\n",
        "# Create figure with two subplots\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "# Subplot 1: Loss curves\n",
        "plt.subplot(1, 2, 1)\n",
        "epochs = range(1, len(history['train_loss']) + 1)\n",
        "plt.plot(epochs, history['train_loss'], 'b-', label='Train Loss', marker='o', markersize=4)\n",
        "plt.plot(epochs, history['val_loss'], 'r-', label='Val Loss', marker='s', markersize=4)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Subplot 2: AUC curves\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs, history['train_auc'], 'b-', label='Train AUC', marker='o', markersize=4)\n",
        "plt.plot(epochs, history['val_auc'], 'r-', label='Val AUC', marker='s', markersize=4)\n",
        "plt.axhline(y=0.82, color='g', linestyle='--', label='Target AUC (0.82)')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('AUC')\n",
        "plt.title('Training and Validation AUC')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print summary statistics\n",
        "print(\"Training Summary:\")\n",
        "print(\"-\" * 40)\n",
        "print(f\"Total epochs trained: {len(history['train_loss'])}\")\n",
        "print(f\"Best validation AUC: {max(history['val_auc']):.4f}\")\n",
        "print(f\"Final train AUC: {history['train_auc'][-1]:.4f}\")\n",
        "print(f\"Final validation AUC: {history['val_auc'][-1]:.4f}\")\n",
        "\n",
        "# Check for overfitting\n",
        "final_gap = history['train_auc'][-1] - history['val_auc'][-1]\n",
        "print(f\"\\nTrain-Val gap: {final_gap:.4f}\")\n",
        "if final_gap > 0.05:\n",
        "    print(\"Model might be overfitting\")\n",
        "else:\n",
        "    print(\"âœ“ No significant overfitting detected\")"
      ],
      "metadata": {
        "id": "QR2gh3sLFHmN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CELL 16: Evaluate on Test Set"
      ],
      "metadata": {
        "id": "-qt2FatlFKDk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 16: Evaluate on Test Set\n",
        "\n",
        "# Load best model\n",
        "checkpoint = torch.load('best_sakt_model.pth', weights_only=False)\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "print(f\"Loaded best model from epoch {checkpoint['epoch']} with val AUC {checkpoint['best_val_auc']:.4f}\")\n",
        "\n",
        "# Evaluate on test set\n",
        "print(\"\\nEvaluating on test set...\")\n",
        "test_loss, test_auc = validate_epoch(model, test_loader, criterion, device)\n",
        "\n",
        "print(f\"\\nTest Set Results:\")\n",
        "print(f\"Test Loss: {test_loss:.4f}\")\n",
        "print(f\"Test AUC: {test_auc:.4f}\")\n",
        "\n",
        "# Compare with validation performance\n",
        "print(f\"\\nGeneralization check:\")\n",
        "print(f\"Validation AUC: {checkpoint['best_val_auc']:.4f}\")\n",
        "print(f\"Test AUC: {test_auc:.4f}\")\n",
        "print(f\"Val-Test gap: {abs(checkpoint['best_val_auc'] - test_auc):.4f}\")\n",
        "\n",
        "# Additional metrics\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Get predictions for additional metrics\n",
        "model.eval()\n",
        "all_predictions = []\n",
        "all_targets = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        interactions = batch['interactions'].to(device)\n",
        "        skills = batch['skills'].to(device)\n",
        "        targets = batch['targets'].to(device)\n",
        "        mask = batch['mask'].to(device)\n",
        "\n",
        "        predictions = model(interactions, skills)\n",
        "\n",
        "        valid_idx = mask == 1\n",
        "        all_predictions.extend(predictions[valid_idx].cpu().numpy())\n",
        "        all_targets.extend(targets[valid_idx].cpu().numpy())\n",
        "\n",
        "# Convert to numpy arrays\n",
        "all_predictions = np.array(all_predictions)\n",
        "all_targets = np.array(all_targets)\n",
        "\n",
        "# Calculate additional metrics using 0.5 threshold\n",
        "binary_predictions = (all_predictions > 0.5).astype(int)\n",
        "\n",
        "accuracy = accuracy_score(all_targets, binary_predictions)\n",
        "precision = precision_score(all_targets, binary_predictions)\n",
        "recall = recall_score(all_targets, binary_predictions)\n",
        "f1 = f1_score(all_targets, binary_predictions)\n",
        "\n",
        "print(f\"\\nAdditional Test Metrics:\")\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1-Score: {f1:.4f}\")"
      ],
      "metadata": {
        "id": "fpUEBkydFMc5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CELL 17: Save Final Model and Results"
      ],
      "metadata": {
        "id": "CohmA6OQFQKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 17: Save Final Model and Results\n",
        "\n",
        "import json\n",
        "from datetime import datetime\n",
        "\n",
        "# Create results summary\n",
        "results = {\n",
        "    'model_name': 'SAKT',\n",
        "    'dataset': 'ASSISTments2009',\n",
        "    'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
        "    'hyperparameters': {\n",
        "        'num_skills': 145,\n",
        "        'embed_dim': 128,\n",
        "        'num_heads': 5,\n",
        "        'dropout': 0.2,\n",
        "        'learning_rate': 0.001,\n",
        "        'batch_size': 64,\n",
        "        'max_seq_len': 100\n",
        "    },\n",
        "    'results': {\n",
        "        'best_val_auc': float(checkpoint['best_val_auc']),\n",
        "        'test_auc': float(test_auc),\n",
        "        'test_accuracy': float(accuracy),\n",
        "        'test_precision': float(precision),\n",
        "        'test_recall': float(recall),\n",
        "        'test_f1': float(f1),\n",
        "        'total_epochs': len(history['train_loss']),\n",
        "        'best_epoch': checkpoint['epoch']\n",
        "    },\n",
        "    'data_stats': {\n",
        "        'train_students': len(data['train']),\n",
        "        'val_students': len(data['val']),\n",
        "        'test_students': len(data['test']),\n",
        "        'unique_skills': data['num_skills']\n",
        "    }\n",
        "}\n",
        "\n",
        "# Save results to JSON\n",
        "with open('sakt_results.json', 'w') as f:\n",
        "    json.dump(results, f, indent=4)\n",
        "\n",
        "# Save final model with all information\n",
        "final_save = {\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'optimizer_state_dict': optimizer.state_dict(),\n",
        "    'results': results,\n",
        "    'history': history,\n",
        "    'skill_to_idx': data['skill_to_idx']\n",
        "}\n",
        "\n",
        "torch.save(final_save, 'sakt_final_model.pth')\n",
        "\n",
        "print(\"Model and results saved!\")\n",
        "print(f\"\\nSummary for thesis:\")\n",
        "print(f\"- SAKT achieved {test_auc:.4f} AUC on ASSISTments2009\")\n",
        "print(f\"- Model has {sum(p.numel() for p in model.parameters()):,} parameters\")\n",
        "print(f\"- Training took {len(history['train_loss'])} epochs\")"
      ],
      "metadata": {
        "id": "K2aDuaUnFSN6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}